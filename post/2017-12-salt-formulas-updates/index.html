
<html class="no-js">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <title>make me: APe</title>
        <meta name="author">
        <meta name="description" content="APe&#39;s blog ~ #family, #tech, #photography, #DevOps, #Cloud, #Chef, #RC planes">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="generator" content="Hugo 0.52" />
        <link href="http://apealive.net/post/index.xml" rel="alternate" type="application/rss+xml" title="make me: APe" />
        <link href="http://apealive.net/post/index.xml" rel="feed" type="application/rss+xml" title="make me: APe" />
        <link href="https://fonts.googleapis.com/css?family=Droid+Sans|Encode+Sans+Condensed|Roboto|Roboto+Condensed" rel="stylesheet">
        <link rel="stylesheet" href="http://apealive.net/css/styles.css">
        
        <link rel="stylesheet" href="http://apealive.net/css/asciidoctor.css">
        
        <link rel="stylesheet" href="http://apealive.net/css/apealive.css">
        
        <link rel="icon" href="http://apealive.net/favicon.ico">
        <link rel="apple-touch-icon" href="http://apealive.net/apple-touch-icon.png" />
        
        <link rel="stylesheet" href="http://apealive.net/css/highlightjs/github.css">

        <script src="//ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/modernizr/2.8.0/modernizr.min.js"></script>
        <script>
            $('#expand').click(function(){
               $(aside).toggle(400);
               $(main).addClass('author-article-view');
            });

        </script>

        <style>
        .site-header h2 .logo {
        background: url(http://apealive.net/img/ZenBG.png) repeat 0 0;
        }
        @media (min--moz-device-pixel-ratio: 1.3), (-o-min-device-pixel-ratio: 2.6 / 2), (-webkit-min-device-pixel-ratio: 1.3), (min-device-pixel-ratio: 1.3), (min-resolution: 1.3dppx) {
          .site-header h2 .logo {
            background-image: url(http://apealive.net/img/ZenBG.png);
        }}
       .site-header {
         background: #2a373d url(http://apealive.net/img/ZenBG.png) repeat center center;
         z-index: -1;
        }
        </style>
    </head>

    <body>
        

        <header class="site-header">
          <div class="transparent-layer">
              <h2>
                  make me : APe
              </h2>
          </div>

        
          
              
              
              
              
         
         

        </header>
        <div class="author" >
          <ul class="social" >
            
<li><a href="//twitter.com/epcim" class="icon-twitter" target="_blank" title="Twitter"></a></li>



<li><a href="//twitter.com/apealive_net" class="icon-twitter" target="_blank" title="Twitter @apealive_net"></a></li>



<li><a href="//facebook.com/petr.michalec" class="icon-facebook" target="_blank" title="Facebook"></a></li>











<li><a href="//github.com/epcim" class="icon-github" target="_blank" title="Github"></a></li>



<li><a href="//plus.google.com/+PetrMichalec-Prague" class="icon-gplus" target="_blank" title="Google+"></a></li>



<li><a href="//delicious.com/epcim" class="icon-delicious" target="_blank" title="Delicious"></a></li>


<li><a href="http://apealive.net/post/index.xml" class="icon-rss" target="_blank" title="RSS"></a></li>

          </ul>
        </div>


<div class="container clearfix">
    <main role="main" class="content expand">
        <article class="post">
            <a class="btn" href="http://apealive.net/" title="Back to home">&laquo; Back to home</a>
            
<h1><a href="http://apealive.net/post/2017-12-salt-formulas-updates/" title="Salt-Formulas updates">Salt-Formulas updates</a></h1>

<footer class="post-info">

&middot; Tagged in
    
    <a href="http://apealive.net/tags/salt">salt</a>
    , 
    
    <a href="http://apealive.net/tags/formulas">formulas</a>
    
    


    </br>
    Read in 151 min.


</span>
</footer>
<p>Sum up all features on salt-formulas project (<a href="https://github.com/salt-formulas">https://github.com/salt-formulas</a>).
For the first, and only this time, for the whole year.</p>

<p>The most features comes from Mirantis and it&rsquo;s MCP product portfolio,
however the public participation in the past year was also important.</p>

<p>As the I generated it, and as I have done only limited post-processing the content might contain
some formatting issues. Further reviews will come &ldquo;seasonal&rdquo; and hopefully better formatted.</p>

<p>If you are using salt-formulas, mind:</p>

<ul>
<li>we foked reclass (<a href="https://github.com/salt-formulas/reclass">https://github.com/salt-formulas/reclass</a>) version, especially check the &ldquo;develop&rdquo; branch</li>
<li>there are many updates in salt-formulas doc at <a href="http://salt-formulas.readthedocs.io/en/latest">http://salt-formulas.readthedocs.io/en/latest</a></li>
<li>there is saltclass ext_pillar in Salt merged, and I shared a system level for it (<a href="https://github.com/epcim/saltclass-system">https://github.com/epcim/saltclass-system</a>)</li>
</ul>

<h1 id="formula-airflow">Formula airflow</h1>

<h3 id="michael-kutý">Michael Kutý<a href="#michael-kutý" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Support storing logs on S3.</p>

<blockquote>
<pre><code>          remote:
            enabled: true
            directory: s3://tmp/logs/prd
            connection_id: db01
            encrypt: false
</code></pre>
</blockquote>

<p>Add ability to load connections and variables.</p>

<blockquote>
<pre><code>        variable:
          my_var:
            name: my_var_name
            value: TOKEN
            type: postgresql
</code></pre>
</blockquote>

<p>Split roles to worker and server.</p>

<blockquote>
<pre><code>      worker:
        enabled: true
</code></pre>
</blockquote>

<p>Add installation from source.</p>

<blockquote>
<pre><code>        source:
          engine: git
          address: https://github.com/apache/incubator-airflow.git
          rev: master       
          pipplugin:
            engine: pip
        connection:
          db01:
            name: db01
            host: localhost
            port: 1234
            user: username
            database: db_name
            password: password
            type: postgres
            extra:
              token: secret
</code></pre>
</blockquote>

<p>Split dags and plugins.</p>

<blockquote>
<pre><code>        dag:
          dagbag:
            engine: git
            address: git@gitlab.com:group/dags.git
            rev: master
        plugin:
          pack-one:
</code></pre>
</blockquote>

<h1 id="formula-aodh">Formula aodh</h1>

<h3 id="filip-pytloun">Filip Pytloun<a href="#filip-pytloun" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Unify Makefile, .gitignore and update readme</p>

<h3 id="nadya-shakhat">Nadya Shakhat<a href="#nadya-shakhat" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Add coordination support via Redis</p>

<p>Aodh is an alarming service for OpenStack. It used to be a part of Ceilometer, but starting from Mitaka it
  is a separate project. Aodh supports several types of alarms like threshold, event, composite and gnocchi-specific.</p>

<p>In cluster mode, coordination is enabled via tooz with Redis backend.</p>

<p>MySQL is used as a data backend for alarms and alarm history.</p>

<p>Cluster aodh service</p>

<blockquote>
<pre><code>        version: mitaka
        ttl: 86400
        cluster: true 
      database:
        engine: &quot;mysql+pymysql&quot;
        host: 10.0.106.20
        port: 3306
        name: aodh
        user: aodh
        password: password
      bind:
        port: 8042
      identity:
        engine: keystone
        host: 10.0.106.20 
        port: 35357
        tenant: service
        password: password 
      message_queue:
        engine: rabbitmq
        port: 5672
        user: openstack
        virtual_host: '/openstack'
</code></pre>

<ul>
<li><a href="https://docs.openstack.org/cli-reference/aodh.html">https://docs.openstack.org/cli-reference/aodh.html</a></li>
<li><a href="https://docs.openstack.org/developer/aodh/">https://docs.openstack.org/developer/aodh/</a></li>
</ul>
</blockquote>

<h3 id="petr-jediný">Petr Jediný<a href="#petr-jediný" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Support for Ocata</p>

<p>Parameterized configuration for Ocata</p>

<p>Configure service under apache2</p>

<p>Other changes:</p>

<blockquote>
<pre><code>  Relates-To: #PROD-10651 #PROD-10652
      cache:
        members:
        - host: 10.10.10.10
            port: 11211
        - host: 10.10.10.11
        - host: 10.10.10.12
</code></pre>
</blockquote>

<h1 id="formula-apache">Formula apache</h1>

<h3 id="aleš-komárek">Aleš Komárek<a href="#aleš-komárek" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Update README.rst</p>

<p>Apache Formula</p>

<p>Sample Pillars</p>

<p>Tuned up log configuration.</p>

<p>More Information</p>

<h3 id="filip-pytloun-1">Filip Pytloun<a href="#filip-pytloun-1" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Unify Makefile, .gitignore and update readme</p>

<h3 id="simon-pasquier">Simon Pasquier<a href="#simon-pasquier" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Add ability to configure the apache logs</p>

<p>This change supports a new entry in the model to tune the configuration
  of the error and custom logs. If absent, it defaults to the same
  settings as before.</p>

<p>Tune the log configuration:</p>

<blockquote>
<pre><code>    parameters:
      apache:
        server:
          site:
            foo:
              enabled: true
              type: static
              log:
                custom:
                  enabled: true
                  file: /var/log/apache2/mylittleponysitecustom.log
                  format: &gt;-
</code></pre>

<p>%{X-Forwarded-For}i %l %u %t \&ldquo;%r\&rdquo; %&gt;s %b %D \&ldquo;%{Referer}i\&rdquo; \&ldquo;%{User-Agent}i\&rdquo;</p>

<pre><code>                error:
                  enabled: false
                  file: /var/log/apache2/foo.error.log
                  level: notice
</code></pre>
</blockquote>

<h1 id="formula-aptcacher">Formula aptcacher</h1>

<h3 id="damian-szeluga">Damian Szeluga<a href="#damian-szeluga" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>aptcacher renamed to apt_cacher_ng</p>

<blockquote>
<pre><code>    apt_cacher_ng:
</code></pre>
</blockquote>

<p>Fix readme</p>

<blockquote>
<pre><code>    aptcacher:
      server:
        enabled: true
        bind:
          address: 0.0.0.0
          port: 3142
        proxy: 'http://proxy-user:proxy-pass@proxy-host:9999'
        passthruurl:
          - 'repos.influxdata.com'
          - 'packagecloud.io'
          - 'packagecloud-repositories.s3.dualstack.us-west-1.amazonaws.com'
          - 'launchpad.net'
          - 'apt.dockerproject.org'
        passhthrupattern:
          - '\.key$'
          - '\.gpg$'
          - '\.pub$'
          - '\.jar$'
</code></pre>
</blockquote>

<p>Fix broken formula + add couple more features</p>

<p>More advanced setup with Proxy and passthru patterns</p>

<blockquote>
<pre><code>      aptcacher:
          server:
              enabled: true
              bind:
                  address: 0.0.0.0
                  port: 3142
              proxy: 'http://proxy-user:proxy-pass@proxy-host:9999'
              passthruurl:
                  - 'repos.influxdata.com'
                  - 'packagecloud.io'
                  - 'packagecloud-repositories.s3.dualstack.us-west-1.amazonaws.com'
                  - 'launchpad.net'
                  - 'apt.dockerproject.org'
              passhthrupattern:
                  - '\.key$'
                  - '\.gpg$'
                  - '\.pub$'
                  - '\.jar$'
</code></pre>
</blockquote>

<h3 id="filip-pytloun-2">Filip Pytloun<a href="#filip-pytloun-2" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Version 2017.3
  aptcacher</p>

<ul>
<li><a href="https://www.unix-ag.uni-kl.de/~bloch/acng/">https://www.unix-ag.uni-kl.de/~bloch/acng/</a></li>
</ul>

<h3 id="pavel-cizinsky">Pavel Cizinsky<a href="#pavel-cizinsky" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>init commit</p>

<p>apt-cacher-ng</p>

<p>Apt-Cacher NG is a caching HTTP proxy intended for use with download clients of system distribution&rsquo;s package managers.</p>

<p>Sample pillars</p>

<p>Single apt-cacher service</p>

<blockquote>
<pre><code>    apt-cacher:
</code></pre>
</blockquote>

<p>Read more</p>

<ul>
<li><a href="https://www.unix-ag.uni-kl.de/~bloch/acng/">https://www.unix-ag.uni-kl.de/~bloch/acng/</a></li>
</ul>

<h1 id="formula-aptly">Formula aptly</h1>

<h3 id="filip-pytloun-3">Filip Pytloun<a href="#filip-pytloun-3" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Add support for defining s3/swift endpoints</p>

<p>Define s3 endpoint:</p>

<blockquote>
<pre><code>    parameters:
      aptly:
        server:
          endpoint:
            mys3endpoint:
              engine: s3
              awsAccessKeyID: xxxx
              awsSecretAccessKey: xxxx
              bucket: test
</code></pre>
</blockquote>

<p>Unify Makefile, .gitignore and update readme</p>

<h1 id="formula-artifactory">Formula artifactory</h1>

<h3 id="alexander-evseev">Alexander Evseev<a href="#alexander-evseev" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Refactor configuration</p>

<p>Repository configuration</p>

<p>Sample pillar above shows basic repository configuration, but you can use any parameters
  described in <a href="https://www.jfrog.com/confluence/display/RTF/Repository+Configuration+JSON">https://www.jfrog.com/confluence/display/RTF/Repository+Configuration+JSON</a></p>

<p>This module does direct map from pillar parameters to repository JSON description
  with two aliases for compatibility:</p>

<ul>
<li>repo_type -&gt; rclass</li>
<li>package_type -&gt; packageType</li>
</ul>

<h3 id="filip-pytloun-4">Filip Pytloun<a href="#filip-pytloun-4" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Unify Makefile, .gitignore and update readme
  * <a href="https://www.jfrog.com/confluence/display/RTF/Repository+Configuration+JSON">https://www.jfrog.com/confluence/display/RTF/Repository+Configuration+JSON</a></p>

<h1 id="formula-avinetworks">Formula avinetworks</h1>

<h3 id="richard-felkl">Richard Felkl<a href="#richard-felkl" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>added support for init scripts</p>

<blockquote>
<pre><code>        saltmaster_ip: 10.0.0.90
</code></pre>
</blockquote>

<p>fixed bugs with mitaka</p>

<blockquote>
<pre><code>        disk_format: qcow2
</code></pre>
</blockquote>

<p>Introduced Avinetworks Vantage formula draft</p>

<p>Avinetworks formula</p>

<p>Sample pillars</p>

<p>Salt formula to setup Avi Networks LBaaS</p>

<blockquote>
<pre><code>    avinetworks:
      server:
        enabled: true
        identity: cloud1
        image_location: http://...
        public_network: INET1


      client:
</code></pre>
</blockquote>

<p>External links</p>

<blockquote>
<pre><code>- https://kb.avinetworks.com/installing-avi-vantage-for-openstack-2/
</code></pre>
</blockquote>

<h1 id="formula-backupninja">Formula backupninja</h1>

<h3 id="ales-komarek">Ales Komarek<a href="#ales-komarek" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Docfixes</p>

<p>Backupninja formula</p>

<p>Backupninja allows you to coordinate system backup by dropping a few simple
  configuration files into /etc/backup.d/. Most programs you might use for
  making backups don&rsquo;t have their own configuration file format.</p>

<p>Backupninja provides a centralized way to configure and schedule many
  different backup utilities. It allows for secure, remote, incremental
  filesytem backup (via rdiff-backup), compressed incremental data, backup
  system and hardware info, encrypted remote backups (via duplicity), safe
  backup of MySQL/PostgreSQL databases, subversion or trac repositories, burn</p>

<p>CD/DVDs or create ISOs, incremental rsync with hardlinking.</p>

<blockquote>
<pre><code>        key:
</code></pre>

<p>client1.domain.com:</p>

<pre><code>            key: ssh-key
</code></pre>
</blockquote>

<p>More information</p>

<h3 id="andrey">Andrey<a href="#andrey" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Allow specifying local engine for client-side storage</p>

<p>If backupninja backs up the database to the local storage
  there is no need to use any engine, no engine config file needs
  to be created.</p>

<p>Backup client with local storage</p>

<blockquote>
<pre><code>    backupninja:
      client:
        enabled: true
        target:
          engine: local
</code></pre>
</blockquote>

<h3 id="filip-pytloun-5">Filip Pytloun<a href="#filip-pytloun-5" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Unify Makefile, .gitignore and update readme</p>

<h3 id="jiri-broulik">Jiri Broulik<a href="#jiri-broulik" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>backup fixes</p>

<blockquote>
<pre><code>            enabled: true
</code></pre>
</blockquote>

<h1 id="formula-barbican">Formula barbican</h1>

<h3 id="kirill-bespalov">Kirill Bespalov<a href="#kirill-bespalov" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>MySQL TLS support</p>

<p>Add ability to use tls for database connections.</p>

<p>PROD-15652</p>

<p>In order to trust remote server&rsquo;s certificate during establishing tls
  connection the CA cert must be provided at client side. By default
  system wide installed CA certs are used. You can change this behavior
  by specifying cacert_file and cacert params (optional).</p>

<p>See examples below:</p>

<blockquote>
<pre><code>- **RabbitMQ**
- **MySQL**


 barbican:
   server:
      database:
        ssl:
          enabled: True
          cacert: cert body if the cacert_file does not exists
          cacert_file: /etc/openstack/mysql-ca.pem
</code></pre>
</blockquote>

<p>RabbitMQ TLS support</p>

<p>Add ability to use tls for messaging.</p>

<p>PROD-15653</p>

<p>Configuring TLS communications</p>

<p><strong>RabbitMQ</strong></p>

<blockquote>
<pre><code>      message_queue:
        port: 5671
          cacert_file: /etc/openstack/rabbitmq-ca.pem
</code></pre>
</blockquote>

<h3 id="oleg-iurchenko">Oleg Iurchenko<a href="#oleg-iurchenko" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Add a feature to import DogTag root cert from Salt Mine</p>

<p>This patch adds a posibility to export  DogTag root cert
  from Salt Mine</p>

<p>There are few sources (engines) to define KRA admin cert:</p>

<p>Engine #1: Define KRA admin cert by pillar.</p>

<p>To define KRA admin cert by pillar need to define the following:</p>

<blockquote>
<pre><code>    barbican:
      server:
        dogtag_admin_cert:
          engine: manual
          key: |
</code></pre>

<p>&hellip; key data &hellip;</p>
</blockquote>

<p>Engine #2: Receive DogTag cert from Salt Mine.</p>

<p>DogTag formula sends KRA cert to dogtag_admin_cert mine function.</p>

<blockquote>
<pre><code>          engine: mine
          minion: ...name of minion which has installed DogTag..
</code></pre>
</blockquote>

<p>Engine #3: No operations.</p>

<p>In case of some additional steps to install KRA certificate which
  are out of scope for the formula, the formula has &lsquo;noop&rsquo; engine
  to perform no operations. If &lsquo;noop&rsquo; engine is defined the formula will
  do nothing to install KRA admin cert.</p>

<blockquote>
<pre><code>          engine: noop
</code></pre>
</blockquote>

<h3 id="petr-jediný-1">Petr Jediný<a href="#petr-jediný-1" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Barbican/Dogtag plugin</p>

<blockquote>
<pre><code>            dogtag_port: 8443
            store_plugin: dogtag_crypto
</code></pre>
</blockquote>

<p>Lower sql_idle_timeout to 180s</p>

<p>HAProxy has default timout 300s so we need to lower it bellow this limit
  by default.
  reconnects before MySQL can drop the connection. If you run MySQL with HAProxy
  you need to consider haproxy client/server timeout parameters.</p>

<blockquote>
<pre><code>          sql_idle_timeout: 180
</code></pre>
</blockquote>

<p>Allow configuration of sql_idle_timeout</p>

<p>This helps to avoid <code>MySQL server has gone away</code> errors.</p>

<p>MySQL server has gone away</p>

<p>MySQL uses a default <code>wait_timeout</code> of 8 hours, after which it will drop
  idle connections. This can result in &lsquo;MySQL Gone Away&rsquo; exceptions. If you
  notice this, you can lower <code>sql_idle_timeout</code> to ensure that SQLAlchemy
  reconnects before MySQL can drop the connection.</p>

<blockquote>
<pre><code>        enabled: true
        version: ocata
        database:
          engine: &quot;mysql+pymysql&quot;
          host: 10.0.106.20
          port: 3306
          name: barbican
          user: barbican
          password: password
          sql_idle_timeout: 1200
</code></pre>
</blockquote>

<p>Basic barbican configuration</p>

<p>Usual suspects:</p>

<blockquote>
<pre><code>  Services:
  Barbican:
</code></pre>
</blockquote>

<p>Add some information to README</p>

<p>Barbican formula</p>

<p>Barbican cluster service</p>

<blockquote>
<pre><code>        host_href: ''
        is_proxied: true
        plugin:
          simple_crypto:
            kek: &quot;YWJjZGVmZ2hpamtsbW5vcHFyc3R1dnd4eXoxMjM0NTY=&quot;
        store:
          software:
            crypto_plugin: simple_crypto
            store_plugin: store_crypto
            global_default: True
        bind:
          address: 10.0.106.20
          port: 9311
          admin_port: 9312
        identity:
          engine: keystone
          port: 35357
          domain: default
          tenant: service
        message_queue:
          engine: rabbitmq
          user: openstack
          virtual_host: '/openstack'
          members:
          - host: 10.10.10.10
            port: 5672
          - host: 10.10.10.11
          - host: 10.10.10.12
        cache:
            port: 11211
</code></pre>
</blockquote>

<p>Running behind loadbalancer</p>

<p>If you are running behind loadbalancer, set the <code>host_href</code> to load balancer&rsquo;s
  address. You can set <code>host_href</code> empty and the api attempts autodetect correct
  address from http requests.</p>

<p>Running behind proxy</p>

<p>If you are running behind proxy, set the <code>is_proxied</code> parameter to <code>true</code>. This
  will allow <code>host_href</code> autodetection with help of proxy headers such as
  <code>X-FORWARDED-FOR</code> and <code>X-FORWARDED-PROTO</code>.</p>

<p>Queuing asynchronous messaging</p>

<p>By default is <code>async_queues_enable</code> set <code>false</code> to invoke worker tasks
  synchronously (i.e. no-queue standalone mode). To enable queuing asynchronous
  messaging you need to set it true.</p>

<blockquote>
<pre><code>        async_queues_enable: true
</code></pre>
</blockquote>

<p>Keystone notification listener</p>

<p>To enable keystone notification listener, set the <code>ks_notification_enable</code>
  to true.
  <code>ks_notifications_allow_requeue</code> enables requeue feature in case of
  notification processing error. Enable this only when underlying transport
  supports this feature.</p>

<blockquote>
<pre><code>        ks_notifications_enable: true
        ks_notifications_allow_requeue: true
</code></pre>
</blockquote>

<p>Configuring plugins</p>

<p>Dogtag KRA</p>

<blockquote>
<pre><code>          dogtag:
            pem_path: '/etc/barbican/kra_admin_cert.pem'
            dogtag_host: localhost
            dogtag_port: 8433
            nss_db_path: '/etc/barbican/alias'
            nss_db_path_ca: '/etc/barbican/alias-ca'
            nss_password: 'password123'
            simple_cmc_profile: 'caOtherCert'
            ca_expiration_time: 1
            plugin_working_dir: '/etc/barbican/dogtag'
</code></pre>
</blockquote>

<p>KMIP HSM</p>

<blockquote>
<pre><code>          kmip:
            username: 'admin'
            password: 'password'
            host: localhost
            port: 5696
            keyfile: '/path/to/certs/cert.key'
            certfile: '/path/to/certs/cert.crt'
            ca_certs: '/path/to/certs/LocalCA.crt'
</code></pre>
</blockquote>

<p>PKCS11 HSM</p>

<blockquote>
<pre><code>          p11_crypto:
            library_path: '/usr/lib/libCryptoki2_64.so'
            login: 'mypassword'
            mkek_label: 'an_mkek'
            mkek_length: 32
            hmac_label: 'my_hmac_label'
</code></pre>
</blockquote>

<p>Software Only Crypto</p>

<p><code>kek</code> is key encryption key created from 32 bytes encoded as Base64. You should
  not use this in production.</p>

<blockquote>
<pre><code>            kek: 'YWJjZGVmZ2hpamtsbW5vcHFyc3R1dnd4eXoxMjM0NTY='
</code></pre>
</blockquote>

<p>Secret stores</p>

<blockquote>
<pre><code>            store_plugin: kmip_plugin
            store_plugin: dogtag_plugin
          pkcs11:
            crypto_plugin: p11_crypto
</code></pre>
</blockquote>

<p>Initial commit</p>

<p>barbican formula</p>

<p>Barbican is a REST API designed for the secure storage, provisioning and
  management of secrets such as passwords, encryption keys and X.509 Certificates.</p>

<p>It is aimed at being useful for all environments, including large ephemeral</p>

<p>Clouds.</p>

<p>Sample pillars</p>

<p>Single barbican service</p>

<ul>
<li><a href="https://docs.openstack.org/barbican/latest/">https://docs.openstack.org/barbican/latest/</a></li>
</ul>

<h1 id="formula-billometer">Formula billometer</h1>

<h3 id="filip-pytloun-6">Filip Pytloun<a href="#filip-pytloun-6" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Unify Makefile, .gitignore and update readme</p>

<h1 id="formula-bind">Formula bind</h1>

<h3 id="ales-komarek-1">Ales Komarek<a href="#ales-komarek-1" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Documentation cleanup</p>

<p>Bind formula</p>

<p>BIND is open source software that enables you to publish your Domain Name System (DNS) information on the Internet, and to resolve DNS queries for your users. The name BIND stands for “Berkeley Internet Name Domain”, because the software originated in the early 1980s at the University of California at Berkeley.</p>

<p>Sample pillars</p>

<blockquote>
<pre><code>    bind:
      server:
        enabled: true
        key:
          keyname:
            secret: xyz
            algorithm: hmac-sha512
        server:
</code></pre>

<p>8.8.8.8:</p>

<pre><code>            keys:
              - keyname
        zone:
</code></pre>

<p>sub.domain.com:</p>

<pre><code>            ttl: 86400
            root: &quot;hostmaster@domain.com&quot;
            type: master
            records:
            - name: @
              type: A
              ttl: 7200
              value: 192.168.0.5
</code></pre>

<p>1.168.192.in-addr.arpa:</p>

<pre><code>            notify: false
</code></pre>

<p>slave.domain.com:</p>

<pre><code>            type: slave
            notify: true
            masters:
              # Masters must be specified by IP address
              - 8.8.8.8
              - 8.8.4.4
        dnssec:
          enabled: true
        # Don't hide version
        version: true
        # Allow recursion, better don't on public dns servers
        recursion:
          hosts:
            - localhost
</code></pre>
</blockquote>

<p>Read more</p>

<ul>
<li><a href="https://github.com/theforeman/puppet-dns">https://github.com/theforeman/puppet-dns</a></li>
<li><a href="https://help.ubuntu.com/community/BIND9ServerHowto">https://help.ubuntu.com/community/BIND9ServerHowto</a></li>
<li><a href="https://www.isc.org/downloads/bind/">https://www.isc.org/downloads/bind/</a></li>
</ul>

<h3 id="filip-pytloun-7">Filip Pytloun<a href="#filip-pytloun-7" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Add client role to configure rndc</p>

<p>Server</p>

<p>Client</p>

<blockquote>
<pre><code>      client:
        option:
          default:
            server: localhost
            port: 953
            key: keyname
</code></pre>
</blockquote>

<p>Allow definition of controls</p>

<blockquote>
<pre><code>        control:
          local:
            enabled: true
            bind:
              address: 127.0.0.1
              port: 953
            allow:
              - 127.0.0.1
              - xyz
</code></pre>
</blockquote>

<p>You can use following command to generate key:</p>

<p>dnssec-keygen -a HMAC-SHA512 -b 512 -n HOST -r /dev/urandom mykey</p>

<p>Unify Makefile, .gitignore and update readme</p>

<h1 id="formula-bird">Formula bird</h1>

<h3 id="aleš-komárek-1">Aleš Komárek<a href="#aleš-komárek-1" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Update and rename README.md to README.rst</p>

<p>BIRD Formula</p>

<p>The BIRD project aims to develop a fully functional dynamic IP routing daemon primarily targeted on (but not limited to) Linux, FreeBSD and other UNIX-like systems and distributed under the GNU General Public License.</p>

<p>Sample Pillars</p>

<blockquote>
<pre><code>    bird:
      server:
        enabled: True
        logging:
          engine: syslog
        protocol:
          my_ospf:
            type: ospf
            tick: 2
            rfc1583compat: True
            ecmp: True
            area:
</code></pre>

<p>0.0.0.0:</p>

<pre><code>                interface:
                  p3p1:
                    type: ptp
                    paramX: xxx
                  p3p2:
                  tap0: {}
                  vhost0:
                    hello: 9
                    type: broadcast
</code></pre>
</blockquote>

<p>More Information</p>

<ul>
<li><a href="http://bird.network.cz/">http://bird.network.cz/</a></li>
<li><a href="https://gitlab.labs.nic.cz/labs/bird/wikis/home">https://gitlab.labs.nic.cz/labs/bird/wikis/home</a></li>
</ul>

<h1 id="formula-cadf">Formula cadf</h1>

<h3 id="aleš-komárek-2">Aleš Komárek<a href="#aleš-komárek-2" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>docfix</p>

<p>CADF Formula</p>

<p>The Cloud Auditing Data Federation (CADF) standard defines a full event model anyone can use
  to fill in the essential data needed to certify, self-manage and self-audit application security
  in cloud environments.</p>

<p>Sample Pillars</p>

<h3 id="filip-pytloun-8">Filip Pytloun<a href="#filip-pytloun-8" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Unify Makefile, .gitignore and update readme</p>

<h1 id="formula-calico">Formula calico</h1>

<h3 id="filip-pytloun-9">Filip Pytloun<a href="#filip-pytloun-9" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Unify Makefile, .gitignore and update readme</p>

<h1 id="formula-cassandra">Formula cassandra</h1>

<h3 id="filip-pytloun-10">Filip Pytloun<a href="#filip-pytloun-10" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Unify Makefile, .gitignore and update readme</p>

<h3 id="jiri-broulik-1">Jiri Broulik<a href="#jiri-broulik-1" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>cassandra backup</p>

<p>Backup client with ssh/rsync remote host</p>

<blockquote>
<pre><code>    cassandra:
      backup:
        client:
          enabled: true
          full_backups_to_keep: 3
          hours_before_full: 24
          target:
            host: cfg01
</code></pre>
</blockquote>

<p>More options to relocate local backups can be done using salt-formula-backupninja.</p>

<p>Backup client with local backup only</p>

<p>Backup server rsync</p>

<blockquote>
<pre><code>        server:
          full_backups_to_keep: 5
          key:
            cassandra_pub_key:
              enabled: true
              key: ssh_rsa
</code></pre>
</blockquote>

<p>Client restore from local backup:</p>

<blockquote>
<pre><code>          restore_latest: 1
          restore_from: local
</code></pre>
</blockquote>

<p>Client restore from remote backup:</p>

<blockquote>
<pre><code>          restore_from: remote
</code></pre>
</blockquote>

<h1 id="formula-ccp">Formula ccp</h1>

<h3 id="ales-komarek-2">Ales Komarek<a href="#ales-komarek-2" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Fix to source</p>

<blockquote>
<pre><code>        nodes:
</code></pre>

<p>&lsquo;ctl&rsquo;:</p>

<pre><code>            roles:
            - controller
            - openvswitch
</code></pre>

<p>ctl0[2-3]:</p>

<pre><code>            - compute
</code></pre>
</blockquote>

<p>Getting there</p>

<h3 id="filip-pytloun-11">Filip Pytloun<a href="#filip-pytloun-11" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Unify Makefile, .gitignore and update readme</p>

<h1 id="formula-ceilometer">Formula ceilometer</h1>

<h3 id="aleš-komárek-3">Aleš Komárek<a href="#aleš-komárek-3" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Update README.rst</p>

<p>Ceilometer Formula</p>

<p>Sample Pillars</p>

<p>More Information</p>

<h3 id="dmitry-ukov">Dmitry Ukov<a href="#dmitry-ukov" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Policy.json should be defined by user</p>

<p>User can override and add values to policy.json by creating flat
  key-value structure under ceilometer:server:policy.</p>

<p>Configuration of policy.json file</p>

<blockquote>
<pre><code>    ceilometer:
      server:
</code></pre>

<p>&hellip;.</p>

<pre><code>        policy:
          segregation: 'rule:context_is_admin'
          # Add key without value to remove line from policy.json
</code></pre>

<p>&lsquo;telemetry:get_resource&rsquo;:</p>
</blockquote>

<h3 id="filip-pytloun-12">Filip Pytloun<a href="#filip-pytloun-12" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Unify Makefile, .gitignore and update readme</p>

<h3 id="nadya-shakhat-1">Nadya Shakhat<a href="#nadya-shakhat-1" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Add coordination for a central agent
  billing, across all current OpenStack components with work underway to</p>

<p>This formula provides different backends for Ceilometer data: MongoDB, InfluxDB. Also,</p>

<p>Graphite and direct (to Elasticsearch) publishers are available. If InfluxDB is used
  as a backend, heka is configured to consume messages from RabbitMQ and write in to</p>

<p>InfluxDB, i.e. ceilometer collector service is not used in this configuration.</p>

<blockquote>
<pre><code>        version: mitaka
</code></pre>
</blockquote>

<p>Databases configuration</p>

<p>MongoDB example:</p>

<blockquote>
<pre><code>          members:
          - host: 10.0.106.10
            port: 27017
          - host: 10.0.106.20
          - host: 10.0.106.30
          password: password
</code></pre>
</blockquote>

<p>InfluxDB/Elasticsearch example:</p>

<blockquote>
<pre><code>        database:
          influxdb:
            host: 10.0.106.10
            port: 8086
            user: ceilometer
            password: password
            database: ceilometer
          elasticsearch:
            enabled: true
            port: 9200
</code></pre>
</blockquote>

<h3 id="ondrej-smola">Ondrej Smola<a href="#ondrej-smola" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>added support for CORS parameters</p>

<p>Enable CORS parameters</p>

<blockquote>
<pre><code>        cors:
          allowed_origin: https:localhost.local,http:localhost.local
          expose_headers: X-Auth-Token,X-Openstack-Request-Id,X-Subject-Token
          allow_methods: GET,PUT,POST,DELETE,PATCH
          allow_headers: X-Auth-Token,X-Openstack-Request-Id,X-Subject-Token
          allow_credentials: True
          max_age: 86400
</code></pre>
</blockquote>

<h3 id="petr-jediný-2">Petr Jediný<a href="#petr-jediný-2" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Introduce Newton and Ocata support</p>

<p>Newton support by Jakub Pavlík <a href="mailto:pavlk.jakub@gmail.com">pavlk.jakub@gmail.com</a></p>

<p>Parameterized configuration files for ceilometer in Newton and Ocata.</p>

<p>For Ocata move deprecated ceilometer-api to webserver</p>

<p>Other changes:</p>

<p>Ceilometer instance discovery method</p>

<blockquote>
<pre><code>      agent:
        discovery_method: naive
</code></pre>
</blockquote>

<p>Keystone auth caching</p>

<blockquote>
<pre><code>        cache:
            - host: 10.10.10.10
              port: 11211
            - host: 10.10.10.11
            - host: 10.10.10.12
</code></pre>
</blockquote>

<h1 id="formula-ceph">Formula ceph</h1>

<h3 id="filip-pytloun-13">Filip Pytloun<a href="#filip-pytloun-13" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Unify Makefile, .gitignore and update readme</p>

<h3 id="jiri-broulik-2">Jiri Broulik<a href="#jiri-broulik-2" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>monitoring</p>

<p>PROD-15486</p>

<p>By default monitoring is setup to collect information from MON and OSD nodes. To change the default values add the following pillar to MON nodes.</p>

<blockquote>
<pre><code>        space_used_warning_threshold: 0.75
        space_used_critical_threshold: 0.85
        apply_latency_threshold: 0.007
        commit_latency_threshold: 0.7
        pool_space_used_utilization_warning_threshold: 0.75
        pool_space_used_critical_threshold: 0.85
        pool_write_ops_threshold: 200
        pool_write_bytes_threshold: 70000000
        pool_read_bytes_threshold: 70000000
        pool_read_ops_threshold: 1000
</code></pre>
</blockquote>

<p>dmcrypt support / osd disk encryption support
  related prod:</p>

<p>PROD-15919</p>

<blockquote>
<pre><code>              dmcrypt: true
</code></pre>
</blockquote>

<p>update Readme for Bluestore / add ceph.common into init
  * Block.db location for Bluestore</p>

<p>There are two ways to setup block.db:
  * <strong>Colocated</strong> block.db partition is created on the same disk as partition for the data. This setup is easier for installation and it doesn&rsquo;t require any other disk to be used. However, colocated setup is significantly slower than dedicated)
  * <strong>Dedicate</strong> block.db is placed on different disk than data (or into partition). This setup can deliver much higher performance than colocated but it require to have more disks in servers. Block.db drives should be carefully selected because high I/O and durability is required.</p>

<ul>
<li>Block.wal location for Bluestore</li>
</ul>

<p>There are two ways to setup block.wal - stores just the internal journal (write-ahead log):
  * <strong>Colocated</strong> block.wal uses free space of the block.db device.
  * <strong>Dedicate</strong> block.wal is placed on different disk than data (better put into partition as the size can be small) and possibly block.db device. This setup can deliver much higher performance than colocated but it require to have more disks in servers. Block.wal drives should be carefully selected because high I/O and durability is required.</p>

<ul>
<li>Journal location for Filestore</li>
</ul>

<p>There are two ways to setup journal:
  * <strong>Colocated</strong> journal is created on the same disk as partition for the data. This setup is easier for installation and it doesn&rsquo;t require any other disk to be used. However, colocated setup is significantly slower than dedicated)
  * <strong>Dedicate</strong> journal is placed on different disk than data (or into partition). This setup can deliver much higher performance than colocated but it require to have more disks in servers. Journal drives should be carefully selected because high I/O and durability is required.</p>

<p>Readme update with Bluestore compression options</p>

<p>Inline compression for Bluestore backend</p>

<blockquote>
<pre><code>    ceph:
      setup:
        pool:
          volumes:
            pg_num: 256
            pgp_num: 256
            type: replicated
            crush_rule: hdd
            application: rbd
            compression_algorithm: snappy
            compression_mode: aggressive
            compression_required_ratio: .875
</code></pre>

<p>&hellip;</p>
</blockquote>

<p>adding manage non admin / mon key keyring capability</p>

<p>Ceph manage keyring keys</p>

<p>Keyrings are dynamically generated unless specified by the following pillar.</p>

<blockquote>
<pre><code>      common:
        manage_keyring: true
        keyring:
          glance:
            name: images
            key: AACf3ulZFFPNDxAAd2DWds3aEkHh4IklZVgIaQ==
            caps:
              mon: &quot;allow r&quot;
              osd: &quot;allow class-read object_prefix rdb_children, allow rwx pool=images&quot;
</code></pre>
</blockquote>

<p>fixes for kraken and lower versions</p>

<p>For Kraken and earlier releases application param is not needed.</p>

<blockquote>
<pre><code>            - osd
</code></pre>
</blockquote>

<p>extend crush map and keyring setup</p>

<blockquote>
<pre><code>            crush_rule: sata
            crush_rule: ssd
</code></pre>
</blockquote>

<p>Generate CRUSH map - Recommended way</p>

<p>It is required to define the <code>type</code> for crush buckets and these types must start with <code>root</code> (top) and end with <code>host</code>. OSD daemons will be assigned to hosts according to it&rsquo;s hostname. Weight of the buckets will be calculated according to weight of it&rsquo;s children.</p>

<p>If the pools that are in use have size of 3 it is best to have 3 children of a specific type in the root CRUSH tree to replicate objects across (Specified in rule steps by &lsquo;type region&rsquo;).</p>

<blockquote>
<pre><code>        crush:
          enabled: True
          tunables:
            choose_total_tries: 50
            choose_local_tries: 0
            choose_local_fallback_tries: 0
            chooseleaf_descend_once: 1
            chooseleaf_vary_r: 1
            chooseleaf_stable: 1
            straw_calc_version: 1
            allowed_bucket_algs: 54
          type:
            - root
            - region
            - rack
            - host
          root:
            - name: root-ssd
            - name: root-sata
          region:
            - name: eu-1
              parent: root-sata
            - name: eu-2
            - name: eu-3
              parent: root-ssd
            - name: us-1
          rack:
            - name: rack01
              parent: eu-1
            - name: rack02
              parent: eu-2
            - name: rack03
              parent: us-1
          rule:
            sata:
              ruleset: 0
              type: replicated
              min_size: 1
              max_size: 10
              steps:
                - take take root-ssd
                - chooseleaf firstn 0 type region
                - emit
            ssd:
              ruleset: 1
                - take take root-sata
</code></pre>
</blockquote>

<p>Generate CRUSH map - Alternative way</p>

<p>It&rsquo;s necessary to create per OSD pillar.</p>

<blockquote>
<pre><code>      osd:
          - type: root
            name: root1
          - type: region
            name: eu-1
          - type: rack
            name: rack01
          - type: host
            name: osd001
</code></pre>
</blockquote>

<p>Apply CRUSH map</p>

<p>Before you apply CRUSH map please make sure that settings in generated file in /etc/ceph/crushmap are correct.</p>

<blockquote>
<pre><code>          enforce: true
          images:
          vms:
</code></pre>
</blockquote>

<p>Persist CRUSH map</p>

<p>After the CRUSH map is applied to Ceph it&rsquo;s recommended to persist the same settings even after OSD reboots.</p>

<blockquote>
<pre><code>        crush_update: false
</code></pre>
</blockquote>

<p>moving osd keyring to service level</p>

<p>crush map fix</p>

<blockquote>
<pre><code>        version: luminous
        fsid: a619c5fc-c4ed-4f22-9ed2-66cf2feca23d
        public_network: 10.0.0.0/24, 10.1.0.0/24
        cluster_network: 10.10.0.0/24, 10.11.0.0/24
          bootstrap-osd:
              mon: &quot;allow profile bootstrap-osd&quot;
</code></pre>

<p>&hellip;.</p>

<pre><code>        crush_parent: rack01
        journal_size: 20480                     (20G)
        bluestore_block_db_size: 10073741824    (10G)
        bluestore_block_wal_size: 10073741824   (10G)
              - chooseleaf firstn 0 type failure_domain
</code></pre>
</blockquote>

<p>grains / crush / auto generate keyrings</p>

<blockquote>
<pre><code>              class: ssd
              weight: 1.666
</code></pre>
</blockquote>

<p>Ceph client roles - &hellip;Deprecated - use ceph:common instead</p>

<p>ceph osd fixes</p>

<blockquote>
<pre><code>            key: BQBHPYhZv5mYDBAAvisaSzCTQkC5gywGUp/voA==


        ceph_host_id: '39'
        journal_size: 20480
        bluestore_block_db_size: 1073741824    (1G)
        bluestore_block_wal_size: 1073741824   (1G)
        bluestore_block_size: 807374182400     (800G)
        backend:
          filestore:
            disks:
            - dev: /dev/sdm
              enabled: false
              rule: hdd
              journal: /dev/ssd
              fs_type: xfs
              class: bestssd
              weight: 1.5
            - dev: /dev/sdl
          bluestore:
            - dev: /dev/sdb
            - dev: /dev/sdc
              block_db: /dev/ssd
              block_wal: /dev/ssd
            - dev: /dev/sdd
</code></pre>
</blockquote>

<h3 id="mateusz-los">Mateusz Los<a href="#mateusz-los" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>ceph mon backup</p>

<p>Ceph cluster is accessed using network and thus you need to have decend capacity to handle all the client. There are two networks required for cluster: <strong>public</strong> network and cluster network. Public network is used for client connections and MONs and OSDs are listening on this network. Second network ic called <strong>cluster</strong> networks and this network is used for communication between OSDs.</p>

<p>Ceph monitor backups</p>

<p>Backup client with ssh/rsync remote host</p>

<blockquote>
<pre><code>      backup:
        client:
          enabled: true
          full_backups_to_keep: 3
          hours_before_full: 24
          target:
            host: cfg01
</code></pre>
</blockquote>

<p>Backup client with local backup only</p>

<p>Backup server rsync</p>

<blockquote>
<pre><code>        server:
          full_backups_to_keep: 5
          key:
            ceph_pub_key:
              enabled: true
              key: ssh_rsa
</code></pre>
</blockquote>

<h3 id="ondrej-smola-1">Ondrej Smola<a href="#ondrej-smola-1" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>added mgr role - only for luminous</p>

<p>Ceph mgr roles</p>

<p>The Ceph Manager daemon (ceph-mgr) runs alongside monitor daemons, to provide additional monitoring and interfaces to external monitoring and management systems. Since the 12.x (luminous) Ceph release, the ceph-mgr daemon is required for normal operations. The ceph-mgr daemon is an optional component in the 11.x (kraken) Ceph release.</p>

<p>By default, the manager daemon requires no additional configuration, beyond ensuring it is running. If there is no mgr daemon running, you will see a health warning to that effect, and some of the other information in the output of ceph status will be missing or stale until a mgr is started.</p>

<blockquote>
<pre><code>      mgr:
        enabled: true
        dashboard:
          host: 10.103.255.252
          port: 7000
</code></pre>
</blockquote>

<p>Added ceph mon and osd funcionality (#5)</p>

<p>Try to define what we are goint to achieve.</p>

<p>Ceph formula</p>

<p>Ceph provides extraordinary data storage scalability. Thousands of client
  hosts or KVMs accessing petabytes to exabytes of data. Each one of your
  applications can use the object, block or file system interfaces to the same</p>

<p>RADOS cluster simultaneously, which means your Ceph storage system serves as a
  flexible foundation for all of your data storage needs.</p>

<p>Use salt-formula-linux for initial disk partitioning.</p>

<p>Common metadata for all nodes/roles</p>

<blockquote>
<pre><code>        version: kraken
            param1: value1
            param2: value1
            param3: value1
          pool_section:
            param1: value2
            param2: value2
            param3: value2
        members:
        - name: cmn01
          host: 10.0.0.1
        - name: cmn02
          host: 10.0.0.2
        - name: cmn03
          host: 10.0.0.3
          admin:
            key: AQBHPYhZv5mYDBAAvisaSzCTQkC5gywGUp/voA==
              mds: &quot;allow *&quot;
              mgr: &quot;allow *&quot;
              mon: &quot;allow *&quot;
              osd: &quot;allow *&quot;
</code></pre>
</blockquote>

<p>Optional definition for cluster and public networks. Cluster network is used
  for replication. Public network for front-end communication.</p>

<p>Ceph mon (control) roles</p>

<blockquote>
<pre><code>Monitors: A Ceph Monitor maintains maps of the cluster state, including the
</code></pre>

<p>monitor map, the OSD map, the Placement Group (PG) map, and the CRUSH map.</p>
</blockquote>

<p>Ceph maintains a history (called an “epoch”) of each state change in the Ceph</p>

<p>Monitors, Ceph OSD Daemons, and PGs.</p>

<blockquote>
<pre><code>        config:
          mon:
            key: value
            key: AQAnQIhZ6in5KxAAdf467upoRMWFcVg5pbh1yg==
</code></pre>
</blockquote>

<p>Ceph OSD (storage) roles</p>

<blockquote>
<pre><code>          osd:
        host_id: 10
        copy_admin_key: true
        journal_type: raw
        dmcrypt: disable
        osd_scenario: raw_journal_devices
        fs_type: xfs
        disk:
</code></pre>

<p>&lsquo;00&rsquo;:</p>

<pre><code>            rule: hdd
            dev: /dev/vdb2
            journal: /dev/vdb1
            class: besthdd
            weight: 1.5
</code></pre>

<p>&lsquo;01&rsquo;:</p>

<pre><code>            dev: /dev/vdc2
            journal: /dev/vdc1
</code></pre>

<p>&lsquo;02&rsquo;:</p>

<pre><code>            dev: /dev/vdd2
            journal: /dev/vdd1
</code></pre>
</blockquote>

<p>Ceph client roles</p>

<p>Simple ceph client service</p>

<blockquote>
<pre><code>      client:
          monitoring:
</code></pre>
</blockquote>

<p>At OpenStack control settings are usually located at cinder-volume or glance-
  registry services.</p>

<p>Ceph gateway</p>

<p>Rados gateway with keystone v2 auth backend</p>

<blockquote>
<pre><code>      radosgw:
        hostname: gw.ceph.lab
        bind:
          address: 10.10.10.1
          port: 8080
        identity:
          engine: keystone
          api_version: 2
          host: 10.10.10.100
          port: 5000
          user: admin
          password: password
          tenant: admin
</code></pre>
</blockquote>

<p>Rados gateway with keystone v3 auth backend</p>

<blockquote>
<pre><code>          api_version: 3
          project: admin
          domain: default
</code></pre>
</blockquote>

<p>Ceph setup role</p>

<p>Replicated ceph storage pool</p>

<blockquote>
<pre><code>          replicated_pool:
            crush_ruleset_name: 0
</code></pre>
</blockquote>

<p>Erasure ceph storage pool</p>

<blockquote>
<pre><code>          erasure_pool:
            type: erasure
            erasure_code_profile: 
</code></pre>
</blockquote>

<p>Ceph monitoring</p>

<p>Collect general cluster metrics</p>

<p>Collect metrics from monitor and OSD services</p>

<p>More information</p>

<p>Documentation and bugs</p>

<h3 id="simon-pasquier-1">Simon Pasquier<a href="#simon-pasquier-1" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Add support for Ceph monitoring</p>

<p>This change adds support for Ceph monitoring:</p>

<p>Because Telegraf runs as a container on the monitoring nodes and
  requires a working Ceph client configuration, this  change also adds
  support for deploying Ceph client in container mode.</p>

<p>Client pillar - usually located at cinder-volume or glance-registry.</p>

<p>Monitoring Ceph cluster - collect cluster metrics</p>

<blockquote>
<pre><code>          global:
</code></pre>

<p>mon initial members: ceph1,ceph2,ceph3
  mon host: 10.103.255.252:6789,10.103.255.253:6789,10.103.255.254:6789</p>

<pre><code>            key: 00000000000000000000000000000000000000==
      monitoring:
        cluster_stats:
          ceph_user: monitoring
</code></pre>
</blockquote>

<p>Monitoring Ceph services - collect metrics from monitor and OSD services</p>

<blockquote>
<pre><code>        node_stats:
</code></pre>
</blockquote>

<h3 id="tomáš-kukrál">Tomáš Kukrál<a href="#tomáš-kukrál" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>describe architecture</p>

<p>Daemons</p>

<p>Ceph uses several daemons to handle data and cluster state. Each daemon type requires different computing capacity and hardware optimization.</p>

<p>These daemons are currently supported by formula:</p>

<ul>
<li>MON (<code>ceph.mon</code>)</li>
<li>OSD (<code>ceph.osd</code>)</li>
<li>RGW (<code>ceph.radosgw</code>)</li>
</ul>

<p>Architecture decisions</p>

<p>Please refer to upstream achritecture documents before designing your cluster. Solid understanding of Ceph principles is essential for making architecture decisions described bellow.</p>

<blockquote>
<pre><code>http://docs.ceph.com/docs/master/architecture/
</code></pre>
</blockquote>

<ul>
<li>Ceph version</li>
</ul>

<p>There is 3 or 4 stable releases every year and many of nighty/dev release. You should decide which version will be used since the only stable releases are recommended for production. Some of the releases are marked LTS (Long Term Stable) and these releases receive bugfixed for longer period - usually until next LTS version is released.</p>

<ul>
<li>Number of MON daemons</li>
</ul>

<p>Use 1 MON daemon for testing, 3 MONs for smaller production clusters and 5 MONs for very large production cluster. There is no need to have more than 5 MONs in normal environment because there isn&rsquo;t any significant benefit in running more than 5 MONs. Ceph require MONS to form quorum so you need to heve more than 50% of the MONs up and running to have fully operational cluster. Every I/O operation will stop once less than 50% MONs is availabe because they can&rsquo;t form quorum.</p>

<ul>
<li>Number of PGs</li>
</ul>

<p>Placement groups are providing mappping between stored data and OSDs. It is necessary to calculate number of PGs because there should be stored decent amount of PGs on each OSD. Please keep in mind <em>decreasing number of PGs</em> isn&rsquo;t possible and <em>increading</em> can affect cluster performance.</p>

<blockquote>
<pre><code>http://docs.ceph.com/docs/master/rados/operations/placement-groups/
http://ceph.com/pgcalc/
</code></pre>
</blockquote>

<ul>
<li>Daemon colocation</li>
</ul>

<p>It is recommended to dedicate nodes for MONs and RWG since colocation can have and influence on cluster operations. Howerver, small clusters can be running MONs on OSD node but it is critical to have enough of resources for MON daemons because they are the most important part of the cluster.</p>

<p>Installing RGW on node with other daemons isn&rsquo;t recommended because RGW daemon usually require a lot of bandwith and it harm cluster health.</p>

<ul>
<li>Journal location</li>
</ul>

<p>There are two way to setup journal:
  * <strong>Colocated</strong> journal is located (usually at the beginning) on the same disk as partition for the data. This setup is easier for installation and it doesn&rsquo;t require any other disk to be used. However, colocated setup is significantly slower than dedicated)
  * <strong>Dedicate</strong> journal is placed on different disk than data. This setup can deliver much higher performance than colocated but it require to have more disks in servers. Journal drives should be carefully selected because high I/O and durability is required.</p>

<ul>
<li>Store type (Bluestore/Filestore)</li>
</ul>

<p>Recent version of Ceph support Bluestore as storage backend and backend should be used if available.</p>

<blockquote>
<pre><code>http://docs.ceph.com/docs/master/rados/configuration/bluestore-config-ref/
</code></pre>
</blockquote>

<ul>
<li>Cluster and public network</li>
</ul>

<p>Ceph cluster is accessed using network and thus you need to have decend capacity to handle all the client. There are two networks required for cluster: <strong>public</strong> network and cluster network. Public network is used for client connections and MONs and OSDs are listening on this network. Second network ic called <strong>cluster</strong> networks and this network is used for communication between OSDs.</p>

<p>Both networks should have dedicated interfaces, bonding interfaces and dedicating vlans on bonded interfaces isn&rsquo;t allowed. Good practise is dedicate more throughput for the cluster network because cluster traffic is more important than client traffic.</p>

<ul>
<li>Pool parameters (size, min_size, type)</li>
</ul>

<p>You should setup each pool according to it&rsquo;s expected usage, at least <code>min_size</code> and <code>size</code> and pool type should be considered.</p>

<ul>
<li><p>Cluster monitoring</p></li>

<li><p>Hardware</p></li>
</ul>

<p>Please refer to upstream hardware recommendation guide for general information about hardware.</p>

<p>Ceph servers are required to fulfil special requirements becauce load generated by Ceph can be diametrically opposed to common load.</p>

<blockquote>
<pre><code>http://docs.ceph.com/docs/master/start/hardware-recommendations/
</code></pre>
</blockquote>

<p>Basic management commands</p>

<p>Cluster</p>

<hr />

<blockquote>
<pre><code>- :code:`ceph health` - check if cluster is healthy (:code:`ceph health detail` can provide more information)
</code></pre>
</blockquote>

<p>root@c-01:~# ceph health</p>

<p>HEALTH_OK</p>

<blockquote>
<pre><code>- :code:`ceph status` - shows basic information about cluster
</code></pre>
</blockquote>

<p>root@c-01:~# ceph status
  cluster e2dc51ae-c5e4-48f0-afc1-9e9e97dfd650
  health HEALTH_OK
  monmap e1: 3 mons at {1=192.168.31.201:<sup>6789</sup>&frasl;<sub>0</sub>,2=192.168.31.202:<sup>6789</sup>&frasl;<sub>0</sub>,3=192.168.31.203:<sup>6789</sup>&frasl;<sub>0</sub>}
  election epoch 38, quorum 0,1,2 1,2,3
  osdmap e226: 6 osds: 6 up, 6 in
  pgmap v27916: 400 pgs, 2 pools, 21233 MB data, 5315 objects
  121 GB used, 10924 GB / 11058 GB avail
  400 active+clean
  client io 481 kB/s rd, 132 kB/s wr, 185 op/</p>

<p>MON</p>

<hr />

<blockquote>
<pre><code>http://ceph.com/docs/master/rados/troubleshooting/troubleshooting-mon/
</code></pre>
</blockquote>

<p>OSD</p>

<blockquote>
<pre><code>http://ceph.com/docs/master/rados/troubleshooting/troubleshooting-osd/


- :code:`ceph osd tree` - show all OSDs and it's state
</code></pre>
</blockquote>

<p>root@c-01:~# ceph osd tree</p>

<p>ID WEIGHT   TYPE NAME     UP/DOWN REWEIGHT PRIMARY-AFFINITY</p>

<blockquote>
<pre><code>  -4        0 host c-04
  -1 10.79993 root default
  -2  3.59998     host c-01
</code></pre>

<p>0  1.79999         osd.0      up  1.00000          1.00000
  1  1.79999         osd.1      up  1.00000          1.00000</p>

<pre><code>  -3  3.59998     host c-02
</code></pre>

<p>2  1.79999         osd.2      up  1.00000          1.00000
  3  1.79999         osd.3      up  1.00000          1.00000</p>

<pre><code>  -5  3.59998     host c-03
</code></pre>

<p>4  1.79999         osd.4      up  1.00000          1.00000
  5  1.79999         osd.5      up  1.00000          1.00000</p>

<pre><code>- :code:`ceph osd pools ls` - list of pool
</code></pre>
</blockquote>

<p>root@c-01:~# ceph osd lspools
  0 rbd,1 test</p>

<p>PG</p>

<hr />

<blockquote>
<pre><code>http://ceph.com/docs/master/rados/troubleshooting/troubleshooting-pg


- :code:`ceph pg ls` - list placement groups
</code></pre>
</blockquote>

<p>root@c-01:~# ceph pg ls | head -n 4
  pg_stat   objects mip degr    misp    unf bytes   log disklog state   state_stamp v   reported    up  up_primary  acting  acting_primary  last_scrub  scrub_stamp last_deep_scrub deep_scrub_stamp
  0.0   11  0   0   0   0   46137344    3044    3044    active+clean    2015-07-02 10:12:40.603692  226&rsquo;10652   226:1798    [4,2,0] 4   [4,2,0] 4   0&rsquo;0 2015-07-01 18:38:33.126953  0&rsquo;0 2015-07-01 18:17:01.904194
  0.1   7   0   0   0   0   25165936    3026    3026    active+clean    2015-07-02 10:12:40.585833  226&rsquo;5808    226:1070    [2,4,1] 2   [2,4,1] 2   0&rsquo;0 2015-07-01 18:38:32.352721  0&rsquo;0 2015-07-01 18:17:01.904198
  0.2   18  0   0   0   0   75497472    3039    3039    active+clean    2015-07-02 10:12:39.569630  226&rsquo;17447   226:3213    [3,1,5] 3   [3,1,5] 3   0&rsquo;0 2015-07-01 18:38:34.308228  0&rsquo;0 2015-07-01 18:17:01.904199</p>

<blockquote>
<pre><code>- :code:`ceph pg map 1.1` - show mapping between PG and OSD
</code></pre>
</blockquote>

<p>root@c-01:~# ceph pg map 1.1
  osdmap e226 pg 1.1 (1.1) -&gt; up [5,1,2] acting [5,1,2]</p>

<p>divide setup to crush, keyring and pool, fix tests</p>

<blockquote>
<pre><code>  ceph:
    setup:
      crush:
        enabled: True
        tunables:
          choose_total_tries: 50
        type:
          - root
          - region
          - rack
          - host
        root:
          - name: root1
          - name: root2
        region:
          - name: eu-1
            parent: root1
          - name: eu-2
          - name: us-1
            parent: root2
        rack:
          - name: rack01
            parent: eu-1
          - name: rack02
            parent: eu-2
          - name: rack03
            parent: us-1
        rule:
          sata:
            ruleset: 0
            min_size: 1
            max_size: 10
            steps:
              - take crushroot.performanceblock.satahss.1
              - choseleaf firstn 0 type failure_domain
              - emit
</code></pre>
</blockquote>

<p>add crushmap template, fix tests</p>

<p>Generate CRUSH map</p>

<p>It is required to define the <code>type</code> for crush buckets and these types must start with <code>root</code> (top) and end with <code>host</code>. OSD daemons will be assigned to hosts according to it&rsquo;s hostname. Weight of the buckets will be calculated according to weight of it&rsquo;s childen.</p>

<blockquote>
<pre><code>  crush:
    enabled: True
    tunables:
      choose_total_tries: 50
    type:
      - root
      - region
      - rack
      - host
    root:
      - name: root1
      - name: root2
    region:
      - name: eu-1
        parent: root1
      - name: eu-2
      - name: us-1
        parent: root2
    rack:
      - name: rack01
        parent: eu-1
      - name: rack02
        parent: eu-2
      - name: rack03
        parent: us-1
    rule:
      sata:
        ruleset: 0
        type: replicated
        min_size: 1
        max_size: 10
        steps:
          - take crushroot.performanceblock.satahss.1
          - choseleaf firstn 0 type failure_domain
          - emit
</code></pre>
</blockquote>

<h1 id="formula-chrony">Formula chrony</h1>

<h3 id="filip-pytloun-14">Filip Pytloun<a href="#filip-pytloun-14" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Unify Makefile, .gitignore and update readme</p>

<h1 id="formula-cinder">Formula cinder</h1>

<h3 id="alexander-noskov">Alexander Noskov<a href="#alexander-noskov" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Add Cinder Block Device driver backend (#18)</p>

<p>Cinder setup with Block Device driver</p>

<blockquote>
<pre><code>    cinder:
      volume:
        enabled: true
        backend:
          bdd:
            engine: bdd
            enabled: true
            type_name: bdd
            devices:
              - sdb
              - sdc
              - sdd
</code></pre>
</blockquote>

<p>Add possibility to specify osapi_volume_base_URL and public_endpoint</p>

<p>Running Cinder application without SSL under load balancer with SSL, we should
  set osapi_volume_base_URL and public_endpoint with proper endpoints for the clients.</p>

<p>public_endpoint and osapi_volume_base_url parameters:
  &ldquo;public_endpoint&rdquo; is used for configuring versions endpoint,
  &ldquo;osapi_volume_base_URL&rdquo; is used to present Cinder URL to users.</p>

<p>They are useful when running Cinder under load balancer in SSL.</p>

<blockquote>
<pre><code>      controller:
        public_endpoint_address: https://${_param:cluster_domain}:8776
</code></pre>
</blockquote>

<h3 id="alexey-chekunov">Alexey Chekunov<a href="#alexey-chekunov" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>NetApp backend support</p>

<p>Cinder setup with NetApp</p>

<blockquote>
<pre><code>          netapp:
            engine: netapp
            type_name: netapp
            user: openstack
            vserver: vm1
            server_hostname: 172.18.2.3
            password: password
            storage_protocol: nfs
            transport_type: https
            lun_space_reservation: enabled
            use_multipath_for_image_xfer: True
              - 172.18.1.2:/vol_1
              - 172.18.1.2:/vol_2
              - 172.18.1.2:/vol_3
              - 172.18.1.2:/vol_4
      compute:
</code></pre>
</blockquote>

<h3 id="andrii-ostapenko">Andrii Ostapenko<a href="#andrii-ostapenko" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Allow to specify custom volumes query filters</p>

<p>Cinder setup with custom non-admin volume query filters:</p>

<blockquote>
<pre><code>        query_volume_filters:
          - name
          - status
          - metadata
          - availability_zone
          - bootable
</code></pre>
</blockquote>

<h3 id="damian-szeluga-1">Damian Szeluga<a href="#damian-szeluga-1" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Add AZ fallback</p>

<blockquote>
<pre><code>        availability_zone_fallback: True
</code></pre>
</blockquote>

<h3 id="dmitry-stremkouski">Dmitry Stremkouski<a href="#dmitry-stremkouski" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>netapp backend should have ability to set
  nas_secure_file_operations and nas_secure_file_permissions.</p>

<blockquote>
<pre><code>            nas_secure_file_operations: false
            nas_secure_file_permissions: false
</code></pre>
</blockquote>

<h3 id="dmitry-stremkovskiy">Dmitry Stremkovskiy<a href="#dmitry-stremkovskiy" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Adding ability to configure cinder_internal_tenant_project_id and cinder_internal_tenant_user_id options</p>

<blockquote>
<pre><code>        cinder_internal_tenant_user_id: f46924c112a14c80ab0a24a613d95eef
        cinder_internal_tenant_project_id: b7455b8974bb4064ad247c8f375eae6c
</code></pre>
</blockquote>

<p>Adding ability to set nas_secure_file_permissions and nas_secure_file_operations options</p>

<blockquote>
<pre><code>        nas_secure_file_permissions: false
        nas_secure_file_operations: false
</code></pre>
</blockquote>

<p>Unhardcode service user/group uid/gid values</p>

<p>Warn to not change cinder uid/gid values after user is created</p>

<blockquote>
<pre><code>        cinder_uid: 304
        cinder_gid: 304
</code></pre>
</blockquote>

<h3 id="dmitry-ukov-1">Dmitry Ukov<a href="#dmitry-ukov-1" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Policy.json should be defined by user</p>

<p>User can override and add values to policy.json by creating flat
  key-value structure under cinder:controller:policy.</p>

<p>Configuration of policy.json file</p>

<p>&hellip;.</p>

<blockquote>
<pre><code>        policy:
</code></pre>

<p>&lsquo;volume:delete&rsquo;: &lsquo;rule:admin_or_owner&rsquo;</p>

<pre><code>          # Add key without value to remove line from policy.json
</code></pre>

<p>&lsquo;volume:extend&rsquo;:</p>
</blockquote>

<h3 id="filip-pytloun-15">Filip Pytloun<a href="#filip-pytloun-15" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Unify Makefile, .gitignore and update readme</p>

<h3 id="jakub-pavlik">Jakub Pavlik<a href="#jakub-pavlik" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>fix netapp compute side. Do not introduce new role for package</p>

<blockquote>
<pre><code>    linux:
      system:
        package:
          nfs-common:
            version: latest
</code></pre>
</blockquote>

<h3 id="jiri-broulik-3">Jiri Broulik<a href="#jiri-broulik-3" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>cinder client</p>

<p>Client role</p>

<blockquote>
<pre><code>      client:
        identity:
          host: 127.0.0.1
          port: 35357
          project: service
          user: cinder
          password: pwd
          protocol: http
          endpoint_type: internalURL
          region_name: RegionOne
          ceph:
            type_name: standard-iops
            engine: ceph
            key:
              conn_speed: fibre-10G
</code></pre>
</blockquote>

<p>cinder NFS</p>

<p>Cinder setup with NFS</p>

<blockquote>
<pre><code>        default_volume_type: nfs-driver
          nfs-driver:
            engine: nfs
            type_name: nfs-driver
            volume_group: cinder-volume
            path: /var/lib/cinder/nfs
            - 172.16.10.110:/var/nfs/cinder
            options: rw,sync
</code></pre>
</blockquote>

<h3 id="kirill-bespalov-1">Kirill Bespalov<a href="#kirill-bespalov-1" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>OpenStack HTTPS Endpoints support</p>

<p>Communication between services usually done via internal
  endpoints that are located in internal network. In some
  cases it is required to encrypt traffic even on internal
  network. This patch unhardcode communication protocol between</p>

<p>Cinder and other services. Also adds possibility to specify
  ca_file to verify SSL certificates of remote peers.</p>

<p>PROD-15735</p>

<p>Configuring TLS communications</p>

<p><strong>Note:</strong> by default system wide installed CA certs are used, so <code>cacert_file</code> param is optional, as well as <code>cacert</code>.</p>

<blockquote>
<pre><code>- **RabbitMQ TLS**
 cinder:
</code></pre>

<p>controller, volume:</p>

<pre><code>        port: 5671
</code></pre>

<p>(optional) cacert: cert body if the cacert_file does not exists
  (optional) cacert_file: /etc/openstack/rabbitmq-ca.pem
  (optional) version: TLSv1_2</p>

<pre><code>- **MySQL TLS**
   controller:
      database:
</code></pre>

<p>(optional) cacert_file: /etc/openstack/mysql-ca.pem</p>

<pre><code>- **Openstack HTTPS API**


      identity:
         protocol: https
</code></pre>

<p>(optional) cacert_file: /etc/openstack/proxy.pem</p>

<pre><code>      glance:
</code></pre>
</blockquote>

<p>RabbitMQ TLS support</p>

<p>OSCORE-381</p>

<blockquote>
<pre><code>  Releases: Mitaka, Newton, Ocata
  Usage: see README.rst
        audit:
</code></pre>
</blockquote>

<p><strong>Client-side RabbitMQ TLS configuration.</strong></p>

<p>|</p>

<p>To enable TLS for oslo.messaging you need to provide the CA certificate.</p>

<p>By default system-wide CA certs are used. Nothing should be specified except <code>ssl.enabled</code>.</p>

<blockquote>
<pre><code>  cinder:
</code></pre>

<p>controller or volume:</p>

<pre><code>      message_queue:
        ssl:
          enabled: True
</code></pre>
</blockquote>

<p>Use <code>cacert_file</code> option to specify the CA-cert file path explicitly:</p>

<blockquote>
<pre><code>          cacert_file: /etc/ssl/rabbitmq-ca.pem
</code></pre>
</blockquote>

<p>To manage content of the <code>cacert_file</code> use the <code>cacert</code> option:</p>

<blockquote>
<pre><code>          cacert: |
</code></pre>
</blockquote>

<p>&hellip;</p>

<blockquote>
<pre><code>          cacert_file: /etc/openstack/rabbitmq-ca.pem


Notice:
</code></pre>

<ul>
<li>The <code>message_queue.port</code> is set to <strong>5671</strong> (AMQPS) by default if <code>ssl.enabled=True</code>.</li>
<li>Use <code>message_queue.ssl.version</code> if you need to specify protocol version. By default is TLSv1 for python &lt; 2.7.9 and TLSv1_2 for version above.</li>
</ul>
</blockquote>

<p>cinder type-key normal-storage set hplh:data_pl=r-10-2 hplh:provisioning=full</p>

<h3 id="michel-nederlof">Michel Nederlof<a href="#michel-nederlof" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Add ceph backend option to report discard support</p>

<blockquote>
<pre><code>            report_discard_supported: True
</code></pre>
</blockquote>

<p>Adding availability zone documentation</p>

<p>Cinder setup with custom availability zones:</p>

<blockquote>
<pre><code>        default_availability_zone: my-default-zone
        storage_availability_zone: my-custom-zone-name
</code></pre>
</blockquote>

<p>The default availability zone is used when a volume has been created, without specifying a zone in the create request. (this zone must exist in your configuration obviously)</p>

<p>The storage availability zone is the actual zone where the node belongs to. Make sure to specify this per node.</p>

<p>Check the documentation of OpenStack for more information</p>

<h3 id="oleg-iurchenko-1">Oleg Iurchenko<a href="#oleg-iurchenko-1" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Add Barbican integration</p>

<p>This patch adds Barbican integration to Cinder</p>

<blockquote>
<pre><code>        barbican:
          enabled: true
</code></pre>
</blockquote>

<p>Enable Barbican integration</p>

<h3 id="ondrej-smola-2">Ondrej Smola<a href="#ondrej-smola-2" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>enable support for cors params</p>

<p>Enable CORS parameters</p>

<blockquote>
<pre><code>        cors:
          allowed_origin: https:localhost.local,http:localhost.local
          expose_headers: X-Auth-Token,X-Openstack-Request-Id,X-Subject-Token
          allow_methods: GET,PUT,POST,DELETE,PATCH
          allow_headers: X-Auth-Token,X-Openstack-Request-Id,X-Subject-Token
          allow_credentials: True
          max_age: 86400
</code></pre>
</blockquote>

<p>added support for cinder-backup service</p>

<p>Enable cinder-backup service for ceph</p>

<blockquote>
<pre><code>        version: mitaka
        backup:
          engine: ceph
          ceph_conf: &quot;/etc/ceph/ceph.conf&quot;
          ceph_pool: backup
          ceph_stripe_count: 0
          ceph_stripe_unit: 0
          ceph_user: cinder
          ceph_chunk_size: 134217728
          restore_discard_excess_bytes: false
</code></pre>
</blockquote>

<p>added cinder ldev range for hitashi vsp storage</p>

<p>Cinder setup with Hitachi VPS with defined ldev range</p>

<blockquote>
<pre><code>          hus100_backend:
            type_name: HUS100
            backend: hus100_backend
            engine: hitachi_vsp
            connection: FC
            ldev_range: 0-1000
</code></pre>
</blockquote>

<h3 id="simon-pasquier-2">Simon Pasquier<a href="#simon-pasquier-2" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Allow to configure the pagination</p>

<blockquote>
<pre><code>        osapi_max_limit: 500
</code></pre>
</blockquote>

<h3 id="stelucz">stelucz<a href="#stelucz" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>add enable_force_upload to sample pillars</p>

<blockquote>
<pre><code>        enable_force_upload: true
        nable_force_upload: true
</code></pre>
</blockquote>

<h1 id="formula-collectd">Formula collectd</h1>

<h3 id="aleš-komárek-4">Aleš Komárek<a href="#aleš-komárek-4" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Update README.rst</p>

<p>Collectd formula</p>

<p>External links</p>

<h3 id="filip-pytloun-16">Filip Pytloun<a href="#filip-pytloun-16" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Unify Makefile, .gitignore and update readme</p>

<h1 id="formula-decapod">Formula decapod</h1>

<h3 id="ales-komarek-3">Ales Komarek<a href="#ales-komarek-3" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Basic structure</p>

<p>Decapod formula</p>

<p>Decapod is intendend to simplify deployment and lifecycle management of Ceph.</p>

<p>Sample pillars</p>

<p>Single decapod service</p>

<blockquote>
<pre><code>    decapod:
      server:
        enabled: true
</code></pre>
</blockquote>

<p>Read more</p>

<ul>
<li><a href="http://decapod.readthedocs.io/en/latest/">http://decapod.readthedocs.io/en/latest/</a></li>
</ul>

<h3 id="filip-pytloun-17">Filip Pytloun<a href="#filip-pytloun-17" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Unify Makefile, .gitignore and update readme</p>

<h3 id="mateusz-los-1">Mateusz los<a href="#mateusz-los-1" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>readme</p>

<blockquote>
<pre><code>   decapod:
    server_discovery_key: https://github.com/Mirantis/ceph-lcm/blob/master/containerization/files/devconfigs/config.yaml api['server_discovery_token']
    decapod_ip: 127.0.0.1


    # as default, all ssd disks will be configured as journal devices,
    # if you want to use them as osd you need to define ssdpools and ssd_size
    ssdpools:
      - /dev/sdb
      - /dev/sdc
      - /dev/sdd
      - /dev/sde
    ssd_size: 1.1T
    # If you are using ssdpools variable you also need to specify journal devices
    cache_devices:
      - /dev/xvde1
      - /dev/xvde2


    # decapod needs ansible user to work
    ansible_private_key:
    ansible_public_key:


    decapod_user: &quot;root&quot;
    decapod_pass: &quot;root&quot;


    # ceph internal network
    storage_network: &quot;192.168.0.0/24&quot;
    # ceph frontend network
    frontend_network: &quot;192.168.1.0/24&quot;


    # internal network interface on physical and virtual nodes
    phys_mon_interface: &quot;eth0.1&quot;
    vm_mon_interface: &quot;ens2&quot;


    journal_size: 512
    max_open_files: 131072


        # if you want to use them as osd you need to define ssdpools and ssd_size


      client:
        admin_key: AQCvCbtToC6MDhAATtuT70Sl+DymPCfDSsyV4w==
        users:
                key: AQCvCbtToC6MDhAATtuT70Sl+DymPCfDSsyV4w==
                caps:
                  osd: 'allow r'
                  mon: 'allow class-read object_prefix rbd_children, allow rwx pool=cinder'
          pools:
            - nova:
                rule: 0
                pg: 100
            - cinder:
</code></pre>
</blockquote>

<p>Decapod Server
  decapod</p>

<blockquote>
<pre><code>        server_discovery_key: https://github.com/Mirantis/ceph-lcm/blob/master/containerization/files/devconfigs/config.yaml api['server_discovery_token']
        decapod_ip: 127.0.0.1
        # as default, all ssd disks will be configured as journal devices,
</code></pre>

<p>if you want to use them as osd you need to define ssdpools and ssd_size</p>

<pre><code>        ssdpools:
          - /dev/sdb
          - /dev/sdc
          - /dev/sdd
          - /dev/sde
        ssd_size: 1.1T
        # If you are using ssdpools variable you also need to specify journal devices
        cache_devices:
          - /dev/xvde1
          - /dev/xvde2


        # decapod needs ansible user to work
        ansible_private_key:
        ansible_public_key:


        decapod_user: &quot;root&quot;
        decapod_pass: &quot;root&quot;


        # ceph internal network
        storage_network: &quot;192.168.0.0/24&quot;
        # ceph frontend network
        frontend_network: &quot;192.168.1.0/24&quot;


        # internal network interface on physical and virtual nodes
        phys_mon_interface: &quot;eth0.1&quot;
        vm_mon_interface: &quot;ens2&quot;


        journal_size: 512
        max_open_files: 131072
</code></pre>
</blockquote>

<p>Decapod Client</p>

<blockquote>
<pre><code>  decapod:
    client:
      admin_key: AQCvCbtToC6MDhAATtuT70Sl+DymPCfDSsyV4w==
      users:
        - nova:
            key: AQCvCbtToC6MDhAATtuT70Sl+DymPCfDSsyV4w==
            caps:
              osd: 'allow r'
              mon: 'allow class-read object_prefix rbd_children, allow rwx pool=nova'
        - cinder:
              mon: 'allow class-read object_prefix rbd_children, allow rwx pool=cinder'
      pools:
            rule: 0
            pg: 100
</code></pre>
</blockquote>

<p>Decapod Discover</p>

<h1 id="formula-dekapod">Formula dekapod</h1>

<h3 id="ales-komarek-4">Ales Komarek<a href="#ales-komarek-4" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Restructure begun</p>

<p>Subproject commit 0d5262b500e56c5c008909b386bbc9e49662e98e</p>

<h3 id="filip-pytloun-18">Filip Pytloun<a href="#filip-pytloun-18" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Update formulas</p>

<p>Subproject commit 93f03ec2dd2c69c4ec40c38bf34785e5145c13b3</p>

<p>Add more formulas</p>

<p>Subproject commit 2c74054c69950ecde71194cab4d3af04ac3ce92e</p>

<h1 id="formula-designate">Formula designate</h1>

<h3 id="aleš-komárek-5">Aleš Komárek<a href="#aleš-komárek-5" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Update README.rst</p>

<p>Designate formula</p>

<p>Designate provides DNSaaS services for OpenStack.</p>

<p>Sample pillars</p>

<h3 id="alexander-noskov-1">Alexander Noskov<a href="#alexander-noskov-1" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Refactoring of formula to support external DNS server</p>

<blockquote>
<pre><code>          backend:
            bind9:
              rndc_key: AahjHKbdslHOLs42h/asQA==
</code></pre>
</blockquote>

<h3 id="filip-pytloun-19">Filip Pytloun<a href="#filip-pytloun-19" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Unify Makefile, .gitignore and update readme</p>

<h3 id="mykyta-karpin">Mykyta Karpin<a href="#mykyta-karpin" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Update Designate docs
  other changes:</p>

<p>For Designate with BIND9 local backend:</p>

<blockquote>
<pre><code>    designate:
      server:
        enabled: true
        region: RegionOne
        domain_id: 5186883b-91fb-4891-bd49-e6769234a8fc
        version: ocata
        backend:
          bind9:
            rndc_key: 4pc+X4PDqb2q+5o72dISm72LM1Ds9X2EYZjqg+nmsS7FhdTwzFFY8l/iEDmHxnyjkA33EQC8H+z0fLLBunoitw==
            rndc_algorithm: hmac-sha512
        bind:
          api:
            address: 127.0.0.1
        database:
          engine: mysql
          host: 127.0.0.1
          port: 3306
          name:
            main_database: designate
            pool_manager: designate_pool_manager
          user: designate
          password: passw0rd
        identity:
          engine: keystone
          port: 35357
          tenant: service
        message_queue:
          engine: rabbitmq
          members:
          - host: 127.0.0.1
          user: openstack
          password: password
          virtual_host: '/openstack'
        pools:
          default:
            description: 'default pool'
            attributes:
              service_tier: GOLD
            ns_records:
              - hostname: 'ns1.example.org.'
                priority: 10
              - host: 127.0.0.1
              default_target:
                type: bind9
                description: 'default target'
                masters:
                  - host: 127.0.0.1
                    port: 5354
                options:
                  host: 127.0.0.1
                  port: 53
                  rndc_host: 127.0.0.1
                  rndc_port: 953
                  rndc_key_file: /etc/designate/rndc.key
</code></pre>
</blockquote>

<p><em>domain_id</em> parameter is UUID of DNS zone managed by designate-sink service. This zone will
  be populated by A records for fixed and floating ip addresses of spawned VMs. After designate
  is deployed and zone is created, this parameter should be updated accordingly to UUID of
  newly created zone. Then designate state should be reapplied.</p>

<p>Pools pillar for BIND9 master and multiple slaves setup:</p>

<blockquote>
<pre><code>    pools:
      default:
        description: 'default pool'
        attributes:
          service_tier: GOLD
        ns_records:
          - hostname: 'ns1.example.org.'
            priority: 10
        nameservers:
          - host: 192.168.0.1
            port: 53
          - host: 192.168.0.2
          - host: 192.168.0.3
        targets:
          default_target:
            type: bind9
            description: 'default target'
            masters:
              - host: 192.168.0.4
                port: 5354
            options:
              host: 192.168.0.4
              port: 53
              rndc_host: 192.168.0.4
              rndc_port: 953
              rndc_key_file: /etc/designate/rndc.key
</code></pre>
</blockquote>

<h1 id="formula-devops-portal">Formula devops-portal</h1>

<h3 id="ilya-kharin">Ilya Kharin<a href="#ilya-kharin" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Add debian specification</p>

<p>DevOps Portal</p>

<p>The DevOps Portal provides dashboards and services to boost operational and
  support needs.</p>

<p>Sample pillars
  ~~~~~~~~~~~~~~</p>

<p>The DevOps Portal supports only predefined set of services, add services into
  pillars to make them accessable from the portal. For example let&rsquo;s consider
  how to add a Jenkins service:</p>

<blockquote>
<pre><code>    devops_portal:
      config:
        service:
          jenkins:
            endpoint:
              address: 172.16.10.254
              port: 8081
              https: true
</code></pre>
</blockquote>

<h1 id="formula-docker">Formula docker</h1>

<h3 id="aleš-komárek-6">Aleš Komárek<a href="#aleš-komárek-6" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Update README.rst</p>

<p>Docker Formula</p>

<p>Sample Pillars</p>

<p>Docker Host</p>

<p>Docker Swarm</p>

<p>Metadata for worker.</p>

<p>Docker Client</p>

<p>Using Docker Compose
  ~~~~~~~~~~~~~~~~~~~~</p>

<p>Docker Registry</p>

<p>More Information</p>

<h3 id="filip-pytloun-20">Filip Pytloun<a href="#filip-pytloun-20" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Add support for docker stack deploy</p>

<p>There are two states that provides this functionality:</p>

<blockquote>
<pre><code>- docker.client.stack
- docker.client.compose
</code></pre>
</blockquote>

<p>Stack is new and works with Docker Swarm Mode. Compose is legacy and works
  only if node isn&rsquo;t member of Swarm.</p>

<p>Metadata for both states are similar and differs only in implementation.</p>

<p>Stack
  ^^^^^</p>

<blockquote>
<pre><code>    docker:
      client:
        stack:
          django_web:
            enabled: true
            update: true
            environment:
              SOMEVAR: somevalue
            service:
              db:
                image: postgres
              web:
                image: djangoapp
                volumes:
                  - /srv/volumes/django:/srv/django
                ports:
                  - 8000:8000
                depends_on:
                  - db
</code></pre>
</blockquote>

<p>Compose
  ^^^^^^^</p>

<p>Service</p>

<p>Unify Makefile, .gitignore and update readme</p>

<h3 id="pavel-svimbersky">Pavel Svimbersky<a href="#pavel-svimbersky" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Add module for mgmt services inside container</p>

<p>Docker container service management</p>

<p>Enforce the service in container is started</p>

<blockquote>
<pre><code>    contrail_control_started:
</code></pre>

<p>dockerng_service.start:</p>

<pre><code>        - container: f020d0d3efa8
        - service: contrail-control
</code></pre>
</blockquote>

<p>or</p>

<blockquote>
<pre><code>        - container: contrail_controller
</code></pre>
</blockquote>

<p>Enforce the service in container is stoped</p>

<blockquote>
<pre><code>    contrail_control_stoped:
</code></pre>

<p>dockerng_service.stop:</p>
</blockquote>

<p>Enforce the service in container will be restarted</p>

<blockquote>
<pre><code>    contrail_control_restart:
</code></pre>

<p>dockerng_service.restart:</p>
</blockquote>

<p>Enforce the service in container is enabled</p>

<blockquote>
<pre><code>    contrail_control_enable:
</code></pre>

<p>dockerng_service.enable:</p>
</blockquote>

<p>Enforce the service in container is disabled</p>

<blockquote>
<pre><code>    contrail_control_disable:
</code></pre>

<p>dockerng_service.disable:</p>
</blockquote>

<h3 id="richard-felkl-1">Richard Felkl<a href="#richard-felkl-1" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Added client.registry role to mirror docker images</p>

<p>Registry
  ^^^^^^^^</p>

<blockquote>
<pre><code>        registry:
          target_registry: apt:5000
          image:
            - registry: docker
              name: compose:1.8.0
            - registry: tcpcloud
              name: jenkins:latest
            - registry: &quot;&quot;
              name: registry:2
              target_registry: myregistry
</code></pre>
</blockquote>

<h3 id="ruslan-khozinov">Ruslan Khozinov<a href="#ruslan-khozinov" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Allow configure experimental mode</p>

<p>Experimental mode allows to see logs of a swarm service,
  e.g. docker service logs <service_id></p>

<blockquote>
<pre><code>        experimental: true
</code></pre>
</blockquote>

<h3 id="simon-pasquier-3">Simon Pasquier<a href="#simon-pasquier-3" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Add possibility to specify Compose file version</p>

<p>This change allows to set the Compose file version in the model instead
  of the hardcoded value.</p>

<blockquote>
<pre><code>            version: &quot;3.1&quot;
</code></pre>
</blockquote>

<h3 id="tomáš-kukrál-1">Tomáš Kukrál<a href="#tomáš-kukrál-1" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>fix README for insecure registries
  option is called insecure-registries</p>

<blockquote>
<pre><code>          insecure-registries:
</code></pre>
</blockquote>

<p>remove supported OS from README</p>

<p>add support for proxy in docker</p>

<p>Configure proxy for docker host</p>

<blockquote>
<pre><code>      host:
        proxy:
          enabled: true
          http: http://user:pass@proxy:3128
          https: http://user:pass@proxy:3128
          no_proxy:
            - localhost
            - 127.0.0.1
            - docker-registry
</code></pre>
</blockquote>

<p>add support for docker daemon.json configuration</p>

<blockquote>
<pre><code>        options:
          bip: 172.31.255.1/16
          insecure_registries:
            - 10.0.0.1
          log-driver: json-file
          log-opts:
            max-size: 50m
</code></pre>
</blockquote>

<h1 id="formula-dovecot">Formula dovecot</h1>

<h3 id="filip-pytloun-21">Filip Pytloun<a href="#filip-pytloun-21" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Unify Makefile, .gitignore and update readme</p>

<h1 id="formula-elasticsearch">Formula elasticsearch</h1>

<h3 id="filip-pytloun-22">Filip Pytloun<a href="#filip-pytloun-22" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Unify Makefile, .gitignore and update readme</p>

<h3 id="volodymyr-stoiko">Volodymyr Stoiko<a href="#volodymyr-stoiko" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Extend template definition options</p>

<p>This change adds:</p>

<p>Client where you download an index template that is stored in the directory</p>

<p>Client where you download an index template from the metadata definition and force index creation:</p>

<blockquote>
<pre><code>    elasticsearch:
      client:
        enabled: true
        server:
          host: elasticsearch.host
          port: 9200
        index:
          my_index:
            enabled: true
            force_operation: true
            definition:
              template: notifications
              settings:
                number_of_shards: 5
                number_of_replicas: 1
              mappings:
                notification:
                  properties:
                    applicationId:
                      type: long
                    content:
                      type: text
                      fields:
                        keyword:
                          type: keyword
                          ignore_above: 256
</code></pre>
</blockquote>

<h1 id="formula-etcd">Formula etcd</h1>

<h3 id="filip-pytloun-23">Filip Pytloun<a href="#filip-pytloun-23" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Unify Makefile, .gitignore and update readme</p>

<h3 id="mceloud">mceloud<a href="#mceloud" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>change image url param (#7)</p>

<blockquote>
<pre><code>        image: etcd:latest
</code></pre>
</blockquote>

<h3 id="tomáš-kukrál-2">Tomáš Kukrál<a href="#tomáš-kukrál-2" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>copy etcd from images
  when server.image is set then binary is copied from docker image
  etcd Formula</p>

<p>Possible <code>source.engine</code>:</p>

<blockquote>
<pre><code>- **pkg** - install etcd package (default)
- **docker_hybrid** - copy binaries from docker image (specified in `server.image`)
</code></pre>
</blockquote>

<p>etcd proxy</p>

<p>Run etcd on k8s</p>

<p>Copy etcd binary from container</p>

<blockquote>
<pre><code>    etcd:
      server:
        image: quay.io/coreos/etcd:latest
</code></pre>
</blockquote>

<ul>
<li><a href="https://github.com/coreos/etcd">https://github.com/coreos/etcd</a></li>
</ul>

<p>add support for etcd over ssl</p>

<p>Certificates</p>

<p>Use certificate authentication (for peers and clients). Certificates must be prepared in advance.</p>

<blockquote>
<pre><code>        enabled: true
        ssl:
          enabled: true
        bind:
          host: 10.0.175.101
        token: $(uuidgen)
        members:
        - host: 10.0.175.101
          name: etcd01
          port: 4001
</code></pre>
</blockquote>

<h1 id="formula-ffmpeg">Formula ffmpeg</h1>

<h3 id="ales-komarek-5">Ales Komarek<a href="#ales-komarek-5" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Updated docs to rst</p>

<p>ffmpeg formula</p>

<p>A complete, cross-platform solution to record, convert and stream audio and video.</p>

<p>Sample pillars</p>

<blockquote>
<pre><code>    ffmpeg:
      server:
        enabled: true
        input:
          video0:
            source: /dev/video0
            bind:
              host: 192.168.2.1
              port: 8888
            video_format: mjpeg
            width: 640
            height: 480
            format: mpeg
            codec: avi


note: type in your browser http://192.168.2.1:8888/video0.mjpeg
</code></pre>
</blockquote>

<p>Read more</p>

<ul>
<li><a href="https://www.ffmpeg.org/">https://www.ffmpeg.org/</a></li>
</ul>

<h3 id="filip-pytloun-24">Filip Pytloun<a href="#filip-pytloun-24" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Unify Makefile, .gitignore and update readme</p>

<h1 id="formula-flower">Formula flower</h1>

<h3 id="aleš-komárek-7">Aleš Komárek<a href="#aleš-komárek-7" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Update and rename README.md to README.rst</p>

<p>Flower Formula</p>

<p>Flower is a web based tool for monitoring and administrating Celery clusters.</p>

<p>Sample Pillars</p>

<p>Flower single broker</p>

<blockquote>
<pre><code>    flower:
      server:
        enabled: true
        bind:
          port: 5555
          address: 0.0.0.0
        broker:
          engine: redis
          host: localhost
          port: 6379
          number: 0
</code></pre>
</blockquote>

<p>Flower with multiple brokers</p>

<blockquote>
<pre><code>        message_queue:
          location_hklab01:
            bind:
              port: 5555
              address: 0.0.0.0
            broker:
              engine: rabbitmq
              host: localhost
              port: 5672
              virtual_host: /test
              user: test
              password: test
</code></pre>
</blockquote>

<p>Flower with redis broker</p>

<p>More Information</p>

<ul>
<li><a href="https://github.com/mher/flower">https://github.com/mher/flower</a></li>
</ul>

<h1 id="formula-fluentbit">Formula fluentbit</h1>

<h3 id="petr-michalec">Petr Michalec<a href="#petr-michalec" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>cleanup deprecated keys + README</p>

<blockquote>
<pre><code> - service.fluentbit.config.input.metrics
 - service.fluentbit.config.filter.stdout
 - service.fluentbit.config.output.influx
 - service.fluentbit.config.output.forward
</code></pre>
</blockquote>

<p>implement basic functionality</p>

<p>Fluent Bit is an open source and multi-platform Log Forwarder which allows you to collect data/logs from different sources, unify and send them to multiple destinations. It&rsquo;s fully compatible with Docker and Kubernetes environments.</p>

<p><strong>NOTE: WORK IN PROGRES</strong></p>

<blockquote>
<pre><code>NOTE: DESIGN OF THIS FORMULA IS NOT YET STABLE AND MAY CHANGE
  fluentbit:
    enabled: true
    service: &amp;service
      flush: 10
      daemon: 'on'
      log_level: info
      parsers_file: parsers.conf
    service:
</code></pre>

<p>defaults.conf:</p>

<pre><code>        service: \*service
</code></pre>

<p>td-agent-bit.conf:</p>

<pre><code>          - '@INCLUDE defaults.conf'
</code></pre>
</blockquote>

<p>Config files are rendered from these <code>section</code> sections:</p>

<blockquote>
<pre><code>    config:
      input: {}
      output: {}
      parser: {}
      service: {}
      filter: {}
</code></pre>
</blockquote>

<p><code>fluentbit:conf</code> section suport syntax can be yaml, plain text or list.</p>

<p>Yaml structured examples:</p>

<blockquote>
<pre><code>          service:
            flush: 10
          output:
            name: es
            match: '*'


    input:
</code></pre>

<p>metrics.conf:</p>

<pre><code>        cpu:
          name: cpu
          tag: my_cpu
        local_disk:
          name: disk
          tag: storage
</code></pre>

<p>systemd.conf:</p>

<pre><code>        systemd:
          tag: 'host.*'
          systemd_filter:
            - _SYSTEM_UNIT=*.service
            - _SYSTEM_UNIT=*.network
            - _SYSTEM_UNIT=*.boot
    output:
</code></pre>

<p>elastic.conf:</p>

<pre><code>        es:
          match: 'my*'
          name: es


        stdout:
          # name: stdout
    parser:
</code></pre>

<p>custom.conf: |
  @INCLUDE parsers.conf</p>
</blockquote>

<p>Plaint text structured example:</p>

<p>td-agent-bit.conf: |
  [SERVICE]</p>

<p>Daemon off</p>

<p>@SET KEY=VAL</p>

<p>[CUSTOM]
  xyz = aaa</p>

<p>@INCLUDE filters_out.conf</p>

<p>List structured example is used for example for include statemetns in the main <code>td-agent-bit.conf</code> file:</p>

<blockquote>
<pre><code>          - '@INCLUDE metrics_in.conf'
          - '@INCLUDE elastic_out.conf'
</code></pre>
</blockquote>

<p>If the filter key may be specified multiple times, define it as a list.</p>

<blockquote>
<pre><code>      systemd:
        tag: 'host.*'
        systemd_filter:
          - _SYSTEM_UNIT=*.service
          - _SYSTEM_UNIT=*.boot
</code></pre>
</blockquote>

<p>You may use <code>section</code> in any <code>fluentbit:*:*.conf</code> section, but for convenience and clean pillars there is special one <code>mixed</code> for
  config files where you will mix individual sections and setup:</p>

<blockquote>
<pre><code>    mixed:
</code></pre>

<p>proc.conf:</p>

<pre><code>        proc_input:
          section: input
          name: proc
          tag: my_proc
        proc_to_stdout:
          section: output
          name: stdout
</code></pre>
</blockquote>

<p>Sample shared metadata/service pillars</p>

<p>This functionality requires <code>&lt;https://github.com/salt-formulas/reclass&gt;</code>_
  and probably you want to reuse all features of salt-formulas and shared
  system model <code>&lt;https://github.com/Mirantis/reclass-system-salt-model/blob/master/fluentbit&gt;</code>_.</p>

<p>There are most common pre-defined service classes for common input:</p>

<blockquote>
<pre><code>classes:
 - system.fluentbit.single
 # the above should load some of these available:
 - service.fluentbit.support
 - service.fluentbit.config.input.system
 - service.fluentbit.config.output.stdout
</code></pre>

<ul>
<li><a href="http://fluentbit.io/">http://fluentbit.io/</a></li>
</ul>
</blockquote>

<p>initial configuration</p>

<p>fluentbit formula</p>

<p>Service fluentbit description</p>

<p>Sample pillars</p>

<p>Single fluentbit service</p>

<blockquote>
<pre><code>    fluentbit:
      fluentbit:
        enabled: true
        version: icehouse
</code></pre>
</blockquote>

<p>More information</p>

<ul>
<li><p>a link</p></li>

<li><p>links</p></li>
</ul>

<h1 id="formula-fluentd">Formula fluentd</h1>

<h3 id="aleš-komárek-8">Aleš Komárek<a href="#aleš-komárek-8" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Update and rename README.md to README.rst</p>

<p>Fluentd Formula</p>

<p>Many web/mobile applications generate huge amount of event logs
  (c,f. login, logout, purchase, follow, etc). Analyzing these event
  logs can be quite valuable for improving services. However, collecting
  these logs easily and reliably is a challenging task.</p>

<p>Fluentd solves the problem by having: easy installation, small footprint,
  plugins reliable buffering, log forwarding, etc.</p>

<p>Sample Pillars</p>

<blockquote>
<pre><code>      fluentd:
        server:
          enabled: true
          plugins:
          - fluent-plugin-elasticsearch
          - fluent-plugin-mongo
          config:
          - name: forward
            type: input
            bind:
              port: 24224
              host: 0.0.0.0
          - name: elasticsearch
            type: output
              port: 9200
              host: localhost
          - name: mongodb
              port: localhost
              host: localhost
</code></pre>
</blockquote>

<p>More Information</p>

<ul>
<li><a href="http://fluentd.org/">http://fluentd.org/</a></li>
<li><a href="http://docs.fluentd.org/">http://docs.fluentd.org/</a></li>
<li><a href="http://docs.fluentd.org/categories/recipes">http://docs.fluentd.org/categories/recipes</a></li>
</ul>

<h3 id="bartosz-kupidura">Bartosz Kupidura<a href="#bartosz-kupidura" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Fluentd initial commit
  <strong>NOTE: WORK IN PROGRES</strong></p>

<blockquote>
<pre><code>NOTE: DESIGN OF THIS FORMULA IS NOT YET STABLE AND MAY CHANGE
NOTE: FORMULA NOT COMPATIBLE WITH OLD VERSION
</code></pre>
</blockquote>

<p>General pillar structure</p>

<blockquote>
<pre><code>  fluentd:
    config:
      label:
        filename:
          input:
            input_name:
</code></pre>

<p>params</p>

<pre><code>          filter:
            filter_name:


            filter_name2:


          match:
            match_name:


      input:
          input_name:


          input_name2:


        filename2:
          input_name3:


      filter:
          filter_name:


          filter_name2:


          filter_name3:


      match:
          match_name:
</code></pre>
</blockquote>

<p>Example pillar</p>

<blockquote>
<pre><code>    enabled: true
        monitoring:
            parse_log:
              tag: 'docker.monitoring.{alertmanager,remote_storage_adapter,prometheus}.*'
              type: parser
              reserve_data: true
              key_name: log
              parser:
                type: regexp
                format: &gt;-
</code></pre>

<p>/^time=&ldquo;(?<time>[^ ]<em>)&rdquo; level=(?<severity>[a-zA-Z]</em>) msg=&ldquo;(?<message>.+?)&ldquo;/</p>

<pre><code>                time_format: '%FT%TZ'
            remove_log_key:
              type: record_transformer
              remove_keys: log
            docker_log:
              tag: 'docker.**'
              type: file
              path: /tmp/flow-docker.log
        grok_example:
            test_log:
              type: tail
              path: /var/log/test
              tag: test.test
                type: grok
                custom_pattern_path: /etc/td-agent/config.d/global.grok
                rule:
                  - pattern: &gt;-
</code></pre>

<p>%{KEYSTONEACCESS}</p>

<pre><code>        syslog:
            add_severity:
              tag: 'syslog.*'
              enable_ruby: true
              record:
                - name: severity
                  value: 'record[&quot;pri&quot;].to_i - (record[&quot;pri&quot;].to_i / 8).floor * 8'
            severity_to_string:
                  value: '{&quot;debug&quot;=&gt;7,&quot;info&quot;=&gt;6,&quot;notice&quot;=&gt;5,&quot;warning&quot;=&gt;4,&quot;error&quot;=&gt;3,&quot;critical&quot;=&gt;2,&quot;alert&quot;=&gt;1,&quot;emerg&quot;=&gt;0}.key(record[&quot;severity&quot;])'
            severity_for_telegraf:
              tag: 'syslog.*.telegraf'
              key_name: message
</code></pre>

<p>/^(?<time>[^ ]<em>) (?<severity>[A-Z])! (?<message>.</em>)/</p>

<pre><code>            severity_for_telegraf_string:
                  value: '{&quot;debug&quot;=&gt;&quot;D&quot;,&quot;info&quot;=&gt;&quot;I&quot;,&quot;notice&quot;=&gt;&quot;N&quot;,&quot;warning&quot;=&gt;&quot;W&quot;,&quot;error&quot;=&gt;&quot;E&quot;,&quot;critical&quot;=&gt;&quot;C&quot;,&quot;alert&quot;=&gt;&quot;A&quot;,&quot;emerg&quot;=&gt;&quot;E&quot;}.key(record[&quot;severity&quot;])'
            prometheus_metric:
              tag: 'syslog.*.*'
              type: prometheus
              label:
                - name: ident
                  type: variable
                  value: ident
                  value: severity
              metric:
                - name: log_messages
                  type: counter
                  desc: The total number of log messages.
            rewrite_tag_key:
              type: rewrite_tag_filter
              rule:
                  regexp: '^(.*)'
                  result: '__TAG__.$1'
            syslog_log:
              path: /tmp/syslog
          syslog_log:
            type: tail
            label: syslog
            path: /var/log/syslog
            tag: syslog.syslog
            parser:
              type: regexp
              format: &gt;-
</code></pre>

<p>&rsquo;/^&lt;(?<pri>[0-9]+)&gt;(?<time>[^ ]<em>) (?<host>[^ ]</em>) (?<ident>[a-zA-Z0-9_\/.-]<em>)(?:[(?<pid>[0-9]+)])?(?:[^:]</em>:)? <em>(?<message>.</em>)$/&rsquo;</p>

<pre><code>              time_format: '%FT%T.%L%:z'
          auth_log:
            path: /var/log/auth.log
            tag: syslog.auth


        prometheus:
          prometheus:
            type: prometheus
          prometheus_monitor:
            type: prometheus_monitor
          prometheus_output_monitor:
            type: prometheus_output_monitor
        forward:
          forward_listen:
            type: forward
            port: 24224
            bind: 0.0.0.0
        docker_monitoring:
          docker_monitoring:
            tag: 'docker.monitoring.{alertmanager,remote_storage_adapter,prometheus}.*'
            type: relabel
            label: monitoring
</code></pre>
</blockquote>

<h1 id="formula-foreman">Formula foreman</h1>

<h3 id="filip-pytloun-25">Filip Pytloun<a href="#filip-pytloun-25" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Unify Makefile, .gitignore and update readme
  * <a href="http://mauricio.github.io/2014/02/09/foreman-and-environment-variables.html">http://mauricio.github.io/2014/02/09/foreman-and-environment-variables.html</a></p>

<h1 id="formula-freeipa">Formula freeipa</h1>

<h3 id="filip-pytloun-26">Filip Pytloun<a href="#filip-pytloun-26" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Unify Makefile, .gitignore and update readme</p>

<h3 id="james-shewey">James Shewey<a href="#james-shewey" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Filename was incorrect</p>

<p>FreeIPA</p>

<p>This forumla installs and configured the FreeIPA Identity Management service
  and client.</p>

<p>Sample pillars</p>

<p>Client</p>

<blockquote>
<pre><code>    freeipa:
      client:
        enabled: true
        server: ipa.example.com
        domain: {{ salt['grains.get']('domain', '') }}
        realm: {{ salt['grains.get']('domain', '').upper() }}
        hostname: {{ salt['grains.get']('fqdn', '') }}
</code></pre>
</blockquote>

<p>To automatically register the client with FreeIPA, you will first need to
  create a Kerberos principal. Start by creating a service account in FreeIPA.</p>

<p>You may wish to restrict that users permissions to only host creation (see <a href="https://www.freeipa.org/page/HowTos#Working_with_FreeIPA">https://www.freeipa.org/page/HowTos#Working_with_FreeIPA</a>). Next, you will
  need to obtain a kerberos ticket as admin on the IPA server, then generate
  a service account principal.</p>

<p><code>kinit admin</code></p>

<p><code>ipa-getkeytab -p service-account@EXAMPLE.com -k ./principal.keytab -s freeipahost.example.com</code></p>

<p><code>scp ./principal.keytab user@saltmaster.example.com:/srv/salt/freeipa/files/principal.keytab</code></p>

<p>Then add to your pillar:</p>

<blockquote>
<pre><code>        install_principal:
          source: salt://freeipa/files/principal.keytab
          mode: 0600
          principal_user: &quot;service-account&quot;
          file_user: &quot;root&quot;
          file_group: &quot;root&quot;
</code></pre>
</blockquote>

<p>This will allow your client to use FreeIPA&rsquo;s JSON interface to create a host
  entry with a One Time Password and then register to the FreeIPA server. For
  security purposes, the kerberos principal will only be pushed down to the client
  if the installer reports it is not registered to the FreeIPA server and will be
  removed from the client as soon as the endpoint has registered with the FreeIPA
  server.</p>

<p>Additionally, the openssh formula (see</p>

<blockquote>
<pre><code>https://github.com/salt-formulas/salt-formula-openssh) is needed and is a 
</code></pre>

<p>dependency for this formula. Configure it thusly:</p>

<pre><code>    openssh:
      server:
        public_key_auth: true
        gssapi_auth: true
        kerberos_auth: false
        authorized_keys_command:
          command: /usr/bin/sss_ssh_authorizedkeys
          user: nobody
</code></pre>
</blockquote>

<p>If you wish to update DNS records using nsupdate, add:</p>

<blockquote>
<pre><code>        nsupdate:
          - name: test.example.com
            ipv4:
              - 8.8.8.8
            ipv6:
              - 2a00:1450:4001:80a::1009
            ttl: 1800
            keytab: /etc/krb5.keytab
</code></pre>
</blockquote>

<p>For requesting certificates using certmonger:</p>

<blockquote>
<pre><code>        cert:
</code></pre>

<p>&ldquo;HTTP/www.example.com&rdquo;:</p>

<pre><code>            user: root
            group: www-data
            mode: 640
            cert: /etc/ssl/certs/http-www.example.com.crt
            key: /etc/ssl/private/http-www.example.com.key
</code></pre>
</blockquote>

<p>Server</p>

<blockquote>
<pre><code>        realm: IPA.EXAMPLE.COM
        domain: ipa.example.com
        admin:
          password: secretpassword
        ldap:
</code></pre>
</blockquote>

<p>Server definition for new verion of freeipa (4.3+). Replicas dont require
  generation of gpg file on master. But principal user has to be defined with</p>

<blockquote>
<pre><code>        principal_user: admin
        servers:
        - idm01.ipa.example.com
        - idm02.ipa.example.com
        - idm03.ipa.example.com
</code></pre>
</blockquote>

<p>Disable CA. Default is True.</p>

<blockquote>
<pre><code>        ca: false
</code></pre>
</blockquote>

<p>Disable LDAP access logs but enable audit</p>

<blockquote>
<pre><code>          logging:
            access: false
            audit: true
</code></pre>
</blockquote>

<p>Read more</p>

<ul>
<li><a href="http://www.freeipa.org/page/Quick_Start_Guide">http://www.freeipa.org/page/Quick_Start_Guide</a></li>
</ul>

<p>Add automatic host attachment (#2)</p>

<p>First draft of preinstaller module which uses a keytab and JSON interface to programattically execute ipa host-add on the freeipa server to create our own host entry to attach to. I am sure there are bugs.</p>

<blockquote>
<pre><code>  DeprecationWarning: Starting in 2015.5, cmd.run uses python_shell=False by default, which doesn't support shellisms (pipes, env variables, etc). cmd.run is currently aliased to cmd.shell to prevent breakage. Please switch to cmd.shell or set python_shell=True to avoid breakage in the future, when this aliasing is removed.
</code></pre>
</blockquote>

<p>This reverts commit 0573511f430f1fd2c7ea7c62c506e0de78d4ff4d.</p>

<p>This reverts commit dd6f14fd80496f8e9449ac9d18be15995a3b40c5.</p>

<p>This reverts commit a93b43bc120f6da48cfd70c0a88ecb1f45fb019b.</p>

<p>This reverts commit 9f15c957d3e5d4706dae13df6a4008230b2b590e.</p>

<p>This reverts commit 64bb556864087310f5e0256268d2a0238e7ba579.</p>

<p>This reverts commit bd85a2855146541757eacabb4f897e2ee0db699f.</p>

<p>This reverts commit a812734a659d340cbdc8027d56ca854daaee2b17.</p>

<p>Needed to specify file user permissions separate from principal permissions because user doesn&rsquo;t exist on the system yet, so can&rsquo;t be the file owner.</p>

<p>Verified on my setup</p>

<h3 id="ondrej-smola-3">Ondrej Smola<a href="#ondrej-smola-3" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>added support for new version of freeipa cluster installation</p>

<p>Server definition for new verion of freeipa (4.3+). Replicas dont require generation of gpg file on master. But principal user has to be defined with</p>

<h1 id="formula-galera">Formula galera</h1>

<h3 id="dmitry-kalashnik">Dmitry Kalashnik<a href="#dmitry-kalashnik" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Add new soft parameters</p>

<p>Make max_connections and innodb_buffer_pool_size configurable
  through soft params instead of hard.</p>

<p>Configurable soft parameters</p>

<blockquote>
<pre><code>- **galera_innodb_buffer_pool_size** - the default value is 3138M
- **galera_max_connections** - the default value is 20000
    _param:
      galera_innodb_buffer_pool_size: 1024M
      galera_max_connections: 200 
</code></pre>
</blockquote>

<h3 id="filip-pytloun-27">Filip Pytloun<a href="#filip-pytloun-27" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Unify Makefile, .gitignore and update readme</p>

<h3 id="kirill-bespalov-2">Kirill Bespalov<a href="#kirill-bespalov-2" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Galera cluster TLS Support</p>

<p>Enable TLS support:</p>

<blockquote>
<pre><code>    galera:
</code></pre>

<p>slave or master:</p>

<pre><code>         ssl:
          enabled: True


          # path
          cert_file: /etc/mysql/ssl/cert.pem
          key_file: /etc/mysql/ssl/key.pem
          ca_file: /etc/mysql/ssl/ca.pem


          # content (not required if files already exists)
          key: &lt;&lt; body of key &gt;&gt;
          cert: &lt;&lt; body of cert &gt;&gt;
          cacert_chain: &lt;&lt; body of ca certs chain &gt;&gt;


      galera_max_connections: 200
</code></pre>
</blockquote>

<p>Make innodb_buffer_pool_size configurable</p>

<blockquote>
<pre><code>  Usage:
</code></pre>
</blockquote>

<p>If innodb_buffer_pool_size is not set then 35% of the available
  ram will be allocated for the buffer pool.</p>

<p>InnoDB parameters</p>

<blockquote>
<pre><code>- **innodb_buffer_pool_size** - the default value is 35% of the available ram


Usage:


        master:
          innodb_buffer_pool_size: 1024M
        slave:
</code></pre>
</blockquote>

<p>Enter current password for root (enter for none):</p>

<p>New password:</p>

<p>Re-enter new password:
  6. uncomment all wsrep* lines except first server, where leave only in my.cnf wsrep_cluster_address=&lsquo;gcomm://&lsquo;;</p>

<h3 id="petr-michalec-1">Petr Michalec<a href="#petr-michalec-1" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Version specified on pillar</p>

<blockquote>
<pre><code>      version:
        mysql: 5.6
        galera: 3
</code></pre>
</blockquote>

<p>add clustercheck script (avoid splitbrain)</p>

<blockquote>
<pre><code>                - grant_option: True
</code></pre>
</blockquote>

<p>Additional check params:</p>

<blockquote>
<pre><code>      clustercheck:
        - enabled: True
        - user: clustercheck
        - password: clustercheck
        - available_when_donor: 0
        - available_when_readonly: 1
        - port 9200
</code></pre>
</blockquote>

<p>Additional users</p>

<p>Additional mysql users:</p>

<blockquote>
<pre><code>    mysql:
      server:
        users:
          - name: clustercheck
            password: clustercheck
            database: '*.*'
            grants: PROCESS
            grant_option: False
          - name: inspector
            host: 127.0.0.1
            password: password
            databases:
              mydb:
                - database: mydb
                - table: mytable
                - grant_option: False
                - grants:
                  - all privileges
</code></pre>
</blockquote>

<h1 id="formula-galera2">Formula galera2</h1>

<h3 id="dmitry-kalashnik-1">Dmitry Kalashnik<a href="#dmitry-kalashnik-1" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Add new soft parameters</p>

<p>Make max_connections and innodb_buffer_pool_size configurable
  through soft params instead of hard.</p>

<p>Configurable soft parameters</p>

<blockquote>
<pre><code>- **galera_innodb_buffer_pool_size** - the default value is 3138M
- **galera_max_connections** - the default value is 20000
    _param:
      galera_innodb_buffer_pool_size: 1024M
      galera_max_connections: 200 
</code></pre>
</blockquote>

<h3 id="filip-pytloun-28">Filip Pytloun<a href="#filip-pytloun-28" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Unify Makefile, .gitignore and update readme</p>

<h3 id="kirill-bespalov-3">Kirill Bespalov<a href="#kirill-bespalov-3" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Galera cluster TLS Support</p>

<p>Enable TLS support:</p>

<blockquote>
<pre><code>    galera:
</code></pre>

<p>slave or master:</p>

<pre><code>         ssl:
          enabled: True


          # path
          cert_file: /etc/mysql/ssl/cert.pem
          key_file: /etc/mysql/ssl/key.pem
          ca_file: /etc/mysql/ssl/ca.pem


          # content (not required if files already exists)
          key: &lt;&lt; body of key &gt;&gt;
          cert: &lt;&lt; body of cert &gt;&gt;
          cacert_chain: &lt;&lt; body of ca certs chain &gt;&gt;


      galera_max_connections: 200
</code></pre>
</blockquote>

<p>Make innodb_buffer_pool_size configurable</p>

<blockquote>
<pre><code>  Usage:
</code></pre>
</blockquote>

<p>If innodb_buffer_pool_size is not set then 35% of the available
  ram will be allocated for the buffer pool.</p>

<p>InnoDB parameters</p>

<blockquote>
<pre><code>- **innodb_buffer_pool_size** - the default value is 35% of the available ram


Usage:


        master:
          innodb_buffer_pool_size: 1024M
        slave:
</code></pre>
</blockquote>

<p>Enter current password for root (enter for none):</p>

<p>New password:</p>

<p>Re-enter new password:
  6. uncomment all wsrep* lines except first server, where leave only in my.cnf wsrep_cluster_address=&lsquo;gcomm://&lsquo;;</p>

<h3 id="petr-michalec-2">Petr Michalec<a href="#petr-michalec-2" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Version specified on pillar</p>

<blockquote>
<pre><code>      version:
        mysql: 5.6
        galera: 3
</code></pre>
</blockquote>

<p>add clustercheck script (avoid splitbrain)</p>

<blockquote>
<pre><code>                - grant_option: True
</code></pre>
</blockquote>

<p>Additional check params:</p>

<blockquote>
<pre><code>      clustercheck:
        - enabled: True
        - user: clustercheck
        - password: clustercheck
        - available_when_donor: 0
        - available_when_readonly: 1
        - port 9200
</code></pre>
</blockquote>

<p>Additional users</p>

<p>Additional mysql users:</p>

<blockquote>
<pre><code>    mysql:
      server:
        users:
          - name: clustercheck
            password: clustercheck
            database: '*.*'
            grants: PROCESS
            grant_option: False
          - name: inspector
            host: 127.0.0.1
            password: password
            databases:
              mydb:
                - database: mydb
                - table: mytable
                - grant_option: False
                - grants:
                  - all privileges
</code></pre>
</blockquote>

<h1 id="formula-galera3">Formula galera3</h1>

<h3 id="dmitry-kalashnik-2">Dmitry Kalashnik<a href="#dmitry-kalashnik-2" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Add new soft parameters</p>

<p>Make max_connections and innodb_buffer_pool_size configurable
  through soft params instead of hard.</p>

<p>Configurable soft parameters</p>

<blockquote>
<pre><code>- **galera_innodb_buffer_pool_size** - the default value is 3138M
- **galera_max_connections** - the default value is 20000
    _param:
      galera_innodb_buffer_pool_size: 1024M
      galera_max_connections: 200 
</code></pre>
</blockquote>

<h3 id="filip-pytloun-29">Filip Pytloun<a href="#filip-pytloun-29" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Unify Makefile, .gitignore and update readme</p>

<h3 id="kirill-bespalov-4">Kirill Bespalov<a href="#kirill-bespalov-4" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Galera cluster TLS Support</p>

<p>Enable TLS support:</p>

<blockquote>
<pre><code>    galera:
</code></pre>

<p>slave or master:</p>

<pre><code>         ssl:
          enabled: True


          # path
          cert_file: /etc/mysql/ssl/cert.pem
          key_file: /etc/mysql/ssl/key.pem
          ca_file: /etc/mysql/ssl/ca.pem


          # content (not required if files already exists)
          key: &lt;&lt; body of key &gt;&gt;
          cert: &lt;&lt; body of cert &gt;&gt;
          cacert_chain: &lt;&lt; body of ca certs chain &gt;&gt;


      galera_max_connections: 200
</code></pre>
</blockquote>

<p>Make innodb_buffer_pool_size configurable</p>

<blockquote>
<pre><code>  Usage:
</code></pre>
</blockquote>

<p>If innodb_buffer_pool_size is not set then 35% of the available
  ram will be allocated for the buffer pool.</p>

<p>InnoDB parameters</p>

<blockquote>
<pre><code>- **innodb_buffer_pool_size** - the default value is 35% of the available ram


Usage:


        master:
          innodb_buffer_pool_size: 1024M
        slave:
</code></pre>
</blockquote>

<p>Enter current password for root (enter for none):</p>

<p>New password:</p>

<p>Re-enter new password:
  6. uncomment all wsrep* lines except first server, where leave only in my.cnf wsrep_cluster_address=&lsquo;gcomm://&lsquo;;</p>

<h3 id="petr-michalec-3">Petr Michalec<a href="#petr-michalec-3" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>add clustercheck script (avoid splitbrain)</p>

<blockquote>
<pre><code>                - grant_option: True
</code></pre>
</blockquote>

<p>Additional check params:</p>

<blockquote>
<pre><code>      clustercheck:
        - enabled: True
        - user: clustercheck
        - password: clustercheck
        - available_when_donor: 0
        - available_when_readonly: 1
        - port 9200
</code></pre>
</blockquote>

<p>Additional users</p>

<p>Additional mysql users:</p>

<blockquote>
<pre><code>    mysql:
      server:
        users:
          - name: clustercheck
            password: clustercheck
            database: '*.*'
            grants: PROCESS
            grant_option: False
          - name: inspector
            host: 127.0.0.1
            password: password
            databases:
              mydb:
                - database: mydb
                - table: mytable
                - grant_option: False
                - grants:
                  - all privileges
</code></pre>
</blockquote>

<h1 id="formula-gateone">Formula gateone</h1>

<h3 id="aleš-komárek-9">Aleš Komárek<a href="#aleš-komárek-9" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Update README.rst</p>

<p>GateOne Formula</p>

<p>Sample Pillars</p>

<p>More Information</p>

<h3 id="filip-pytloun-30">Filip Pytloun<a href="#filip-pytloun-30" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Unify Makefile, .gitignore and update readme</p>

<h1 id="formula-gerrit">Formula gerrit</h1>

<h3 id="alexander-noskov-2">Alexander Noskov<a href="#alexander-noskov-2" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Add possibility to inherit access for project from another project</p>

<blockquote>
<pre><code>            inherit_access: All-Projects
</code></pre>
</blockquote>

<p>By default &ldquo;Submit Type&rdquo; is: merge if necessary</p>

<blockquote>
<pre><code>            action: &quot;fast forward only&quot;
</code></pre>
</blockquote>

<h3 id="filip-pytloun-31">Filip Pytloun<a href="#filip-pytloun-31" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Unify Makefile, .gitignore and update readme</p>

<h1 id="formula-git">Formula git</h1>

<h3 id="ales-komarek-6">Ales Komarek<a href="#ales-komarek-6" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Docfixes</p>

<h3 id="botond-zoltán">Botond Zoltán<a href="#botond-zoltán" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Add SSL varification related examples</p>

<p>This change adds the SSL verification related examples to the
  readme file.</p>

<p>GIT with user and SSL setup</p>

<blockquote>
<pre><code>    git:
      client:
        disable_ssl_verification: True
        enabled: true
        user:
        - user:
            name: jdoe
            email: j@doe.com
</code></pre>
</blockquote>

<p>Reclass with GIT with user and SSL setup</p>

<blockquote>
<pre><code>        - user: ${linux:system:user:jdoe}
</code></pre>
</blockquote>

<h3 id="filip-pytloun-32">Filip Pytloun<a href="#filip-pytloun-32" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Unify Makefile, .gitignore and update readme</p>

<h3 id="richard-felkl-2">Richard Felkl<a href="#richard-felkl-2" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>added support for custom git repos + mirror fix</p>

<blockquote>
<pre><code>          - name: custom-repo-1
          - name: custom-repo-2
</code></pre>
</blockquote>

<p>Reclass with GIT over HTTP server setup. Requires web server. Mirrored upsream repos example.</p>

<blockquote>
<pre><code>      server:
        directory: /srv/git
        repos:
</code></pre>
</blockquote>

<p>introduced git.server role</p>

<p>Reclass with GIT over HTTP server setup. Requires web server.</p>

<blockquote>
<pre><code>          - name: gerritlib
            url: https://github.com/openstack-infra/gerritlib.git
          - name: jeepyb
            url: https://github.com/openstack-infra/jeepyb.git
</code></pre>
</blockquote>

<h1 id="formula-gitlab">Formula gitlab</h1>

<h3 id="ales-komarek-7">Ales Komarek<a href="#ales-komarek-7" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>doc fixes</p>

<p>Gitlab server with local MTA and PostgreSQL database</p>

<blockquote>
<pre><code>          engine: 'postgresql'
        server_name: 'repo1.domain.com'
</code></pre>
</blockquote>

<p>Gitlab repository enforcement with import url repository and deploy keys and
  hooks.</p>

<p>docfixes</p>

<p>Sample metadata</p>

<blockquote>
<pre><code>        source:
          engine: pkg
          from: 'gitlab@domain.com'
          no_reply: 'no-reply@domain.com'
        server_name: 'repo.domain.com'
</code></pre>
</blockquote>

<p>More information</p>

<p>Documentation cleanup</p>

<p>Gitlab formula</p>

<p>Gitlab is a free git repository management application based on Ruby on Rails.</p>

<p>It is distributed under the MIT License and its source code can be found on</p>

<p>Github. It is a very active project with a monthly release cycle and ideal for
  businesses that want to keep their code private. Consider it as a self hosted</p>

<p>Github but open source.</p>

<p>Server role</p>

<p>Client role</p>

<h3 id="filip-pytloun-33">Filip Pytloun<a href="#filip-pytloun-33" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Unify Makefile, .gitignore and update readme</p>

<h1 id="formula-gitolite">Formula gitolite</h1>

<h3 id="filip-pytloun-34">Filip Pytloun<a href="#filip-pytloun-34" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Fix readme</p>

<blockquote>
<pre><code>    server:
      user:
        testusr:
          key: ssh-rsa ...
      group:
        admin:
          members:
            - testusr
      repository:
        gitolite-admin:
          name: gitolite-admin
          permission:
</code></pre>

<p>&ldquo;RW+&rdquo;:</p>

<pre><code>              - &quot;@admin&quot;
</code></pre>

<p>&ldquo;RW (?!master|develop|release/)&rdquo;:</p>

<pre><code>              - testusr
</code></pre>
</blockquote>

<p>Initial commit</p>

<p>gitolite formula</p>

<p>Service gitolite description</p>

<p>Sample pillars</p>

<p>Single gitolite service</p>

<blockquote>
<pre><code>server:
  user:
    testusr:
      key: ssh-rsa ...
  group:
    admin:
      members:
        - testusr
  repository:
    gitolite-admin:
      name: gitolite-admin
      permission:


          - &quot;@admin&quot;


          - testusr
</code></pre>
</blockquote>

<p>More information</p>

<ul>
<li><p><a href="http://gitolite.com/gitolite/index.html">http://gitolite.com/gitolite/index.html</a></p></li>

<li><p>links</p></li>
</ul>

<h1 id="formula-glance">Formula glance</h1>

<h3 id="aleš-komárek-10">Aleš Komárek<a href="#aleš-komárek-10" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Update README.rst</p>

<p>Glance formula</p>

<p>Sample pillars</p>

<p>Keystone and cinder region</p>

<blockquote>
<pre><code>    glance:
      server:
        enabled: true
        version: kilo
</code></pre>

<p>&hellip;</p>

<pre><code>        identity:
          engine: keystone
          host: 127.0.0.1
          region: RegionTwo
</code></pre>
</blockquote>

<p>Ceph integration glance</p>

<blockquote>
<pre><code>        version: juno
        storage:
          engine: rbd,http
          user: glance
          pool: images
          chunk_size: 8
          client_glance_key: AQDOavlU6BsSJhAAnpFR906mvdgdfRqLHwu0Uw==
</code></pre>
</blockquote>

<p>RabbitMQ HA setup</p>

<p>Enable auditing filter (CADF):</p>

<p>Client role</p>

<p>Glance images</p>

<blockquote>
<pre><code>  glance:
    client:
      enabled: true
        profile_admin:
          image:
            cirros-test:
              visibility: public
              protected: false
              location: http://download.cirros-cloud.net/0.3.4/cirros-0.3.4-i386-disk.img
</code></pre>
</blockquote>

<p>Usage</p>

<p>Import new public image</p>

<blockquote>
<pre><code>    glance image-create --name 'Windows 7 x86_64' --is-public true --container-format bare --disk-format qcow2  &lt; ./win7.qcow2
</code></pre>
</blockquote>

<p>Change new image&rsquo;s disk properties</p>

<blockquote>
<pre><code>    glance image-update &quot;Windows 7 x86_64&quot; --property hw_disk_bus=ide
</code></pre>
</blockquote>

<p>Change new image&rsquo;s NIC properties</p>

<blockquote>
<pre><code>    glance image-update &quot;Windows 7 x86_64&quot; --property hw_vif_model=rtl8139
</code></pre>
</blockquote>

<p>External links</p>

<h3 id="dmitry-stremkovskiy-1">Dmitry Stremkovskiy<a href="#dmitry-stremkovskiy-1" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Unhardcode glance user/group uid/gid values</p>

<p>Warn to not change glance uid/gid values after user is created</p>

<blockquote>
<pre><code>        glance_uid: 302
        glance_gid: 302
</code></pre>
</blockquote>

<h3 id="dmitry-ukov-2">Dmitry Ukov<a href="#dmitry-ukov-2" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Policy.json should be defined by user</p>

<p>User can override and add values to policy.json by creating flat
  key-value structure under glance:server:policy.</p>

<p>Configuration of policy.json file</p>

<p>&hellip;.</p>

<blockquote>
<pre><code>        policy:
          publicize_image: &quot;role:admin&quot;
          # Add key without value to remove line from policy.json
          add_member:
</code></pre>
</blockquote>

<h3 id="filip-pytloun-35">Filip Pytloun<a href="#filip-pytloun-35" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Unify Makefile, .gitignore and update readme</p>

<h3 id="kirill-bespalov-5">Kirill Bespalov<a href="#kirill-bespalov-5" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>RabbitMQ TLS support</p>

<p>OSCORE-380</p>

<blockquote>
<pre><code>  Releases: Mitaka, Newton, Ocata
  Usage: see README.rst
</code></pre>
</blockquote>

<p>Client-side RabbitMQ TLS configuration:</p>

<p>To enable TLS for oslo.messaging you need to provide the CA certificate.</p>

<p>By default system-wide CA certs are used. Nothing should be specified except <code>ssl.enabled</code>.</p>

<blockquote>
<pre><code>    server:


      message_queue:
        ssl:
          enabled: True
</code></pre>
</blockquote>

<p>Use <code>cacert_file</code> option to specify the CA-cert file path explicitly:</p>

<blockquote>
<pre><code>          cacert_file: /etc/ssl/rabbitmq-ca.pem
</code></pre>
</blockquote>

<p>To manage content of the <code>cacert_file</code> use the <code>cacert</code> option:</p>

<blockquote>
<pre><code>          cacert: |


          cacert_file: /etc/openstack/rabbitmq-ca.pem


Notice:
</code></pre>

<ul>
<li>The <code>message_queue.port</code> is set to <strong>5671</strong> (AMQPS) by default if <code>ssl.enabled=True</code>.</li>
<li>Use <code>message_queue.ssl.version</code> if you need to specify protocol version. By default is TLSv1 for python &lt; 2.7.9 and TLSv1_2 for version above.</li>
</ul>
</blockquote>

<h3 id="michel-nederlof-1">Michel Nederlof<a href="#michel-nederlof-1" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Add support for showing / returing multiple locations (#8)</p>

<p>Enable Viewing Multiple Locations</p>

<p>If you want to expose all locations available (for example when you have
  multiple backends configured), then you can configure this like so:</p>

<blockquote>
<pre><code>        show_multiple_locations: True
        location_strategy: store_type
        store_type_preference: rbd,swift,file
</code></pre>
</blockquote>

<p>Please note: the show_multiple_locations option is deprecated since Newton and is planned
  to be handled by policy files <em>only</em> starting with the Pike release.</p>

<p>This feature is convenient in a scenario when you have swift and rbd configured and want to
  benefit from rbd enhancements.</p>

<p>Adding support for (multiple) swift backends (#7)</p>

<p>Another way, which also supports multiple swift backends, can be configured like this:</p>

<blockquote>
<pre><code>        version: mitaka
          engine: swift,http
          swift:
            store:
              endpoint_type: publicURL
              container: glance
              create_container_on_put: true
              retry_get_count: 5
              references:
                my_objectstore_reference_1:
                  auth:
                    address: http://keystone.example.com:5000/v2.0
                    version: 2
                  user: 2ec7966596504f59acc3a76b3b9d9291:glance-user
                  key: someRandomPassword
</code></pre>
</blockquote>

<h3 id="mnederlof">mnederlof<a href="#mnederlof" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Allow ability to use image cache (#5)</p>

<p>Enable Glance Image Cache:</p>

<blockquote>
<pre><code>        image_cache:
          enabled: true
          enable_management: true
          directory: /var/lib/glance/image-cache/
          max_size: 21474836480
</code></pre>
</blockquote>

<h3 id="oleg-iurchenko-2">Oleg Iurchenko<a href="#oleg-iurchenko-2" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Add Barbican integration</p>

<p>This patch adds Barbican integration to Glance</p>

<blockquote>
<pre><code>        barbican:
</code></pre>
</blockquote>

<p>Barbican integration glance</p>

<blockquote>
<pre><code>          barbican:
            enabled: true
</code></pre>
</blockquote>

<h3 id="ondrej-smola-4">Ondrej Smola<a href="#ondrej-smola-4" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>enable support for cors params</p>

<p>Enable CORS parameters</p>

<blockquote>
<pre><code>        cors:
          allowed_origin: https:localhost.local,http:localhost.local
          expose_headers: X-Auth-Token,X-Openstack-Request-Id,X-Subject-Token
          allow_methods: GET,PUT,POST,DELETE,PATCH
          allow_headers: X-Auth-Token,X-Openstack-Request-Id,X-Subject-Token
          allow_credentials: True
          max_age: 86400
</code></pre>
</blockquote>

<h3 id="petr-michalec-4">Petr Michalec<a href="#petr-michalec-4" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>update readme, cirros image url and add it to tests</p>

<blockquote>
<pre><code>        - name: &quot;CirrOS&quot;
          file: cirros-0.4.0-x86_64-disk.img
          source: https://download.cirros-cloud.net/0.4.0/cirros-0.4.0-x86_64-disk.img
          visibility: true
</code></pre>
</blockquote>

<h3 id="richard-felkl-3">Richard Felkl<a href="#richard-felkl-3" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Introduce glance client role</p>

<h3 id="robertjansen1">RobertJansen1<a href="#robertjansen1" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>swift as glance backend (#2)</p>

<p>This reverts commit 4fdd899db23cdb9e26c158b74397ede79495a136.</p>

<p>Swift integration glance</p>

<blockquote>
<pre><code>              auth:
                address: http://keystone.example.com:5000/v2.0
                version: 2
              user: 2ec7966596504f59acc3a76b3b9d9291:glance-user
              key: someRandomPassword
</code></pre>
</blockquote>

<h3 id="simon-pasquier-4">Simon Pasquier<a href="#simon-pasquier-4" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Allow to configure the pagination parameters</p>

<blockquote>
<pre><code>        api_limit_max: 100
        limit_param_default: 50
</code></pre>
</blockquote>

<p>The pagination is controlled by the <em>api_limit_max</em> and <em>limit_param_default</em>
  parameters as shown above:</p>

<ul>
<li><p><em>api_limit_max</em> defines the maximum number of records that the server will
return.</p></li>

<li><p><em>limit_param_default</em> is the default <em>limit</em> parameter that
applies if the request didn&rsquo;t defined it explicitly.</p></li>
</ul>

<h1 id="formula-glusterfs">Formula glusterfs</h1>

<h3 id="filip-pytloun-36">Filip Pytloun<a href="#filip-pytloun-36" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Unify Makefile, .gitignore and update readme</p>

<h1 id="formula-gnocchi">Formula gnocchi</h1>

<h1 id="formula-gource">Formula gource</h1>

<h3 id="filip-pytloun-37">Filip Pytloun<a href="#filip-pytloun-37" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Unify Makefile, .gitignore and update readme</p>

<h1 id="formula-grafana">Formula grafana</h1>

<h3 id="filip-pytloun-38">Filip Pytloun<a href="#filip-pytloun-38" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Unify Makefile, .gitignore and update readme</p>

<h3 id="guillaume-thouvenin">Guillaume Thouvenin<a href="#guillaume-thouvenin" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Allow the installation of Grafana plugins</p>

<p>Server with two additionals plugins. It requires to have access to the Internet.</p>

<blockquote>
<pre><code>    grafana:
      server:
        enabled: true
        plugins:
          grafana-piechart-panel:
            enabled: true
          grafana-example-app:
</code></pre>
</blockquote>

<h1 id="formula-graphite">Formula graphite</h1>

<h3 id="filip-pytloun-39">Filip Pytloun<a href="#filip-pytloun-39" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Unify Makefile, .gitignore and update readme</p>

<h1 id="formula-haproxy">Formula haproxy</h1>

<h3 id="adam-tengler">Adam Tengler<a href="#adam-tengler" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Add support for tcp-check and http-check configuration</p>

<p>Custom listener with tcp-check options specified (for Redis cluster with Sentinel)</p>

<blockquote>
<pre><code>  haproxy:
    proxy:
      listen:
        redis_cluster:
          service_name: redis
          check:
            tcp:
              enabled: True
              options:
                - send PING\r\n
                - expect string +PONG
                - send info\ replication\r\n
                - expect string role:master
                - send QUIT\r\n
                - expect string +OK
          binds:
            - address: ${_param:cluster_address}
              port: 6379
          servers:
            - name: ${_param:cluster_node01_name}
              host: ${_param:cluster_node01_address}
              params: check inter 1s
            - name: ${_param:cluster_node02_name}
              host: ${_param:cluster_node02_address}
            - name: ${_param:cluster_node03_name}
              host: ${_param:cluster_node03_address}
</code></pre>
</blockquote>

<h3 id="brian-mcqueen">Brian McQueen<a href="#brian-mcqueen" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>readme to change listens to listen and bind to binds</p>

<blockquote>
<pre><code>        listen:
          https-in:
            binds:
            - address: 0.0.0.0
              port: 443
            servers:
            - name: server1
              host: 10.0.0.1
              port: 8443
            - name: server2
              host: 10.0.0.2
              params: 'maxconn 256'
              address: 0.0.0.0
          listen:
            mysql:
              type: mysql
              binds:
              - address: 10.0.88.70
                port: 3306
              servers:
              - name: node1
                host: 10.0.88.13
                params: check inter 15s fastinter 2s downinter 1s rise 5 fall 3
              - name: node2
                host: 10.0.88.14
                params: check inter 15s fastinter 2s downinter 1s rise 5 fall 3 backup
              - name: node3
                host: 10.0.88.15
            rabbitmq:
              type: rabbitmq
                port: 5672
                port: 5673
                params: check inter 5000 rise 2 fall 3
                params: check inter 5000 rise 2 fall 3 backup
            keystone-1:
              type: general-service
              - address: 10.0.106.170
                port: 5000
                params: check
</code></pre>
</blockquote>

<h3 id="filip-pytloun-40">Filip Pytloun<a href="#filip-pytloun-40" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Add support for deploying SSL certificates</p>

<p>It&rsquo;s also possible to use multiple certificates for one listener (eg. when
  it&rsquo;s bind on multiple interfaces):</p>

<blockquote>
<pre><code>    haproxy:
      proxy:
          dummy_site:
            mode: http
              - address: 127.0.0.1
                port: 8080
                ssl:
                  enabled: true
                  key: |
</code></pre>

<p>my super secret key follows</p>

<pre><code>                  cert: |
</code></pre>

<p>certificate</p>

<pre><code>                  chain: |
</code></pre>
</blockquote>

<p>CA chain (if any)</p>

<blockquote>
<pre><code>              - address: 127.0.1.1
                port: 8081
</code></pre>
</blockquote>

<p>Definition above will result in creation of <code>/etc/haproxy/ssl/dummy_site</code>
  directory with files <code>1-all.pem</code> and <code>2-all.pem</code> (per binds).</p>

<p>Unify Makefile, .gitignore and update readme</p>

<h3 id="guillaume-thouvenin-1">Guillaume Thouvenin<a href="#guillaume-thouvenin-1" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Renamed the key &lsquo;check&rsquo; into &lsquo;health-check&rsquo;</p>

<p>The key &lsquo;check&rsquo; is already used in meta/heka.yml to enable/disable the
  monitoring of the haproxy backends.</p>

<blockquote>
<pre><code>          health-check:
</code></pre>
</blockquote>

<h3 id="ildar-svetlov">Ildar Svetlov<a href="#ildar-svetlov" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Add customizable forwardfor option</p>

<p>The Reliable, High Performance TCP/HTTP Load Balancer.</p>

<blockquote>
<pre><code>              params: check inter 5000 rise 2 fall 3
</code></pre>
</blockquote>

<p>Enable customisable <code>forwardfor</code> option in <code>defaults</code> section.</p>

<blockquote>
<pre><code>      enabled: true
      mode: tcp
      logging: syslog
      max_connections: 1024
      forwardfor:
        enabled: true
        except:
        header:
        if-none: false


        except: 127.0.0.1
        header: X-Real-IP
</code></pre>
</blockquote>

<h3 id="sergey-otpuschennikov">Sergey Otpuschennikov<a href="#sergey-otpuschennikov" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Add ability to create redirect</p>

<p>Frontend for routing between exists listeners via URL with SSL an redirects.</p>

<p>You can use one backend for several URLs.</p>

<blockquote>
<pre><code>        service_proxy:
          mode: http
          balance: source
          format: end
           - address: ${_param:haproxy_bind_address}
             port: 80
             ssl: ${_param:haproxy_frontend_ssl}
             ssl_port: 443
          redirects:
           - code: 301
             location: domain.com/images
             conditions:
               - type: hdr_dom(host)
                 condition: images.domain.com
          acls:
           - name: gerrit
                 condition: gerrit.domain.com
           - name: jenkins
                 condition: jenkins.domain.com
           - name: docker
             backend: artifactroy
                 condition: docker.domain.com
</code></pre>
</blockquote>

<h1 id="formula-heat">Formula heat</h1>

<h3 id="aleš-komárek-11">Aleš Komárek<a href="#aleš-komárek-11" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Update README.rst</p>

<p>Heat Formula</p>

<p>Sample Pillars</p>

<h3 id="dennis-dmitriev">Dennis Dmitriev<a href="#dennis-dmitriev" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Allow keystone endpoint_type interpolation for Heat clients</p>

<blockquote>
<pre><code>          endpoint_type_default: internalURL
          endpoint_type_heat: publicURL
</code></pre>
</blockquote>

<h3 id="dmitry-stremkouski-1">Dmitry Stremkouski<a href="#dmitry-stremkouski-1" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>One should have an ability to set up max_nested_stack_depth option</p>

<p>Customer requires from us to set up max_nested_stack_depth in
  heat.conf.</p>

<p>This option is not enabled and has value of 5 by default.</p>

<p>We need to enable this option in templates so customer would be
  able to set it to a desired value.</p>

<p>Customer-Found</p>

<p>PROD-15878</p>

<blockquote>
<pre><code>        max_nested_stack_depth: 10
</code></pre>
</blockquote>

<p>One should have an ability to set up max_stacks_per_tenant option</p>

<p>Customer requires from us to set up max_stacks_per_tenant in</p>

<p>This option is not enabled and has value of 100 by default.</p>

<p>PROD-15877</p>

<blockquote>
<pre><code>        max_stacks_per_tenant: 150
</code></pre>
</blockquote>

<h3 id="dmitry-ukov-3">Dmitry Ukov<a href="#dmitry-ukov-3" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Policy.json should be defined by user</p>

<p>User can override and add values to policy.json by creating flat
  key-value structure under heat:server:policy.</p>

<p>Configuration of policy.json file</p>

<blockquote>
<pre><code>    heat:
      server:
</code></pre>

<p>&hellip;.</p>

<pre><code>        policy:
          deny_stack_user: 'not role:heat_stack_user'
</code></pre>

<p>&lsquo;cloudformation:ValidateTemplate&rsquo;: &lsquo;rule:deny_stack_user&rsquo;</p>

<pre><code>          # Add key without value to remove line from policy.json
</code></pre>

<p>&lsquo;cloudformation:DescribeStackResource&rsquo;:</p>
</blockquote>

<h3 id="filip-pytloun-41">Filip Pytloun<a href="#filip-pytloun-41" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Unify Makefile, .gitignore and update readme</p>

<blockquote>
<pre><code>    https://wiki.openstack.org/wiki/Meetings/openstack-salt
</code></pre>
</blockquote>

<h3 id="jiri-broulik-4">Jiri Broulik<a href="#jiri-broulik-4" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>iterate and enforce multiple git repos</p>

<blockquote>
<pre><code>        template:
          admin:
            domain: default
            source:
              engine: git
              address: git@repo.domain.com/admin-templates.git
              revision: master
          default:
              address: git@repo.domain.com/default-templates.git
</code></pre>
</blockquote>

<h3 id="kirill-bespalov-6">Kirill Bespalov<a href="#kirill-bespalov-6" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>RabbitMQ TLS support</p>

<p>OSCORE-387</p>

<blockquote>
<pre><code>  Releases: Mitaka, Newton, Ocata
  Usage: see README.rst
</code></pre>
</blockquote>

<p>Heat system definition of several stacks/systems</p>

<p>Client-side RabbitMQ TLS configuration:</p>

<p>|</p>

<p>To enable TLS for oslo.messaging you need to provide the CA certificate.</p>

<p>By default system-wide CA certs are used. Nothing should be specified except <code>ssl.enabled</code>.</p>

<blockquote>
<pre><code>      message_queue:
        ssl:
          enabled: True
</code></pre>
</blockquote>

<p>Use <code>cacert_file</code> param to specify the CA-cert file location explicitly:</p>

<blockquote>
<pre><code>          cacert_file: /etc/ssl/rabbitmq-ca.pem
</code></pre>
</blockquote>

<p>To manage content of the <code>cacert_file</code> use the <code>cacert</code> param:</p>

<blockquote>
<pre><code>          cacert: { file content here }
          cacert_file: /etc/openstack/rabbitmq-ca.pem


Notice:
</code></pre>

<ul>
<li>The <code>message_queue.port</code> is set to <strong>5671</strong> (AMQPS) by default if <code>ssl.enabled=True</code>.</li>
<li>Use <code>message_queue.ssl.version</code> if you need to specify protocol version. By default is TLSv1 for python &lt; 2.7.9 and TLSv1_2 for version above.</li>
</ul>
</blockquote>

<h3 id="oleksii-chupryn">Oleksii Chupryn<a href="#oleksii-chupryn" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Add ability to configure protocol for</p>

<blockquote>
<pre><code>            protocol: http
</code></pre>
</blockquote>

<h3 id="ondrej-smola-5">Ondrej Smola<a href="#ondrej-smola-5" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>parametrized heat clients_keystone paramater</p>

<p>Define server clients keystone parameter</p>

<blockquote>
<pre><code>        clients:
          keystone:
            protocol: https
            host: 10.0.106.10
            port: 5000
            insecure: false
</code></pre>
</blockquote>

<p>enable support for cors params</p>

<p>Enable CORS parameters</p>

<blockquote>
<pre><code>        cors:
          allowed_origin: https:localhost.local,http:localhost.local
          expose_headers: X-Auth-Token,X-Openstack-Request-Id,X-Subject-Token
          allow_methods: GET,PUT,POST,DELETE,PATCH
          allow_headers: X-Auth-Token,X-Openstack-Request-Id,X-Subject-Token
          allow_credentials: True
          max_age: 86400
</code></pre>
</blockquote>

<h1 id="formula-heka">Formula heka</h1>

<h3 id="filip-pytloun-42">Filip Pytloun<a href="#filip-pytloun-42" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Unify Makefile, .gitignore and update readme</p>

<h3 id="ildar-svetlov-1">Ildar Svetlov<a href="#ildar-svetlov-1" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Make max_message_size customizable</p>

<blockquote>
<pre><code>        max_message_size: 262144
</code></pre>

<ul>
<li><code>max_message_size: 262144</code></li>
</ul>
</blockquote>

<h1 id="formula-helm">Formula helm</h1>

<h3 id="ales-komarek-8">Ales Komarek<a href="#ales-komarek-8" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Doc cleanup, introduced environmental params</p>

<p>Helm formula</p>

<p>More Information</p>

<ul>
<li><a href="https://github.com/kubernetes/charts">https://github.com/kubernetes/charts</a></li>
<li><a href="https://fabric8.io/helm/">https://fabric8.io/helm/</a></li>
</ul>

<h3 id="petr-michalec-5">Petr Michalec<a href="#petr-michalec-5" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Rebase to master + minor updates
  which just re-call the helm.release_managed</p>

<p>such as the version of <code>kubectl</code> to install and the path where the binary should</p>

<p>Optionally installs a Tiller deployment to the Kubernetes cluster per the</p>

<h3 id="tmeneau">tmeneau<a href="#tmeneau" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<blockquote>
<pre><code>  task: make changes requested in code review
</code></pre>
</blockquote>

<p>The changes included with this commit are:
  binaries</p>

<p>See the <a href="metadata/service/client.yml">default reclass pillar configuration</a> for
  a documented example pillar file.</p>

<blockquote>
<pre><code>  fix: change detection and speed for releases_managed
</code></pre>

<p>for most of the Helm global parameters)</p>
</blockquote>

<p>Refs salt-formulas/salt-formula-helm#6</p>

<p>Refs salt-formulas/salt-formula-helm#9
  manages releases in it.</p>

<p>Availale States</p>

<p>The default state applied by this formula (e.g. if just applying <code>helm</code>) will
  apply the <code>helm.releases_managed</code> state.</p>

<p><code>kubectl_installed</code></p>

<p>Optionally installs the kubectl binary per the configured pillar values,
  such as the version of <code>kubectl</code> to instlal and the path where the binary should
  be installed.</p>

<p><code>kubectl_configured</code></p>

<p>Manages a kubectl configuration file and gce_token json file per the configured
  pillar values. Note that the available configuration values allow the path of
  the kube config file to be placed at a different location than the default
  installation path; this is recommended to avoid confusion if the kubectl
  binary on the minion might be manually used with multiple contexts.</p>

<p><strong>includes</strong>:
  * <code>kubectl_installed</code></p>

<p><code>client_installed</code></p>

<p>Installs the helm client binary per the configured pillar values, such as where
  helm home should be, which version of the helm binary to install and that path
  for the helm binary.</p>

<ul>
<li>`kubectl_installed</li>
</ul>

<p><code>tiller_installed</code></p>

<p>Optionally installs a Tiller deployment to the kubernetes cluster per the
  <code>helm:client:tiller:install</code> pillar value. If the pillar value is set to
  install tiller to the cluster, the version of the tiller installation will
  match the version of the Helm client installed per the <code>helm:client:version</code>
  configuration parameter</p>

<ul>
<li><code>client_installed</code></li>
<li><code>kubectl_configured</code></li>
</ul>

<p><code>repos_managed</code></p>

<p>Ensures the repositories configured per the pillar (and only those repositories)
  are registered at the configured helm home, and synchronizes the local cache
  with the remote repository with each state execution.</p>

<p><code>releases_managed</code></p>

<p>Ensures the releases configured with the pillar are in the expected state with
  the Kubernetes cluster. This state includes change detection to determine
  whether the pillar configurations match the release&rsquo;s state in the cluster.</p>

<p>Note that changes to an existing release&rsquo;s namespace will trigger a deletion and
  re-installation of the release to the cluster.</p>

<ul>
<li><code>tiller_installed</code></li>
<li><code>repos_managed</code></li>
</ul>

<p>Availale Modules</p>

<p>To view documentation on the available modules, run:</p>

<p><code>salt '{{ tgt }}' sys.doc helm</code></p>

<p>Known Issues</p>

<ol>
<li>Unable to remove all user supplied values</li>
</ol>

<p>If a release previously has had user supplied value overrides (via the
  release&rsquo;s <code>values</code> key in the pillar), subsequently removing all <code>values</code>
  overrides (so that there is no more <code>values</code> key for the release in the
  pillar) will not actually update the Helm deployment. To get around this,
  specify a fake key/value pair in the release&rsquo;s pillar; Tiller will override
  all previously user-supplied values with the new fake key and value. For</p>

<blockquote>
<pre><code>example:


    helm:
      client:
        releases:
          zoo1:
            enabled: true
</code></pre>

<p>&hellip;</p>

<pre><code>            values:
              fake_key: fake_value


  fix: split client.sls into distinct, dedicated SLS files
</code></pre>
</blockquote>

<p>Additionally export a <code>constants</code> dict from the <code>map.jinja</code> to handle
  any shared or verbose constants — such as file paths, environment
  variables, etc… — and make the individual SLS files more concise. Also,
  fix a minor logic issue introduced in commit c42a332 that would not
  reference the kubectl configuration managed by this formula if the
  kubectl binary is not installed by the formula.</p>

<p>Fixes salt-formulas/salt-formula-helm#8</p>

<p>The default pillar configuration will install the helm client on the target
  node, and Tiller to the Kubernetes cluster (assuming kubectl config or local
  cluster endpoint have already been configured.</p>

<blockquote>
<pre><code>  fix: add sensible defaults and defensive property references
</code></pre>
</blockquote>

<p>Additionally deprecate certain duplicated or unnecessary configuration
  parameters, such as:
  version</p>

<blockquote>
<pre><code>  helm:client:bind:address
</code></pre>

<p>since that’s an implementation detail of the formula that shouldn’t
  matter to consumers</p>
</blockquote>

<p>Fixes salt-formulas/salt-formula-helm#2</p>

<p>See the <a href="pillar.example">pillar.example</a> for a documented example pillar file.</p>

<p>Example Configurations</p>

<p><em>The following examples demonstrate configuring the formula for different
  use cases.</em></p>

<blockquote>
<pre><code>        version: 2.6.0  # defaults to 2.6.2 currently
            # directly translated to cluster definition in kubeconfig
            cluster: 
            cluster_name: kubernetes.example
            # directly translated to user definition in kubeconfig
            user:
            user_name: admin 
            cluster:
            user_name: gce_user
</code></pre>
</blockquote>

<h3 id="yuriy-taraday">Yuriy Taraday<a href="#yuriy-taraday" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Add user guide</p>

<p>Helm Formula README</p>

<p>Add pillar examples to README</p>

<p>This formula installs Helm client, installs Tiller on Kubernetes cluster and
  creates releases in it.</p>

<p>Enable formula, install helm client on node and tiller on Kubernetes (assuming
  already configured kubectl config or local cluster endpoint):</p>

<p>Change version of helm being downloaded and installed:</p>

<blockquote>
<pre><code>        version: 2.6.0  # defaults to 2.4.2 currently
        download_hash: sha256=youneedtocalculatehashandputithere
</code></pre>
</blockquote>

<p>Don&rsquo;t install tiller and use existing one exposed on some well-known address:</p>

<blockquote>
<pre><code>        tiller:
          install: false
          host: 10.11.12.13:14151
</code></pre>
</blockquote>

<p>Change namespace where tiller is isntalled and looked for:</p>

<blockquote>
<pre><code>          namespace: not-kube-system  # kube-system is default
</code></pre>
</blockquote>

<p>Install Mirantis repository and deploy zookeper chart from it:</p>

<blockquote>
<pre><code>        repos:
          mirantisworkloads: https://mirantisworkloads.storage.googleapis.com/
            name: my-zookeeper
            chart: mirantisworkloads/zookeeper  # we reference installed repo
            version: 1.2.0  # select any available version
              logLevel: INFO  # any values used by chart can specified here
</code></pre>
</blockquote>

<p>Delete that release:</p>

<blockquote>
<pre><code>            enabled: false
</code></pre>
</blockquote>

<p>Install kubectl and manage remote cluster:</p>

<blockquote>
<pre><code>        kubectl:
          install: true  # installs kubectl 1.6.7 by default
          config:
            cluster:  # directly translated to cluster definition in kubeconfig
              server: https://kubernetes.example.com
              certificate-authority-data: base64_of_ca_certificate
            user:  # same for user
              username: admin
              password: uberadminpass
</code></pre>
</blockquote>

<p>Change kubectl download URL and use it with GKE-based cluster:</p>

<blockquote>
<pre><code>          install: true
          download_url: https://dl.k8s.io/v1.6.7/kubernetes-client-linux-amd64.tar.gz
          download_hash: sha256=calculate_hash_here
              server: https://3.141.59.265
              auth-provider:
                name: gcp
            gce_service_token: base64_of_json_token_downloaded_from_cloud_console
</code></pre>
</blockquote>

<p>Initial cookiecutter result</p>

<p>helm</p>

<p>Service helm description</p>

<p>Sample pillars</p>

<p>Single helm service</p>

<blockquote>
<pre><code>        enabled: true
        version: icehouse
</code></pre>
</blockquote>

<p>Development and testing</p>

<p>Development and test workflow with <code>Test Kitchen &lt;http://kitchen.ci&gt;</code>_ and
  <code>kitchen-salt &lt;https://github.com/simonmcc/kitchen-salt&gt;</code>_ provisioner plugin.</p>

<p>Test Kitchen is a test harness tool to execute your configured code on one or more platforms in isolation.</p>

<p>There is a <code>.kitchen.yml</code> in main directory that defines <em>platforms</em> to be tested and <em>suites</em> to execute on them.</p>

<p>Kitchen CI can spin instances locally or remote, based on used <em>driver</em>.</p>

<p>For local development <code>.kitchen.yml</code> defines a <code>vagrant &lt;https://github.com/test-kitchen/kitchen-vagrant&gt;</code>_ or
  <code>docker  &lt;https://github.com/test-kitchen/kitchen-docker&gt;</code>_ driver.</p>

<p>To use backend drivers or implement your CI follow the section <code>INTEGRATION.rst#Continuous Integration</code>__.</p>

<p>The <code>Busser &lt;https://github.com/test-kitchen/busser&gt;</code>_ <em>Verifier</em> is used to setup and run tests
  implementated in <code>&lt;repo&gt;/test/integration</code>. It installs the particular driver to tested instance
  (<code>Serverspec &lt;https://github.com/neillturner/kitchen-verifier-serverspec&gt;</code><em>,
  <code>InSpec &lt;https://github.com/chef/kitchen-inspec&gt;</code></em>, Shell, Bats, &hellip;) prior the verification is executed.</p>

<blockquote>
<pre><code>Usage:


  # list instances and status
</code></pre>

<p>kitchen list</p>

<pre><code>  # manually execute integration tests
</code></pre>

<p>kitchen [test || [create|converge|verify|exec|login|destroy|&hellip;]] [instance] -t tests/integration</p>

<pre><code>  # use with provided Makefile (ie: within CI pipeline)
</code></pre>

<p>make kitchen</p>
</blockquote>

<p>Read more</p>

<ul>
<li>links</li>
</ul>

<h1 id="formula-home-assistant">Formula home-assistant</h1>

<h3 id="ales-komarek-9">Ales Komarek<a href="#ales-komarek-9" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Git config</p>

<p>Home Assistant formula</p>

<p>Home Assistant is an open-source home automation platform running on Python 3.</p>

<p>Track and control all devices at home and automate control.
  home-assistant service wit git based configuration</p>

<blockquote>
<pre><code>    home_assistant:
      server:
        enabled: true
        bind:
          address: 0.0.0.0
          port: 8123
        config:
          engine: git
          address: '${_param:home_assistant_config_repository}'
          branch: ${_param:home_assistant_config_revision}
</code></pre>
</blockquote>

<p>More information</p>

<h3 id="filip-pytloun-43">Filip Pytloun<a href="#filip-pytloun-43" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Unify Makefile, .gitignore and update readme</p>

<h1 id="formula-horizon">Formula horizon</h1>

<h3 id="adam-tengler-1">Adam Tengler<a href="#adam-tengler-1" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Manage policy files in Horizon</p>

<p>Manage policy files in Horizon conf directory, either dynamically
  from Salt Mine or statically from Horizon formula directory.</p>

<p>Horizon with policy files metadata. With source mine you can obtain real time policy file state from targeted node (OpenStack control node), provided you have policy file published to specified grain key. Source file will obtain static policy definition from formula files directory.</p>

<blockquote>
<pre><code>    horizon:
      server:
        enabled: true
        policy:
          identity:
            source: mine
            host: ctl01.my-domain.local
            name: keystone_policy.json
            grain_name: keystone_policy
            enabled: true
          compute:
            source: file
            name: nova_policy.json
          network:
            name: neutron_policy.json
          image:
            name: glance_policy.json
          volume:
            name: cinder_policy.json
          telemetry:
            name: ceilometer_policy.json
          orchestration:
            name: heat_policy.json
</code></pre>
</blockquote>

<p>SESSION_ENGINE parameter introduced, pillar for SESSION_TIMEOUT changed. Actualized pillar for horizon.server.cluster service</p>

<p>Horizon with custom SESSION_ENGINE (default is &ldquo;signed_cookies&rdquo;, valid options are: &ldquo;signed_cookies&rdquo;, &ldquo;cache&rdquo;, &ldquo;file&rdquo;) and SESSION_TIMEOUT</p>

<blockquote>
<pre><code>        enabled: True
        secure: True
        session:
          engine: 'cache'
          timeout: 43200
</code></pre>
</blockquote>

<h3 id="ales-komarek-10">Ales Komarek<a href="#ales-komarek-10" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Fixed PROD-9967</p>

<p>Horizon Formula</p>

<p>Horizon is the canonical implementation of OpenStack’s Dashboard, which
  provides a web based user interface to OpenStack services including Nova,</p>

<p>Swift, Keystone, etc.</p>

<p>Sample Pillars</p>

<p>Horizon with enabled SSL security (when SSL is realised by proxy)</p>

<p>More Information</p>

<h3 id="dmitry-stremkovskiy-2">Dmitry Stremkovskiy<a href="#dmitry-stremkovskiy-2" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>multidomain and default_domain options should be independent and configurable</p>

<p>Multidomain setup for horizon</p>

<blockquote>
<pre><code>        default_domain: MYDOMAIN
        multidomain: True
</code></pre>
</blockquote>

<h3 id="filip-pytloun-44">Filip Pytloun<a href="#filip-pytloun-44" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Unify Makefile, .gitignore and update readme</p>

<h3 id="oleksii-chupryn-1">Oleksii Chupryn<a href="#oleksii-chupryn-1" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Define identity api version in api_versions dict</p>

<blockquote>
<pre><code>        api_versions:
          identity: 2
            api_versions:
              identity: 2
              identity: 3
</code></pre>
</blockquote>

<h1 id="formula-hovercraft">Formula hovercraft</h1>

<h3 id="dresl">dresl<a href="#dresl" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Proper title
  hovercraft-formula</p>

<p>Cleanup</p>

<blockquote>
<pre><code>parameters:
  hovercraft:
    server:
      slides:
        name_of_slides:
          builder: html
          path: source
          source: 
            engine: git
            address: git_address
            rev: master
</code></pre>
</blockquote>

<p>Update readme</p>

<p>Hovercraft’s power comes from the combination of reStructuredText’s convenience with the cool of impress.js, together with a flexible and powerful solution to position the slides.</p>

<blockquote>
<pre><code>    http://hovercraft.readthedocs.io/
    https://github.com/salt-formulas/salt-formula-hovercraft/issues
    https://github.com/salt-formulas/salt-formula-hovercraft
</code></pre>
</blockquote>

<p>Add files</p>

<p>Hovercraft</p>

<p>Sphinx is a tool that makes it easy to create intelligent and beautiful documentation, written by Georg Brandl and licensed under the BSD license. It was originally created for the new Python documentation, and it has excellent facilities for the documentation of Python projects, but C/C++ is already supported as well, and it is planned to add special support for other languages as well.</p>

<p>Sample pillars</p>

<p>Simple documentation with local source</p>

<blockquote>
<pre><code>    sphinx:
      server:
        enabled: true
        doc:
          board:
            builder: 'html'
            source: 
              engine: local
              path: '/path/to/sphinx/documentation'
</code></pre>
</blockquote>

<p>Simple documentation with Git source</p>

<blockquote>
<pre><code>              engine: git
              address: 'git@repo1.domain.com/repo.git'
              revision: master
</code></pre>
</blockquote>

<p>Simple documentation with reclass source</p>

<blockquote>
<pre><code>              engine: reclass
</code></pre>
</blockquote>

<p>Read more</p>

<ul>
<li><a href="http://sphinx-doc.org/tutorial.html">http://sphinx-doc.org/tutorial.html</a></li>
</ul>

<h1 id="formula-hubblestack">Formula hubblestack</h1>

<h3 id="petr-michalec-6">Petr Michalec<a href="#petr-michalec-6" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Initial version</p>

<p>hubblestack formula</p>

<p>Hubble is a modular, open-source security compliance framework. The project provides on-demand profile-based auditing,
  real-time security event notifications, automated remediation, alerting and reporting.</p>

<p>More information</p>

<ul>
<li><a href="https://github.com/hubblestack/hubble-salt">https://github.com/hubblestack/hubble-salt</a></li>
</ul>

<p>Deviations from hubblestack.io hubble-salt formula</p>

<p>As no <code>gitfs</code> is used paths are shifted from <code>salt://</code> to <code>salt://hubblestack</code>.</p>

<p>Sample pillars</p>

<p>Required configuration:</p>

<blockquote>
<pre><code>  hubblestack:
    nova:
      saltenv: base
      module_dir: salt://hubblestack/hubblestack_nova
      profile_dir: salt://hubblestack/hubblestack_nova_profiles
</code></pre>
</blockquote>

<ul>
<li>links</li>
</ul>

<h1 id="formula-influxdb">Formula influxdb</h1>

<h3 id="abednarik">abednarik<a href="#abednarik" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Update tests and README about custom paths</p>

<p>Single-node influxdb where you specify paths for data and metastore directories. Custom
  directories are created by this formula:</p>

<h3 id="filip-pytloun-45">Filip Pytloun<a href="#filip-pytloun-45" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Unify Makefile, .gitignore and update readme</p>

<h3 id="petr-michalec-7">Petr Michalec<a href="#petr-michalec-7" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<blockquote>
<pre><code>  FEATURE: add custom ad-hoc queries for schedulled run, etc
            continuous_query:
</code></pre>
</blockquote>

<p>Prunning data and data management:</p>

<p>Intended to use in scheduled jobs, executed to maintain data life cycle above retention policy. These states are executed by
  <code>query.sls</code> and you are expected to trigger <code>sls_id</code> individually.</p>

<blockquote>
<pre><code>    influxdb:
      client:
        database:
          mydb1:
            query:
              drop_measurement_h2o: &gt;-
</code></pre>
</blockquote>

<p>DROP MEASUREMENT h2o_quality</p>

<blockquote>
<pre><code>              drop_shard_h2o: &gt;-
</code></pre>
</blockquote>

<p>DROP SHARD h2o_quality</p>

<blockquote>
<pre><code>              drop_series_h2o_feet: &gt;-
</code></pre>
</blockquote>

<p>DROP SERIES FROM &ldquo;h2o_feet&rdquo;</p>

<blockquote>
<pre><code>              drop_series_h2o_feet_loc_smonica: &gt;-
</code></pre>
</blockquote>

<p>DROP SERIES FROM &ldquo;h2o_feet&rdquo; WHERE &ldquo;location&rdquo; = &lsquo;santa_monica&rsquo;</p>

<blockquote>
<pre><code>              delete_h2o_quality_rt3: &gt;-
</code></pre>
</blockquote>

<p>DELETE FROM &ldquo;h2o_quality&rdquo; WHERE &ldquo;randtag&rdquo; = &lsquo;3&rsquo;</p>

<blockquote>
<pre><code>              delete_h2o_quality: &gt;-
</code></pre>
</blockquote>

<p>DELETE FROM &ldquo;h2o_quality&rdquo;</p>

<p>salt * state.sls_id influxdb_query_delete_h2o_quality influxdb.query</p>

<p>FEATURE, add continuous query support</p>

<p>Create an continuous queries:</p>

<blockquote>
<pre><code>            continuos_query:
              cq_avg_bus_passengers: &gt;-
</code></pre>
</blockquote>

<p>SELECT mean(&ldquo;passengers&rdquo;) INTO &ldquo;transportation&rdquo;.&ldquo;three_weeks&rdquo;.&ldquo;average_passengers&rdquo; FROM &ldquo;bus_data&rdquo; GROUP BY time(1h)</p>

<h3 id="simon-pasquier-5">Simon Pasquier<a href="#simon-pasquier-5" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Enable telemetry for InfluxDB relay</p>

<blockquote>
<pre><code>        telemetry:
          enabled: true
          bind:
            address: 127.0.0.1
            port: 9196
</code></pre>
</blockquote>

<p>Add support for influxdb-relay</p>

<p>InfluxDB relay with HTTP outputs:</p>

<blockquote>
<pre><code>      relay:
        enabled: true
        listen:
          http_backend:
            type: http
            bind:
              address: 127.0.0.1
              port: 9096
            output:
              server1:
                location: http://server1:8086/write
                timeout: 20s
                buffer_size_mb: 512
                max_batch_kb: 1024
                max_delay_interval: 30s
              server2:
                location: http://server2:8086/write
</code></pre>
</blockquote>

<p>Add client role</p>

<p>The client role is responsible for provisioning the users, databases
  and privileges. It is required when running InfluxDB in a container
  because the deployment of InfluxDB and the provisioning phase are
  decoupled. Non-containerized deployments are left unchanged with the
  provisioning managed by the server state.</p>

<p>InfluxDB client for configuring databases, users and retention policies:</p>

<blockquote>
<pre><code>        server:
          protocol: http
          host: 127.0.0.1
          port: 8086
          user: admin
          password: foobar
        user:
          user1:
            enabled: true
            admin: true
            name: username1
            name: mydb1
            retention_policy:
            - name: rp_db1
              duration: 30d
              replication: 1
              is_default: true
        grant:
          username1_mydb1:
            user: username1
            database: mydb1
            privilege: all
</code></pre>
</blockquote>

<p>Expose more parameters for the data service</p>

<blockquote>
<pre><code>          cache_max_memory_size: 524288000
          cache_snapshot_memory_size: 26214400
          cache_snapshot_write_cold_duration: &quot;5m&quot;
          compact_full_write_cold_duration: &quot;2h&quot;2h&quot;
          max_values_per_tag: 5000
</code></pre>
</blockquote>

<p>Expose max-series-per-database parameter</p>

<p>InfluxDB server with customized parameters for the data service:</p>

<blockquote>
<pre><code>      server:
        data:
          max_series_per_database: 20000000
</code></pre>
</blockquote>

<h1 id="formula-iptables">Formula iptables</h1>

<h3 id="dennis-van-dok">Dennis van Dok<a href="#dennis-van-dok" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>add the option to specify the family per rule to support ipv6 (#3)</p>

<blockquote>
<pre><code>  Closes: #2
          enabled: True
</code></pre>
</blockquote>

<p>IPv6 is supported as well</p>

<blockquote>
<pre><code>    parameters:
      iptables:
        service:
          ipv6: True
          chain:
            INPUT:
              rules:
                - protocol: tcp
                  family: ipv6
                  destination_port: 22
                  source_network: 2001:DB8::/32
                  jump: ACCEPT
</code></pre>
</blockquote>

<h3 id="dmitry-stremkouski-2">Dmitry Stremkouski<a href="#dmitry-stremkouski-2" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Adding comment option to iptables rule</p>

<blockquote>
<pre><code>                  comment: Blah
</code></pre>
</blockquote>

<h3 id="filip-pytloun-46">Filip Pytloun<a href="#filip-pytloun-46" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Unify Makefile, .gitignore and update readme</p>

<h3 id="nitin-madhok">Nitin Madhok<a href="#nitin-madhok" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Correct parameters spelling</p>

<h3 id="piotr-pieprzycki">Piotr Pieprzycki<a href="#piotr-pieprzycki" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Changed file permissions - fix (#14)</p>

<p>added README example</p>

<p>Support logging with custom prefix and log level</p>

<blockquote>
<pre><code>            POSTROUTING:
                - table: nat
                  protocol: tcp
                  match: multiport
                  destination_ports:
                    - 21
                    - 80
                    - 443
                    - 2220
                  source_network: '10.20.30.0/24'
                  log_level: 7
                  log_prefix: 'iptables-logging: '
                  jump: LOG
</code></pre>
</blockquote>

<h1 id="formula-ironic">Formula ironic</h1>

<h3 id="vasyl-saienko">Vasyl Saienko<a href="#vasyl-saienko" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Initial commit</p>

<p>This patch adds initial formula that allow to install Ironic api
  and conductor.</p>

<p>ironic</p>

<p>Service ironic description</p>

<p>Sample pillars</p>

<p>Single ironic service</p>

<blockquote>
<pre><code>  ironic:
    api:
      enabled: true
      version: mitaka
      bind:
        address: 0.0.0.0
        port: 6385
      database:
        engine: mysql
        host: localhost
        port: 3306
        name: ironic
        user: ironic
        password: password
      identity:
        engine: keystone
        region: RegionOne
        port: 35357
        tenant: service
      message_queue:
        engine: rabbitmq
        port: 5672
        user: openstack
        virtual_host: '/openstack'
    conductor:
</code></pre>
</blockquote>

<p>Standalone ironic without keystone</p>

<blockquote>
<pre><code>        engine: noauth
</code></pre>
</blockquote>

<h1 id="formula-isc-dhcp">Formula isc-dhcp</h1>

<h3 id="ales-komarek-11">Ales Komarek<a href="#ales-komarek-11" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Documentation fixes</p>

<p>ISC DHCP formula</p>

<p>Sample pillars</p>

<p>More information</p>

<h3 id="filip-pytloun-47">Filip Pytloun<a href="#filip-pytloun-47" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Unify Makefile, .gitignore and update readme</p>

<h1 id="formula-java">Formula java</h1>

<h3 id="filip-pytloun-48">Filip Pytloun<a href="#filip-pytloun-48" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Unify Makefile, .gitignore and update readme</p>

<h1 id="formula-jenkins">Formula jenkins</h1>

<h3 id="ales-komarek-12">Ales Komarek<a href="#ales-komarek-12" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Jenkins job templating</p>

<p>Jenkins formula</p>

<p>Jenkins is an application that monitors executions of repeated jobs, such as
  building a software project or jobs run by cron.</p>

<p>Setup jenkins client, works with Salt 2016.3+, supports pipeline workflow
  projects only now.</p>

<p>Master role</p>

<p>Jenkins master with experimental plugin source support</p>

<p>SMTP server settings</p>

<blockquote>
<pre><code>    jenkins:
      master:
        email:
          engine: &quot;smtp&quot;
          host: &quot;smtp.domain.com&quot;
          user: &quot;user@domain.cz&quot;
          password: &quot;smtp-password&quot;
          port: 25
</code></pre>
</blockquote>

<p>Script approvals</p>

<blockquote>
<pre><code>        approved_scripts:
        - method groovy.json.JsonSlurperClassic parseText java.lang.String
</code></pre>
</blockquote>

<p>User enforcement</p>

<blockquote>
<pre><code>        user:
          admin:
            api_token: xxxxxxxxxx
            password: admin_password
            email: admin@domain.com
          user01:
            password: user_password
            email: user01@domain.com
</code></pre>
</blockquote>

<p>Agent (slave) role</p>

<p>Client role</p>

<p>Inline Groovy scripts</p>

<p>GIT controlled groovy scripts</p>

<p>Using job templates in similar way as in jjb. For now just 1 defined param is
  supported.</p>

<blockquote>
<pre><code>      client:
        job_template:
          test_workflow_template:
            name: test-{{formula}}-workflow
            template:
              type: workflow
              display_name: Test jenkins {{name}} workflow
              param:
                repo_param:
                  type: string
                  default: repo/{{formula}}
              script:
                repository: base
                file: workflows/test_formula_workflow.groovy
            param:
              formula:
              - aodh
              - linux
              - openssh
</code></pre>
</blockquote>

<p>Interpolating parameters for job templates.</p>

<blockquote>
<pre><code>    _param:
      salt_formulas:
      - aodh
      - git
      - nova
      - xorg
</code></pre>

<p>&hellip;</p>

<pre><code>              formula: ${_param:salt_formulas}


        job:
          my-amazing-job:
            type: workflow
        node:
          node01:
            remote_home: /remote/home/path
            desc: node-description
            num_executors: 1
            node_mode: Normal
            ret_strategy: Always
            labels:
              - example
              - label
            launcher:
               type: jnlp
              - label 
               type: ssh
               host: test-launcher
               port: 22
               username: launcher-user
               password: launcher-pass
        label:
          node-name:
            lbl_text: label-offline
            append: false # set true for label append instead of replace
          team_domain: example.com
          token: slack-token
          room: slack-room
          token_credential_id: cred_id 
          send_as: Some slack user
</code></pre>
</blockquote>

<p>External links</p>

<h3 id="filip-pytloun-49">Filip Pytloun<a href="#filip-pytloun-49" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Add support for replacing more params using job templates</p>

<p>Or simply define multiple jobs and it&rsquo;s parameters to replace from template:</p>

<blockquote>
<pre><code>   jenkins:
     client:
       job_template:
         test_workflow_template:
           name: test-{{name}}-{{myparam}}
           template:


           jobs:
             - name: firstjob
               myparam: dummy
             - name: secondjob
               myparam: dummyaswell
</code></pre>
</blockquote>

<p>Unify Makefile, .gitignore and update readme</p>

<h3 id="jakub-josef">Jakub Josef<a href="#jakub-josef" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Fixed credentials enforcing, updated readme</p>

<p>Jenkins Global env properties enforcing</p>

<blockquote>
<pre><code>     jenkins:
       client:
         globalenvprop:
           OFFLINE_DEPLOYMENT:
             enabled: true
             name: &quot;OFFLINE_DEPLOYMENT&quot; # optional, default using dict key
             value: &quot;true&quot;
</code></pre>
</blockquote>

<p>Added jenkins master configuration possibility</p>

<p>Configure Jenkins master</p>

<blockquote>
<pre><code>          master:
            node_mode: Normal # or Exclusive
</code></pre>
</blockquote>

<p>Implemented artifactory server enforcing</p>

<p>Artifactory server enforcing</p>

<blockquote>
<pre><code>        artifactory:
          my-artifactory-server:
            enabled: true
            url: https://path-to-my-library
            credential_id: github
</code></pre>
</blockquote>

<p>Improved Jenkins SMTP settings</p>

<p>Jenkins admin user email enforcement from client</p>

<blockquote>
<pre><code>        smtp:
          admin_email: &quot;My Jenkins &lt;jenkins@myserver.com&gt;&quot;
</code></pre>
</blockquote>

<p>Extended jenkins views enforcing by Categorize Views</p>

<blockquote>
<pre><code>           include_regex: &quot;.*&quot;
- include_regex for ListView and CategorizedJobsView
- categories for CategorizedJobsView
</code></pre>
</blockquote>

<p>Categorized views</p>

<blockquote>
<pre><code>        view:
          my-categorized-view:
            type: CategorizedJobsView
            include_regex: &quot;.*&quot;
            categories:
              - group_regex: &quot;aptly-.*-nightly-testing&quot;
                naming_rule: &quot;Nightly -&gt; Testing&quot;
              - group_regex: &quot;aptly-.*-nightly-production&quot;
                naming_rule: &quot;Nightly -&gt; Production&quot;
</code></pre>
</blockquote>

<p>Implemented jenkins scripts approving from client size</p>

<p>Script approvals from client</p>

<blockquote>
<pre><code>          - method groovy.json.JsonSlurperClassic parseText java.lang.String
</code></pre>
</blockquote>

<p>Implemented Jenkins global libs configuration by salt.</p>

<blockquote>
<pre><code>          token_credential_id: cred_id
</code></pre>
</blockquote>

<p>Pipeline global libraries setup</p>

<blockquote>
<pre><code>        lib:
          my-pipeline-library:
            branch: master # optional, default master
            implicit: true # optional default true
</code></pre>
</blockquote>

<p>Implemented max keep builds property on jobs</p>

<p>Setting job max builds to keep (amount of last builds stored on Jenkins master)</p>

<blockquote>
<pre><code>            discard:
              build:
                keep_num: 5
                keep_days: 5
              artifact:
                keep_num: 6
                keep_days: 6
</code></pre>
</blockquote>

<p>Impemented Jenkins jobs cleanup - uninstallation of all undefined jobs.</p>

<p>Purging undefined jobs from Jenkins</p>

<blockquote>
<pre><code>        purge_jobs: true
</code></pre>
</blockquote>

<p>Implemented Jenkins Slack plugin configuration.</p>

<p>Slack plugin configuration</p>

<blockquote>
<pre><code>        slack:
           team_domain: example.com
           token: slack-token
           room: slack-room
           token_credential_id: cred_id 
           send_as: Some slack user
</code></pre>
</blockquote>

<p>Implemented SMTP settings from client side via script api.</p>

<p>SMTP server settings from master</p>

<p>SMTP server settings from client</p>

<blockquote>
<pre><code>          username: &quot;user@domain.cz&quot;
          ssl: false
          reply_to: reply_to@address.com
</code></pre>
</blockquote>

<p>Matrix security extended to use GlobalMatrixAuthStrategy or ProjectMatrixAuthStrategy</p>

<blockquote>
<pre><code>            # set true for use ProjectMatrixAuthStrategy instead of GlobalMatrixAuthStrategy
            project_based: false  
</code></pre>
</blockquote>

<p>Implemented Jenkins views enforcing.</p>

<p>Views enforcing from client</p>

<blockquote>
<pre><code>         my-list-view:
           enabled: true
           type: ListView
           include_regex: &quot;.\*.&quot;
         my-view:
           # set false to disable
           type: MyView
</code></pre>
</blockquote>

<p>View specific params:</p>

<blockquote>
<pre><code>- include_regex for ListView
</code></pre>
</blockquote>

<p>Implemented LDAP config and matrix auth security enforcements.</p>

<p>LDAP configuration (depends on LDAP plugin)</p>

<blockquote>
<pre><code>        security:
          ldap:
            server: 1.2.3.4
            root_dn: dc=foo,dc=com
            user_search_base: cn=users,cn=accounts
            manager_dn: &quot;&quot;
            manager_password: password
            user_search: &quot;&quot;
            group_search_base: &quot;&quot;
            inhibit_infer_root_dn: false
</code></pre>
</blockquote>

<p>Matrix configuration (depends on auth-matrix plugin)</p>

<blockquote>
<pre><code>          matrix:
            permissions:
              Jenkins:
                # administrator access
                ADMINISTER:
                  - admin
                # read access (anonymous too)
                READ:
                  - anonymous
                  - user1
                  - user2
                # agents permissions
                MasterComputer: 
                  BUILD: 
                    - user3
              # jobs permissions
              hudson: 
                model:
                  Item:
                    BUILD: 
                      - user4
</code></pre>
</blockquote>

<p><code>Common matrix strategies &lt;https://github.com/arbabnazar/configuration/blob/c08a5eaf4e04a68d2481375502a926517097b253/playbooks/roles/tools_jenkins/templates/projectBasedMatrixSecurity.groovy.j2&gt;</code>_</p>

<h3 id="tomáš-kukrál-3">Tomáš Kukrál<a href="#tomáš-kukrál-3" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>add support for job timer</p>

<p>I&rsquo;d like to run some jobs on daily basis and this PR add this
  functionality.</p>

<blockquote>
<pre><code>              timer:
                spec: &quot;H H * * *&quot;
</code></pre>
</blockquote>

<h1 id="formula-jupyter">Formula jupyter</h1>

<h3 id="ales-komarek-13">Ales Komarek<a href="#ales-komarek-13" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Added schema, fixed variable requirements</p>

<blockquote>
<pre><code>          requirements: true
</code></pre>
</blockquote>

<h3 id="filip-pytloun-50">Filip Pytloun<a href="#filip-pytloun-50" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Unify Makefile, .gitignore and update readme</p>

<h1 id="formula-kalliope">Formula kalliope</h1>

<h3 id="ales-komarek-14">Ales Komarek<a href="#ales-komarek-14" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Initial commit</p>

<p>Kalliope Formula</p>

<p>Kalliope is a modular always-on voice controlled personal assistant designed
  for home automation.</p>

<p>Sample Pillars</p>

<p>Single kalliope service with synapses defined</p>

<blockquote>
<pre><code>    kalliope:
      server:
        enabled: true
        bind:
          address: 0.0.0.0
          port: 5000
        synapse:
          default-synapse:
            signals:
            - order: &quot;default-synapse&quot;
            neurons:
            - say:
                message:
                - &quot;I haven't understood&quot;
                - &quot;I don't know this order&quot;
                - &quot;I don't recognize that order&quot;
</code></pre>
</blockquote>

<p>Single kalliope service with extra resources defined</p>

<blockquote>
<pre><code>        trigger:
          primary:
            engine: snowboy
            default: true
            pmdl_file: &quot;trigger/kalliope-EN-12samples.pmdl&quot;
        speech_to_text:
          google:
            engine: google
            language: &quot;en-EN&quot;
        text_to_speech:
        resource:
          hue:
            type: neuron
            source:
              engine: git
              address: https://github.com/kalliope-project/kalliope_neuron_hue.git
</code></pre>
</blockquote>

<p>More Information</p>

<ul>
<li><p><a href="https://github.com/kalliope-project/kalliope">https://github.com/kalliope-project/kalliope</a></p></li>

<li><p>links</p></li>
</ul>

<h1 id="formula-kedb">Formula kedb</h1>

<h3 id="filip-pytloun-51">Filip Pytloun<a href="#filip-pytloun-51" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Unify Makefile, .gitignore and update readme</p>

<h1 id="formula-keepalived">Formula keepalived</h1>

<h3 id="filip-pytloun-52">Filip Pytloun<a href="#filip-pytloun-52" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Unify Makefile, .gitignore and update readme</p>

<h3 id="petr-michalec-8">Petr Michalec<a href="#petr-michalec-8" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Allow multiple vrrp scripts and weight</p>

<blockquote>
<pre><code>            track_script:
              check_random_exit:
                interval: 10
              check_port:
                weight: 50
            weight: 50
</code></pre>
</blockquote>

<p>foundation for vrrp check scripts</p>

<blockquote>
<pre><code>            track_script: check_haproxy
            track_script: check_random_exit
          check_haproxy:
            name: check_pidof
            args:
              - haproxy
          check_mysql_port:
            name: check_port
              - 3306
              - TCP
              - 4
          check_ssh:
            args: &quot;22&quot;
          check_mysql_cluster:
              # github: olafz/percona-clustercheck
              # &lt;user&gt; &lt;pass&gt; &lt;available_when_donor=0|1&gt; &lt;log_file&gt; &lt;available_when_readonly=0|1&gt; &lt;defaults_extra_file&gt;
              - clustercheck
              - available_when_donor=0
              - available_when_readonly=0
          check_random_exit:
</code></pre>
</blockquote>

<p>add track_script option</p>

<p>Track/vrrp scripts for keepalived instance:</p>

<blockquote>
<pre><code>    keepalived:
      cluster:
        enabled: True
        instance:
          VIP2:
            priority: 100
            virtual_router_id: 10
            password: pass
            addresses:
            - 192.168.11.1
            - 192.168.11.2
            interface: eth0
            track_script: haproxy_check
          VIP3:
            virtual_router_id: 11
            - 192.168.10.1
            - 192.168.10.2
            track_script: random_check
        vrrp_scripts:
          random_check:
            interval: 10
            content: |
              #!/bin/bash
</code></pre>

<p>exit $(($RANDOM%2))</p>
</blockquote>

<h3 id="tomáš-kukrál-4">Tomáš Kukrál<a href="#tomáš-kukrál-4" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>add suport for keepalived over unicast</p>

<p>Keepalived can be used via unicast and it is required for for
  running keepalived in Amazon VPC.</p>

<p>You can includes <code>unicast_src_ip</code> in list of peers because it will be
  skipped.</p>

<p>Use unicast</p>

<blockquote>
<pre><code>          VIP1:
            nopreempt: True
            priority: 100 (highest priority must be on primary server, different for cluster members)
            virtual_router_id: 51
            address: 192.168.10.1
            unicast_src_ip: 172.16.10.1
            unicast_peer:
</code></pre>

<p>172.16.10.2
  172.16.10.3</p>
</blockquote>

<h3 id="vasyl-saienko-1">Vasyl Saienko<a href="#vasyl-saienko-1" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Allow to set auth_type for VRRP group</p>

<p>Keepalived supports two auth_types: PASS and AH</p>

<p>This patch allows to set auth_type per VRRP instance and change
  default value to AH</p>

<blockquote>
<pre><code>            auth_type: AH
            auth_type: PASS
</code></pre>
</blockquote>

<h1 id="formula-keystone">Formula keystone</h1>

<h3 id="adam-tengler-2">Adam Tengler<a href="#adam-tengler-2" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Keystone policy module, states and grain</p>

<p>Keystone with custom policies. Keys with specified rules are created or set to this value if they already exists. Keys with no value (like our &ldquo;existing_rule&rdquo;) are deleted from the policy file.</p>

<blockquote>
<pre><code>    keystone:
      server:
        enabled: true
        policy:
          new_rule: &quot;rule:admin_required&quot;
          existing_rule:
</code></pre>
</blockquote>

<h3 id="andrey-1">Andrey<a href="#andrey-1" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Keystone configures parameter of admin project.</p>

<p>By default admin_project_name and admin_project_domain_name are undefined.</p>

<p>This affects Heat client while listing the stacks. For any user having
  admin rights in the tenant, all stacks across the entire cloud are shown.</p>

<p>Setting up default admin project name and domain</p>

<p>&hellip;.</p>

<blockquote>
<pre><code>        admin_project:
          name: &quot;admin&quot;
          domain: &quot;default&quot;
</code></pre>
</blockquote>

<h3 id="dmitry-ukov-4">Dmitry Ukov<a href="#dmitry-ukov-4" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Policy.json should be defined by user</p>

<p>Implementation has been moved from template based to pillar based. User
  can override and add values to policy.json by creating flat key-value
  structure under keystone:server:policy.</p>

<p>Configuration of policy.json file</p>

<blockquote>
<pre><code>          admin_or_token_subject: 'rule:admin_required or rule:token_subject'
</code></pre>
</blockquote>

<h3 id="filip-pytloun-53">Filip Pytloun<a href="#filip-pytloun-53" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Unify Makefile, .gitignore and update readme</p>

<h3 id="jiri-broulik-5">Jiri Broulik<a href="#jiri-broulik-5" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>endpoint token fix</p>

<blockquote>
<pre><code>              project: admin
              region_name: RegionOne
              protocol: https
      client:
        server:
          keystone01:
            admin:
              host: 10.0.0.2
              port: 5000
              project: 'admin'
              user: admin
              password: 'workshop'
          keystone02:
              host: 10.0.0.3
</code></pre>
</blockquote>

<p>keystone tenat quota setup fix</p>

<blockquote>
<pre><code>                quota:
                  instances: 100
                  cores: 24
                  ram: 151200
                  floating_ips: 50
                  fixed_ips: -1
                  metadata_items: 128
                  injected_files: 5
                  injected_file_content_bytes: 10240
                  injected_file_path_bytes: 255
                  key_pairs: 100
                  security_groups: 20
                  security_group_rules: 40
                  server_groups: 20
                  server_group_members: 20
</code></pre>
</blockquote>

<p>Tenant quotas</p>

<blockquote>
<pre><code>              project: 'token'
              password: 'passwd'
            roles:
            - admin
            - member
            project:
              tenant01:
                description: &quot;test env&quot;
</code></pre>
</blockquote>

<p>Usage</p>

<p>Apply state <code>keystone.client.service</code> first and then <code>keystone.client</code> state.</p>

<h3 id="kirill-bespalov-7">Kirill Bespalov<a href="#kirill-bespalov-7" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>RabbitMQ TLS support</p>

<blockquote>
<pre><code>  Usage: see README.rst
  Releases: Mitaka, Newton, Ocata
</code></pre>
</blockquote>

<p>OSCORE-383</p>

<p>Client-side RabbitMQ TLS configuration:</p>

<p>|</p>

<p>By default system-wide CA certs are used. Nothing should be specified except <code>ssl.enabled</code>.</p>

<blockquote>
<pre><code>  keystone:
    server:


      message_queue:
        ssl:
          enabled: True
</code></pre>
</blockquote>

<p>Use <code>cacert_file</code> option to specify the CA-cert file path explicitly:</p>

<blockquote>
<pre><code>          cacert_file: /etc/ssl/rabbitmq-ca.pem
</code></pre>
</blockquote>

<p>To manage content of the <code>cacert_file</code> use the <code>cacert</code> option:</p>

<blockquote>
<pre><code>          cacert: |
</code></pre>
</blockquote>

<p>&hellip;</p>

<blockquote>
<pre><code>          cacert_file: /etc/openstack/rabbitmq-ca.pem


Notice:
</code></pre>

<ul>
<li>The <code>message_queue.port</code> is set to <strong>5671</strong> (AMQPS) by default if <code>ssl.enabled=True</code>.</li>
<li>Use <code>message_queue.ssl.version</code> if you need to specify protocol version. By default is TLSv1 for python &lt; 2.7.9 and TLSv1_2 for version above.</li>
</ul>
</blockquote>

<h3 id="mnederlof-1">mnederlof<a href="#mnederlof-1" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Add k2k identity backend</p>

<p>Use a custom identity driver with custom options</p>

<blockquote>
<pre><code>        backend: k2k
        k2k:
          auth_url: 'https://keystone.example.com/v2.0'
          read_user: 'example_user'
          read_pass: 'password'
          read_tenant_id: 'admin'
          identity_driver: 'sql'
          id_prefix: 'k2k:'
          domain: 'default'
          caching: true
          cache_time: 600
</code></pre>
</blockquote>

<h3 id="oleksii-chupryn-2">Oleksii Chupryn<a href="#oleksii-chupryn-2" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Add support of Keystone OIDC Federation</p>

<p>Some parameters are optional while some other ones are exlusive between each other.</p>

<p>Enable SAML2 Federated keystone</p>

<blockquote>
<pre><code>          saml2:
            protocol: saml2
            remote_id_attribute: Shib-Identity-Provider
            shib_url_scheme: https
            shib_compat_valid_user: 'on'
            - https://${_param:cluster_public_host}/horizon/auth/websso/
</code></pre>
</blockquote>

<p>Enable OIDC Federated keystone</p>

<blockquote>
<pre><code>        auth_methods:
        - password
        - token
        - oidc
        federation:
        oidc:
            protocol: oidc
            remote_id_attribute: HTTP_OIDC_ISS
            remote_id_attribute_value: https://accounts.google.com
            oidc_claim_prefix: &quot;OIDC-&quot;
            oidc_response_type: id_token
            oidc_scope: &quot;openid email profile&quot;
            oidc_provider_metadata_url: https://accounts.google.com/.well-known/openid-configuration
            oidc_client_id: &lt;openid_client_id&gt;
            oidc_client_secret: &lt;openid_client_secret&gt;
            oidc_crypto_passphrase: openstack
            oidc_redirect_uri: https://key.example.com:5000/v3/auth/OS-FEDERATION/websso/oidc/redirect
            oidc_oauth_introspection_endpoint: https://www.googleapis.com/oauth2/v1/tokeninfo
            oidc_oauth_introspection_token_param_name: access_token
            oidc_oauth_remote_user_claim: user_id
            oidc_ssl_validate_server: 'off'
        federated_domain_name: Federated
        federation_driver: keystone.contrib.federation.backends.sql.Federation
        trusted_dashboard:
          - https://${_param:cluster_public_host}/auth/websso/
    apache:
        pkgs:
          - apache2
          - libapache2-mod-auth-openidc
        modules:
          - wsgi
          - auth_openidc


Notes: Ubuntu Trusty repository doesn't contain libapache2-mod-auth-openidc package. Additonal repository should be added to source list.
</code></pre>
</blockquote>

<p>Rename  websso to federation</p>

<p>Add ability to configure keystone auth methods</p>

<p>Keystone auth methods</p>

<blockquote>
<pre><code>        - external
        - oauth1


        - saml2
</code></pre>
</blockquote>

<p>Add ability to provide extra parameters to config
  since Mitaka openstack release</p>

<p>Extra config params in keystone.conf (since Mitaka release)</p>

<blockquote>
<pre><code>        extra_config:
          ini_section1:
            param1: value
            param2: value
          ini_section2:
</code></pre>
</blockquote>

<p>Add support of new param - federated_domain_name</p>

<blockquote>
<pre><code>          federated_domain_name: Federated
</code></pre>
</blockquote>

<p>Add ability to specify ShibURLScheme</p>

<blockquote>
<pre><code>          shib_url_scheme: https
</code></pre>
</blockquote>

<h3 id="ondrej-smola-6">Ondrej Smola<a href="#ondrej-smola-6" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>added support for cors parameters</p>

<p>Enable CORS parameters</p>

<blockquote>
<pre><code>        cors:
          allowed_origin: https:localhost.local,http:localhost.local
          expose_headers: X-Auth-Token,X-Openstack-Request-Id,X-Subject-Token
          allow_methods: GET,PUT,POST,DELETE,PATCH
          allow_headers: X-Auth-Token,X-Openstack-Request-Id,X-Subject-Token
          allow_credentials: True
          max_age: 86400
</code></pre>
</blockquote>

<h3 id="ramon-melero">Ramon Melero<a href="#ramon-melero" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>LDAP packages are missing</p>

<p>When trying to add ldap configurations we get the following error first:
  2017-08-15 15:29:20.130 9467 ERROR keystone.common.wsgi     import ldap.filter
  2017-08-15 15:29:20.130 9467 ERROR keystone.common.wsgi ImportError: No module named ldap.filter</p>

<p>Fixed by installing python-ldap, then we get the following error:
  2017-08-15 15:38:01.892 12591 ERROR keystone.common.wsgi     from keystone.identity.backends.ldap import common as common_ldap
  2017-08-15 15:38:01.892 12591 ERROR keystone.common.wsgi   File &ldquo;/usr/lib/python2.7/dist-packages/keystone/identity/backends/ldap/common.py&rdquo;, line 25, in <module>
  2017-08-15 15:38:01.892 12591 ERROR keystone.common.wsgi     import ldappool
  2017-08-15 15:38:01.892 12591 ERROR keystone.common.wsgi ImportError: No module named ldappool
  fixed by adding python-ldappool
  also found error in documentation syntax that was causing this:</p>

<blockquote>
<pre><code>  local:
          external:
            description: &quot;Testing domain&quot;
            backend: ldap
            assignment:
              backend: sql
            ldap:
              url: &quot;ldaps://idm.domain.com&quot;
              suffix: &quot;dc=cloud,dc=domain,dc=com&quot;
              # Will bind as uid=keystone,cn=users,cn=accounts,dc=cloud,dc=domain,dc=com
              uid: keystone
              password: password
</code></pre>
</blockquote>

<h3 id="richard-felkl-4">Richard Felkl<a href="#richard-felkl-4" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Added identity information to salt minion conf</p>

<p>Multiple servers example</p>

<blockquote>
<pre><code>    client:
      enabled: true
        keystone01:
          admin:
            host: 10.0.0.2
            port: 5000
            project: 'admin'
            user: admin
            password: 'workshop'
            region_name: RegionOne
            protocol: https
        keystone02:
            host: 10.0.0.3
</code></pre>
</blockquote>

<h1 id="formula-kibana">Formula kibana</h1>

<h3 id="filip-pytloun-54">Filip Pytloun<a href="#filip-pytloun-54" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Unify Makefile, .gitignore and update readme</p>

<h1 id="formula-knot">Formula knot</h1>

<h1 id="formula-kodi">Formula kodi</h1>

<h3 id="ales-komarek-15">Ales Komarek<a href="#ales-komarek-15" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Updated docs to rst</p>

<p>KODI formula</p>

<p>Kodi (formerly known as XBMC) is a software media center for playing videos, music, pictures, games, and more.</p>

<p>Sample pillars</p>

<blockquote>
<pre><code>    kodi:
      server:
        enabled: True
</code></pre>
</blockquote>

<p>Usage</p>

<p>plugin repositories
  tvheadend</p>

<p>Read more</p>

<ul>
<li><a href="https://code.google.com/p/dmd-xbmc/">https://code.google.com/p/dmd-xbmc/</a></li>
<li><a href="http://kodi-czsk.github.io/repository/">http://kodi-czsk.github.io/repository/</a></li>
<li><a href="https://tvheadend.org/projects/tvheadend/wiki/AptRepository">https://tvheadend.org/projects/tvheadend/wiki/AptRepository</a></li>
<li><a href="https://kodi.tv/">https://kodi.tv/</a></li>
</ul>

<h3 id="filip-pytloun-55">Filip Pytloun<a href="#filip-pytloun-55" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Unify Makefile, .gitignore and update readme</p>

<h1 id="formula-kubernetes">Formula kubernetes</h1>

<h3 id="ales-komarek-16">Ales Komarek<a href="#ales-komarek-16" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Service update orchestrations</p>

<p>Kubernetes is an open-source system for automating deployment, scaling, and
  management of containerized applications. This formula deploys production
  ready Kubernetes and generate Kubernetes manifests as well.</p>

<p>Sample Pillars</p>

<p>More Information
  * <a href="https://github.com/Juniper/kubernetes/blob">https://github.com/Juniper/kubernetes/blob</a>
  /opencontrail-integration/docs /getting-started-guides/opencontrail.md
  * <a href="https://github.com/kubernetes/kubernetes/tree/master/cluster/saltbase">https://github.com/kubernetes/kubernetes/tree/master/cluster/saltbase</a></p>

<h3 id="andrey-shestakov">Andrey Shestakov<a href="#andrey-shestakov" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Add Designate support for externaldns.</p>

<p>This change allows to pass Openstack credentials to externaldns when provider
  is Designate.</p>

<p>Enable external DNS addon with Designate provider</p>

<blockquote>
<pre><code>    parameters:
      kubernetes:
        common:
          addons:
            externaldns:
              externaldns:
                enabled: True
                domain: company.mydomain
                provider: designate
                designate_os_options:
                  OS_AUTH_URL: https://keystone_auth_endpoint:5000
                  OS_PROJECT_DOMAIN_NAME: default
                  OS_USER_DOMAIN_NAME: default
                  OS_PROJECT_NAME: admin
                  OS_USERNAME: admin
                  OS_PASSWORD: password
                  OS_REGION_NAME: RegionOne
</code></pre>
</blockquote>

<p>Update virtlet deployment procedure.</p>

<p>Added dockershim service.</p>

<p>Removed kubelet.conf as not required.</p>

<p>Bump version to v0.8.0.</p>

<blockquote>
<pre><code>              image: mirantis/virtlet:v0.8.0
</code></pre>
</blockquote>

<h3 id="filip-pytloun-56">Filip Pytloun<a href="#filip-pytloun-56" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Add state to control rbac</p>

<p>Role-based access control</p>

<p>To enable RBAC, you need to set following option on your apiserver:</p>

<blockquote>
<pre><code>    kubernetes:
      master:
        auth:
          mode: RBAC
</code></pre>
</blockquote>

<p>Then you can use <code>kubernetes.control.role</code> state to orchestrate role and
  rolebindings. Following example shows how to create brand new role and binding
  for service account:</p>

<blockquote>
<pre><code>    control:
      role:
        etcd-operator:
          kind: ClusterRole
          rules:
            - apiGroups:
                - etcd.coreos.com
              resources:
                - clusters
              verbs:
                - &quot;*&quot;
                - extensions
                - thirdpartyresources
                - create
                - storage.k8s.io
                - storageclasses
                - &quot;&quot;
                - replicasets
          binding:
            etcd-operator:
              kind: ClusterRoleBinding
              namespace: test # &lt;-- if no namespace, then it's clusterrolebinding
              subject:
                etcd-operator:
                  kind: ServiceAccount
</code></pre>
</blockquote>

<p>Simplest possible use-case, add user test edit permissions on it&rsquo;s test</p>

<blockquote>
<pre><code>namespace:


      control:
        role:
          edit:
            kind: ClusterRole
            # No rules defined, so only binding will be created assuming role
            # already exists
            binding:
              test:
                namespace: test
                subject:
                  test:
                    kind: User
</code></pre>
</blockquote>

<p>Make auth options configurable</p>

<p>Enable basic, token and http authentication, disable ssl auth, create some
  static users:</p>

<blockquote>
<pre><code>          basic:
            enabled: true
            user:
              jdoe:
                password: dummy
                groups:
                  - system:admin
          http:
            header:
              user: X-Remote-User
              group: X-Remote-Group
          ssl:
            enabled: false
          token:
                token: dummytoken
</code></pre>
</blockquote>

<p>Unify Makefile, .gitignore and update readme</p>

<p>Revert &ldquo;update sources of k8s and calico installation&rdquo;</p>

<p>This reverts commit 76ecb2d41fc078054259ba68a7ba9720dbf02968.</p>

<h3 id="jakub-pavlik-1">Jakub Pavlik<a href="#jakub-pavlik-1" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Kubernetes StorageClass for AWS EBS</p>

<p>Kubernetes Storage Class</p>

<p>AWS EBS storageclass integration. It also requires to create IAM policy and profiles for instances and tag all resources by KubernetesCluster in EC2.</p>

<blockquote>
<pre><code>      common:
        addons:
          storageclass:
            aws_slow:
              name: slow
              enabled: True
              default: True
              provisioner: aws-ebs
              type: gp2
              iopspergb: &quot;10&quot;
              zones: xxx
</code></pre>
</blockquote>

<p>Enable Virtlet Addon for Kubernetes</p>

<p>Enable virtlet addon for kubernetes deployment.</p>

<blockquote>
<pre><code>  Initiative: PROD-10135
  Signed-off-by: Sergii Golovatiuk &lt;sgolovatiuk@mirantis.com&gt;
</code></pre>
</blockquote>

<p>Enable virtlet addon</p>

<blockquote>
<pre><code>        master:
            virtlet:
              enabled: true
              namespace: kube-system
              hosts:
              - cmp01
              - cmp02
              image: mirantis/virtlet:latest
</code></pre>
</blockquote>

<h3 id="marek-celoud">Marek Celoud<a href="#marek-celoud" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>change kubelet connection to secure endpoint/use ssl certs</p>

<p>Label node:</p>

<blockquote>
<pre><code>  kubernetes:
    master:
      label:
        label01:
          value: value01
          node: node01
          enabled: true
          key: key01
</code></pre>
</blockquote>

<h3 id="matthew-mosesohn">Matthew Mosesohn<a href="#matthew-mosesohn" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Add support for openstack cloudprovider</p>

<p>Enable OpenStack cloud provider</p>

<blockquote>
<pre><code>          cloudprovider:
            enabled: True
            type: openstack
            params:
              auth_url: https://openstack.mydomain:5000/v3
              username: nova
              password: nova
              region: RegionOne
              tenant_id: 4bce4162d8744c599e350099cfa22a0a
              domain_name: default
              subnet_id: 72407854-aca6-4cf1-b873-e9affb09484b
              lb_version: v2
</code></pre>
</blockquote>

<p>Add externaldns addon with CoreDNS support</p>

<p>Enable external DNS addon with CoreDNS provider</p>

<blockquote>
<pre><code>              coredns:
                provider: coredns
</code></pre>
</blockquote>

<p>Add state for kubernetes federation</p>

<p>Enable Kubenetes Federation control plane</p>

<blockquote>
<pre><code>          federation:
            name: federation
            namespace: federation-system
            source: https://dl.k8s.io/v1.6.6/kubernetes-client-linux-amd64.tar.gz
            hash: 94b2c9cd29981a8e150c187193bab0d8c0b6e906260f837367feff99860a6376
            service_type: NodePort
            dns_provider: coredns
            childclusters:
              - secondcluster.mydomain
              - thirdcluster.mydomain
</code></pre>
</blockquote>

<p>Set cluster name and domain
  cluster_name : mycluster</p>

<blockquote>
<pre><code>          cluster_name: cluster
</code></pre>
</blockquote>

<p>New variable kubernetes_cluster_domain</p>

<p>Use cluster_domain/domain variable consistently across all templated files.</p>

<p>All kubeconfig fields contain cluster domain for uniqueness.</p>

<p>Using kubernetes_cluster_domain param now to allow physical servers
  to have a different domain from kubernetes cluster if necessary, but
  the default keeps them the same.</p>

<p>Set cluster domain</p>

<blockquote>
<pre><code>          kubernetes_cluster_domain: mycluster.domain
</code></pre>
</blockquote>

<p>Refactor contrail-network-controller</p>

<blockquote>
<pre><code>          contrail_network_controller:
            image: yashulyak/contrail-controller:latest
</code></pre>
</blockquote>

<p>Add mtu for calico</p>

<blockquote>
<pre><code>            mtu: 1500
          mtu: 1500
</code></pre>
</blockquote>

<p>add calico policy controller</p>

<p>Also fix check for add-on if its namespace is not kube-system.</p>

<p>Enable calico-policy addon</p>

<blockquote>
<pre><code>            calico_policy:
</code></pre>
</blockquote>

<p>By default kube-apiserver, kube-scheduler, kube-controllermanager, kube-proxy, etcd running in docker containers through manifests. For stable production environment this should be run in systemd.</p>

<p>Running with calico-policy controller:</p>

<blockquote>
<pre><code>      pool:
        network:
          engine: calico
</code></pre>
</blockquote>

<h3 id="petr-michalec-9">Petr Michalec<a href="#petr-michalec-9" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>add PV storageclass</p>

<blockquote>
<pre><code>            nfs_shared:
              name: elasti01
              provisioner: nfs
              spec:
                name: elastic_data
                nfs:
                  server: 10.0.0.1
                  path: /exported_path
</code></pre>
</blockquote>

<h3 id="sergii-golovatiuk">Sergii Golovatiuk<a href="#sergii-golovatiuk" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Add Google CloudDNS support to external-dns</p>

<blockquote>
<pre><code>            coredns:
              domain: company.mydomain
              provider: coredns
              provider: designate
              designate_os_options:
                OS_AUTH_URL: https://keystone_auth_endpoint:5000
                OS_PROJECT_DOMAIN_NAME: default
                OS_USER_DOMAIN_NAME: default
                OS_PROJECT_NAME: admin
                OS_USERNAME: admin
                OS_PASSWORD: password
                OS_REGION_NAME: RegionOne
              provider: aws
              aws_options:
                AWS_ACCESS_KEY_ID: XXXXXXXXXXXXXXXXXXXX
                AWS_SECRET_ACCESS_KEY: XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
</code></pre>
</blockquote>

<p>Enable external DNS addon with Google CloudDNS provider</p>

<blockquote>
<pre><code>              provider: google
              google_options:
                key: ''
                project: default-123
</code></pre>

<p>key should be exported from google console and processed as <code>cat key.json | tr -d '\n'</code></p>
</blockquote>

<p>Add AWS support for externaldns.</p>

<p>Enable external DNS addon with AWS provider</p>

<blockquote>
<pre><code>                provider: aws
                aws_options:
                  AWS_ACCESS_KEY_ID: XXXXXXXXXXXXXXXXXXXX
                  AWS_SECRET_ACCESS_KEY: XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
</code></pre>
</blockquote>

<p>Move addons to common section</p>

<p>Doc-Impact</p>

<blockquote>
<pre><code>            image: gcr.io/google_containers/hyperkube:v1.6.5
              image: mirantis/virtlet:v0.7.0
          namespace:
</code></pre>
</blockquote>

<h3 id="swann-croiset">Swann Croiset<a href="#swann-croiset" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Explicitly configure the insecure_port for apiserver</p>

<blockquote>
<pre><code>            secure_port: 443
            insecure_address: 127.0.0.1
            insecure_port: 8080
</code></pre>
</blockquote>

<h3 id="tomáš-kukrál-5">Tomáš Kukrál<a href="#tomáš-kukrál-5" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>minor fixes to cloudprovider features</p>

<blockquote>
<pre><code>            provider: openstack
</code></pre>
</blockquote>

<p>fix typho in readme
  ssh cfg01 &ldquo;sudo ssh ctl01 /etc/kubernetes/kubeconfig.sh&rdquo; &gt; ~/.kube/config</p>

<p>fix hardcoded contrail-api server</p>

<blockquote>
<pre><code>          config:
            api:
              host: 10.0.170.70
</code></pre>
</blockquote>

<p>add kubeconfig generator
  ready Kubernetes and generate Kubernetes manifests as well.</p>

<p>You can download <code>kubectl</code> configuration and connect to your cluster. However,
  keep in mind <code>kubernetes_control_address</code> needs to be accessible from your computer:</p>

<p>mkdir -p ~/.kube
  [ -f ~/.kube/config ] &amp;&amp; cp -v ~/.kube/config ~/.kube/config-backup
  ssh cfg01 &ldquo;sudo ssh ctl01 /etc/kubenetes/kubeconfig.sh&rdquo; &gt; ~/.kube/config
  kubectl get no</p>

<p><code>cfg01</code> is Salt master node and <code>ctl01</code> is one of Kubernetes masters</p>

<p>switch kube-network-manager to addon</p>

<blockquote>
<pre><code>          kube_network_manager:
            namespace: kube-system
          image: yashulyak/contrail-controller:latest
</code></pre>
</blockquote>

<p>Dashboard public IP must be configured when Contrail network is used:</p>

<blockquote>
<pre><code>          public_ip: 1.1.1.1
</code></pre>
</blockquote>

<p>remove yaml apply</p>

<p>apply custom yaml files</p>

<p>Apply custom yaml files</p>

<blockquote>
<pre><code>        services:
          myservice:
            files:
              - /srv/kubernetes/myservice-svc.yml
              - /srv/kubernetes/myservice-pvc.yml
              - /srv/kubernetes/myservice-deploy.yml
</code></pre>
</blockquote>

<p>add parameter for verbosity</p>

<p>This PR add posibility to set verbosity of kube daemons. Default value
  is 2.</p>

<p>Configure service verbosity</p>

<blockquote>
<pre><code>          verbosity: 2
        pool:
</code></pre>
</blockquote>

<p>update metadata for dns autoscaler</p>

<p>Using <code>enabled: True</code> to conform with the rest of metadata structures.</p>

<p>Introductiong poll-perios-seconds parameter</p>

<p>Enable autoscaler for dns addon. Poll period can be skipped.</p>

<blockquote>
<pre><code>            dns:
              domain: cluster.local
              replicas: 1
              server: 10.254.0.10
              autoscaler:
                enabled: true
                poll-period-seconds: 60
</code></pre>
</blockquote>

<p>enable prometheus metrics in felix</p>

<p>Calico Felix can listen on port 9091 (configurable) and export
  prometheus metrics here.</p>

<p>Enable Prometheus metrics in Felix</p>

<blockquote>
<pre><code>          prometheus:
</code></pre>
</blockquote>

<p>add netchecker addon</p>

<p>Enable helm-tiller addon</p>

<p>Enable netchecker addon</p>

<blockquote>
<pre><code>            netchecker:
</code></pre>
</blockquote>

<p>add helm-tiller as addon</p>

<p>Enable helm-tiller addons</p>

<blockquote>
<pre><code>            helm:
</code></pre>
</blockquote>

<p>syntax fix in README</p>

<p>update version in README</p>

<blockquote>
<pre><code>            image: gcr.io/google_containers/hyperkube:v1.5.2
</code></pre>
</blockquote>

<p>add support for etcd over ssl</p>

<p>Running with secured etcd:</p>

<blockquote>
<pre><code>          etcd:
            ssl:
</code></pre>
</blockquote>

<p>allow to pass options to daemons</p>

<p>Requested for labs deployment.</p>

<p>Pass aditional parameters to daemons:</p>

<blockquote>
<pre><code>          apiserver:
            daemon_opts:
              storage-backend: pigeon
          controller_manager:
              log-dir: /dev/nulL
          kubelet:
              max-pods: &quot;6&quot;
</code></pre>
</blockquote>

<p>update sources of k8s and calico installation
  move basic k8s setup to common
  copy cni from hyperkube
  configurable calico node image
  use calico/cni image for obtaining cnis
  use calico/ctl image for obtaining calicoctl binary
  add cross requirement for k8s services and hyperkube
  update metadata for new pillar model
  update manifests to use hyperkube from common
  <strong>REQUIRED:</strong> Define image to use for hyperkube, CNIs and calicoctl image</p>

<blockquote>
<pre><code>          hyperkube:
            image: gcr.io/google_containers/hyperkube:v1.4.6
          network:
            calicoctl:
              image: calico/ctl
            cni:
              image: calico/cni
</code></pre>
</blockquote>

<h1 id="formula-leonardo">Formula leonardo</h1>

<h3 id="ales-komarek-17">Ales Komarek<a href="#ales-komarek-17" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Support for single app management</p>

<p>Django-Leonardo formula</p>

<p>Python/django based CMS.</p>

<p>Sample metadata</p>

<p>Without setting formula produce somethink like this <code>Example app</code> from your
  site name <code>site_name</code></p>

<p>More information</p>

<h3 id="filip-pytloun-57">Filip Pytloun<a href="#filip-pytloun-57" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Unify Makefile, .gitignore and update readme
  * <a href="https://github.com/leonardo-modules/leonardo-auth-ldap">https://github.com/leonardo-modules/leonardo-auth-ldap</a></p>

<h3 id="jakub-josef-1">Jakub Josef<a href="#jakub-josef-1" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Added posibility to disable strict host checking on proxy nodes.</p>

<blockquote>
<pre><code>            # disable strict host check on nginx proxy at app node
            dev: true
</code></pre>
</blockquote>

<h1 id="formula-letsencrypt">Formula letsencrypt</h1>

<h3 id="alexander-bauer">Alexander Bauer<a href="#alexander-bauer" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Add <code>install_units</code> option to install Systemd units even when <code>method: pkg</code> (#16)</p>

<p>This can be passed in the case that the provided package does not
  include the <code>.service</code> and <code>.timer</code> files.</p>

<p>If the <code>certbot</code> package doesn&rsquo;t include Systemd <code>.service</code> and
  <code>.timer</code> files, you can set them to be installed by this formula by
  supplying <code>install_units: True</code> and <code>cli</code>.</p>

<blockquote>
<pre><code>      letsencrypt:
        client:
          source:
            engine: pkg
            cli: /usr/bin/certbot
            install_units: true
</code></pre>
</blockquote>

<h3 id="filip-pytloun-58">Filip Pytloun<a href="#filip-pytloun-58" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Update name for multidomain</p>

<blockquote>
<pre><code>          # Following will produce multidomain certificate:
</code></pre>

<p>site.dummy.org:</p>

<pre><code>            enabled: true
            names:
              - dummy.org
              - www.dummy.org
</code></pre>
</blockquote>

<p>You are able to use multidomain certificates:</p>

<blockquote>
<pre><code>    letsencrypt:
      client:
        email: sylvain@home
        staging: true
        auth:
          method: apache
        domain:
          keynotdomain:
            name: ls.opensource-expert.com
            - www.ls.opensource-expert.com
            - vim22.opensource-expert.com
            - www.vim22.opensource-expert.com
</code></pre>

<p>rm.opensource-expert.com:</p>

<pre><code>            - www.rm.opensource-expert.com
</code></pre>

<p>vim7.opensource-expert.com:</p>

<pre><code>            - www.vim7.opensource-expert.com
</code></pre>

<p>vim88.opensource-expert.com:</p>

<pre><code>            - www.vim88.opensource-expert.com
            - awk.opensource-expert.com
            - www.awk.opensource-expert.com
</code></pre>
</blockquote>

<p>Unify Makefile, .gitignore and update readme</p>

<p>Fix metadata model</p>

<p>Installation</p>

<p>Usage</p>

<p>Default authentication method using standalone server on specified port.</p>

<p>But this won&rsquo;t work without configuration of apache/nginx (read on) unless you
  don&rsquo;t have webserver running so you can select port 80 or 443.</p>

<blockquote>
<pre><code>        email: root@dummy.org
          method: standalone
          type: http-01
          port: 9999
</code></pre>

<p>dummy.org:</p>
</blockquote>

<p>www.dummy.org:</p>

<p>However ACME server always visits port 80 (or 443) where most likely Apache or</p>

<p>Nginx is listening. This means that you need to configure
  <code>/.well-known/acme-challenge/</code> to proxy requests on localhost:9999.</p>

<p>For example, ensure you have following configuration for Apache:</p>

<blockquote>
<pre><code>::
</code></pre>
</blockquote>

<p>ProxyPass &ldquo;/.well-known/acme-challenge/&rdquo; &ldquo;<a href="http://127.0.0.1:9999/.well-known/acme-challenge/&quot;">http://127.0.0.1:9999/.well-known/acme-challenge/&quot;</a> retry=1</p>

<p>ProxyPassReverse &ldquo;/.well-known/acme-challenge/&rdquo; &ldquo;<a href="http://127.0.0.1:9999/.well-known/acme-challenge/&quot;">http://127.0.0.1:9999/.well-known/acme-challenge/&quot;</a></p>

<p><Location "/.well-known/acme-challenge/"></p>

<p>ProxyPreserveHost On</p>

<p>Order allow,deny</p>

<p>Allow from all</p>

<p>Require all granted
  </Location></p>

<p>You can also use <code>apache</code> or <code>nginx</code> auth methods and let certbot do
  what&rsquo;s needed, this should be the simplest option.</p>

<blockquote>
<pre><code>        auth: apache
</code></pre>
</blockquote>

<p>Alternatively you can use webroot authentication (using eg. existing apache
  installation serving directory for all sites):</p>

<blockquote>
<pre><code>          method: webroot
          path: /var/www/html
          port: 80
</code></pre>
</blockquote>

<p>It&rsquo;s also possible to override auth method or other options only for single</p>

<blockquote>
<pre><code>domain:


            auth:
              method: webroot
              path: /var/www/html/dummy.org
              port: 80
</code></pre>
</blockquote>

<p>Legacy configuration</p>

<ul>
<li><code>Certbot authentication plugins &lt;https://letsencrypt.readthedocs.io/en/latest/using.html#getting-certificates-and-choosing-plugins&gt;</code>_</li>
</ul>

<p>Use certbot and add more installation options</p>

<blockquote>
<pre><code>  Fixes: #1
</code></pre>
</blockquote>

<p>There are 3 installation methods available:</p>

<blockquote>
<pre><code>- package (default for Debian)
</code></pre>
</blockquote>

<p>For Debian Jessie, you need to use jessie-backports repository. For Ubuntu,
  use Launchpad PPA providing certbot package. You can use linux formula to
  manage these APT sources.</p>

<blockquote>
<pre><code>- URL to certbot-auto (default)
</code></pre>
</blockquote>

<p>This is default installation method for systems with no available certbot
  package.</p>

<blockquote>
<pre><code>            engine: url
            url: &quot;https://dl.eff.org/certbot-auto&quot;


- Docker container
</code></pre>
</blockquote>

<p>Alternate installation method where Docker image is used to provide certbot
  tool and executed using wrapper script.</p>

<blockquote>
<pre><code>            engine: docker
            image: &quot;deliverous/certbot&quot;
</code></pre>
</blockquote>

<p>Common metadata:</p>

<h1 id="formula-libvirt">Formula libvirt</h1>

<h3 id="filip-pytloun-59">Filip Pytloun<a href="#filip-pytloun-59" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Unify Makefile, .gitignore and update readme
  * <a href="https://github.com/bechtoldt/saltstack-libvirt-formula">https://github.com/bechtoldt/saltstack-libvirt-formula</a></p>

<h3 id="michael-polenchuk">Michael Polenchuk<a href="#michael-polenchuk" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Get unix_sock_group &amp; config_sys as an option</p>

<blockquote>
<pre><code>        unix_sock_group: libvirt
</code></pre>
</blockquote>

<h1 id="formula-linux">Formula linux</h1>

<h3 id="ales-komarek-18">Ales Komarek<a href="#ales-komarek-18" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Allow mining for the dns records for local hosts records</p>

<p>Use <code>/etc/environment</code> for static system wide variable assignment after
  boot. Variable expansion is frequently not supported.</p>

<p>The profile.d scripts are being sourced during .sh execution and support
  variable expansion in opposite to /etc/environment global settings in
  <code>/etc/environment</code>.</p>

<p>Linux with hosts collected from mine</p>

<p>In this case all dns records defined within infrastrucuture will be passed to
  local hosts records or any DNS server. Only hosts with <code>grain</code> parameter to
  true will be propagated to the mine.</p>

<blockquote>
<pre><code>    linux:
      network:
        purge_hosts: true
        mine_dns_records: true
        host:
          node1:
            address: 192.168.10.200
            grain: true
            names:
            - node2.domain.com
            - service2.domain.com
</code></pre>

<p>setting custom TX queue length for tap interfaces</p>
</blockquote>

<h3 id="aleš-komárek-12">Aleš Komárek<a href="#aleš-komárek-12" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Update README.rst</p>

<p>Linux Fomula</p>

<p>Sample Pillars</p>

<p>Linux System</p>

<p>Cmds_Alias {{ alias }}={{ commands }}
  saltuser1 ALL=(ALL) NOPASSWD: ALL</p>

<h3 id="andrey-shestakov-1">Andrey Shestakov<a href="#andrey-shestakov-1" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Allow configure systemd</p>

<p>Systemd settings:</p>

<blockquote>
<pre><code>      system:
</code></pre>

<p>&hellip;</p>

<pre><code>        systemd:
          system:
            Manager:
              DefaultLimitNOFILE: 307200
              DefaultLimitNPROC: 307200
          user:
              DefaultLimitCPU: 2
              DefaultLimitNPROC: 4
</code></pre>
</blockquote>

<h3 id="andrii-petrenko">Andrii Petrenko<a href="#andrii-petrenko" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<blockquote>
<pre><code>  Feature: automatically set txqueuelen for all tap* network interfaces
  Config:
  linux:
</code></pre>

<p>in case of configuration parameter defined will create file:
  with content:</p>
</blockquote>

<p>KERNEL==”tap[0-9a-z-]*&ldquo;, RUN+=&rdquo;/sbin/ip link set %k txqueuelen 10000&rdquo;
  <strong>setting custom TX queue length for tap interfaces</strong></p>

<blockquote>
<pre><code>        tap_custom_txqueuelen: 10000
</code></pre>
</blockquote>

<h3 id="azvyagintsev">azvyagintsev<a href="#azvyagintsev" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Extend linux.user</p>

<p>Linux with system users, some with password set:</p>

<p>If no &lsquo;password&rsquo; variable has been passed - any predifined password
  will be removed.</p>

<blockquote>
<pre><code>            full_name: 'With clear password'
            hash_password: true
            password: &quot;userpassword&quot;
          mark:
            name: 'mark'
            enabled: true
            full_name: &quot;unchange password'
            home: '/home/mark'
            password: false
          elizabeth:
            name: 'elizabeth'
            full_name: 'With hased password'
            home: '/home/elizabeth'
            password: &quot;$6$nUI7QEz3$dFYjzQqK5cJ6HQ38KqG4gTWA9eJu3aKx6TRVDFh6BVJxJgFWg2akfAA7f1fCxcSUeOJ2arCO6EEI6XXnHXxG10&quot;
</code></pre>
</blockquote>

<h3 id="bruno-binet">Bruno Binet<a href="#bruno-binet" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Add system.autoupdates state (#61)
  only Debian-based systems are supported for now</p>

<p>Linux with autoupdates (automatically install security package updates)</p>

<blockquote>
<pre><code>        autoupdates:
          enabled: true
          mail: root@localhost
          mail_only_on_error: true
          remove_unused_dependencies: false
          automatic_reboot: true
          automatic_reboot_time: &quot;02:00&quot;
</code></pre>
</blockquote>

<h3 id="dmitry-stremkouski-3">Dmitry Stremkouski<a href="#dmitry-stremkouski-3" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>One should be able to change ovs port type if its not virtual.</p>

<p>If ovs port is virtual, we use OVSIntPort to create it.</p>

<p>Otherwise it should be OVSPort.</p>

<p>I&rsquo;ve added new key: ovs_port_type to not intersect with current
  deployments and not hurt anyone.</p>

<p>I&rsquo;ve updated doc to have an example of ovs peering patch.</p>

<p>Customer-Found</p>

<blockquote>
<pre><code>          br-prv:
            type: ovs_bridge
            mtu: 65000
          br-ens7:
            name: br-ens7
            proto: manual
            mtu: 9000
            use_interfaces:
            - ens7
          patch-br-ens7-br-prv:
            name: ens7-prv
            ovs_type: ovs_port
            type: ovs_port
            bridge: br-ens7
            port_type: patch
            peer: prv-ens7
          patch-br-prv-br-ens7:
            name: prv-ens7
            bridge: br-prv
            peer: ens7-prv
          ens7:
            name: ens7
            ovs_port_type: OVSPort
            ovs_bridge: br-ens7
</code></pre>
</blockquote>

<h3 id="filip-pytloun-60">Filip Pytloun<a href="#filip-pytloun-60" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Add linux.system.apt state</p>

<p>Setup custom apt config options:</p>

<blockquote>
<pre><code>        apt:
          config:
            compression-workaround:
</code></pre>

<p>&ldquo;Acquire::CompressionTypes::Order&rdquo;: &ldquo;gz&rdquo;</p>

<pre><code>            docker-clean:
</code></pre>

<p>&ldquo;DPkg::Post-Invoke&rdquo;:</p>

<pre><code>                - &quot;rm -f /var/cache/apt/archives/*.deb /var/cache/apt/archives/partial/*.deb /var/cache/apt/*.bin || true&quot;
</code></pre>

<p>&ldquo;APT::Update::Post-Invoke&rdquo;:</p>
</blockquote>

<p>Add linux.system.directory state</p>

<p>Ensure presence of directory:</p>

<blockquote>
<pre><code>        directory:
</code></pre>

<p>/tmp/test:</p>

<pre><code>            user: root
            group: root
            mode: 700
            makedirs: true
</code></pre>
</blockquote>

<p>Add ca_certificates into readme</p>

<blockquote>
<pre><code>  Fixes: #120
</code></pre>
</blockquote>

<p>Certificates
  ~~~~~~~~~~~~</p>

<p>Add certificate authority into system trusted CA bundle</p>

<blockquote>
<pre><code>        ca_certificates:
          mycert: |
</code></pre>
</blockquote>

<p>MIICPDCCAaUCEHC65B0Q2Sk0tjjKewPMur8wDQYJKoZIhvcNAQECBQAwXzELMAkG</p>

<p>A1UEBhMCVVMxFzAVBgNVBAoTDlZlcmlTaWduLCBJbmMuMTcwNQYDVQQLEy5DbGFz
  cyAzIFB1YmxpYyBQcmltYXJ5IENlcnRpZmljYXRpb24gQXV0aG9yaXR5MB4XDTk2</p>

<p>MDEyOTAwMDAwMFoXDTI4MDgwMTIzNTk1OVowXzELMAkGA1UEBhMCVVMxFzAVBgNV</p>

<p>BAoTDlZlcmlTaWduLCBJbmMuMTcwNQYDVQQLEy5DbGFzcyAzIFB1YmxpYyBQcmlt</p>

<p>YXJ5IENlcnRpZmljYXRpb24gQXV0aG9yaXR5MIGfMA0GCSqGSIb3DQEBAQUAA4GN</p>

<p>ADCBiQKBgQDJXFme8huKARS0EN8EQNvjV69qRUCPhAwL0TPZ2RHP7gJYHyX3KqhE</p>

<p>BarsAx94f56TuZoAqiN91qyFomNFx3InzPRMxnVx0jnvT0Lwdd8KkMaOIG+YD/is</p>

<p>I19wKTakyYbnsZogy1Olhec9vn2a/iRFM9x2Fe0PonFkTGUugWhFpwIDAQABMA0G</p>

<p>CSqGSIb3DQEBAgUAA4GBALtMEivPLCYATxQT3ab7/AoRhIzzKBxnki98tsX63/Do
  lbwdj2wsqFHMc9ikwFPwTtYmwHYBV4GSXiHx0bH/59AhWM1pF+NEHJwZRDmJXNyc</p>

<p>AA9WjQKZ7aKQRUzkuxCkPfAyAw7xzvjoyVGM5mKf5p/AfbdynMk2OmufTqj/ZA1k</p>

<p>Add support for sysfs</p>

<p>Sysfs
  ~~~~~</p>

<p>Install sysfsutils and set sysfs attributes:</p>

<blockquote>
<pre><code>        sysfs:
          scheduler:
</code></pre>

<p>block/sda/queue/scheduler: deadline</p>

<pre><code>          power:
            mode:
</code></pre>

<p>power/state: 0660</p>

<pre><code>            owner:
</code></pre>

<p>power/state: &ldquo;root:power&rdquo;
  devices/system/cpu/cpu0/cpufreq/scaling_governor: powersave</p>
</blockquote>

<p>Use cron job identifier to be able to update command</p>

<p>By default it will use name as an identifier, unless identifier key is
  explicitly set or False (then it will use Salt&rsquo;s default behavior which is
  identifier same as command resulting in not being able to change it)</p>

<blockquote>
<pre><code>            identifier: cmd1
</code></pre>
</blockquote>

<p>Unify Makefile, .gitignore and update readme</p>

<p>Allow enforcing of whole /etc/hosts</p>

<p>Parameter purge_hosts will enforce whole /etc/hosts file, removing entries
  that are not defined in model except defaults for both IPv4 and IPv6 localhost
  and hostname + fqdn.</p>

<p>It&rsquo;s good to use this option if you want to ensure /etc/hosts is always in a
  clean state however it&rsquo;s not enabled by default for safety.</p>

<blockquote>
<pre><code>          # No need to define this one if purge_hosts is true
          hostname:
            address: 127.0.1.1
            - ${linux:network:fqdn}
            - ${linux:network:hostname}
</code></pre>
</blockquote>

<h3 id="jakub-pavlik-2">Jakub Pavlik<a href="#jakub-pavlik-2" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Extend by mkfs for XFS filesystem</p>

<blockquote>
<pre><code>                  mkfs: True
                  type: xfs
</code></pre>
</blockquote>

<p>Disk partitioning</p>

<p>Ability to create partitions on empty disks.</p>

<p>Create partitions on disk. Specify size in MB. It expects empty
  disk without any existing partitions.</p>

<blockquote>
<pre><code>      linux:
        storage:
          disk:
            first_drive:
              name: /dev/loop1
              type: gpt
              partitions:
                - size: 200 #size in MB
                  type: fat32
                - size: 300 #size in MB
                  type: ext4
</code></pre>

<p>/dev/vda1:</p>

<pre><code>                - size: 5
                  type: ext2
                - size: 10
</code></pre>
</blockquote>

<h3 id="jakub-josef-2">Jakub Josef<a href="#jakub-josef-2" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Added ability to enable SETENV in sudoers</p>

<blockquote>
<pre><code>            setenv: true # Enable sudo -E option
</code></pre>
</blockquote>

<p>Linux OVS dpdk vxlan tunnel endpoint ip address</p>

<p>Add possiblity to add ip address and mtu on dpdk ovs bridge to
  be able use VXLAN as tenant segmentation.</p>

<blockquote>
<pre><code>  Epic: PROD-8957
</code></pre>

<p><strong>DPDK OVS bridge for VXLAN</strong></p>
</blockquote>

<p>If VXLAN is used as tenant segmentation then ip address must be set on br-prv</p>

<blockquote>
<pre><code>        interface:
            type: dpdk_ovs_bridge
            address: 192.168.50.0
            netmask: 255.255.255.0
</code></pre>
</blockquote>

<p>Linux OVS-dpdk and multiqueue support</p>

<p>Introduce dpdk support for linux OVS configuration.</p>

<p>It configures dpdk interface bind, ovs dpdk ports, bonding,
  parameters for dpdk cpu pmd and set multique queues for specific
  ovs dpdk interfaces.</p>

<blockquote>
<pre><code>  Epic: PROD-8958
          - 8.8.4.4
          - 8.8.8.8
          - my.example.com
          - example.com
          - ndots: 5
          - timeout: 2
          - attempts: 2
</code></pre>
</blockquote>

<p>DPDK OVS interfaces</p>

<p><strong>DPDK OVS NIC</strong></p>

<blockquote>
<pre><code>        bridge: openvswitch
        dpdk:
          driver: uio/vfio-pci
        openvswitch:
          pmd_cpu_mask: &quot;0x6&quot;
          dpdk_socket_mem: &quot;1024,1024&quot;
          dpdk_lcore_mask: &quot;0x400&quot;
          memory_channels: 2
          dpkd0:
            name: ${_param:dpdk_nic}
            pci: 0000:06:00.0
            driver: igb_uio/vfio
            type: dpdk_ovs_port
            n_rxq: 2
</code></pre>
</blockquote>

<p><strong>DPDK OVS Bond</strong></p>

<blockquote>
<pre><code>          dpdk_second_nic:
            name: ${_param:primary_second_nic}
            bond: dpdkbond0
          dpdk_first_nic:
            name: ${_param:primary_first_nic}
            pci: 0000:05:00.0
          dpdkbond0:
            type: dpdk_ovs_bond
            mode: active-backup
</code></pre>
</blockquote>

<p>Linux storage</p>

<p>File swap configuration</p>

<p>Partition swap configuration</p>

<p>Implement isolcpu grub configuration</p>

<p>Separate cpu for host os from workload.</p>

<blockquote>
<pre><code>  Epic: PROD-8959
</code></pre>
</blockquote>

<p>Isolate CPU options
  ~~~~~~~~~~~~~~~~~~~</p>

<p>Remove the specified CPUs, as defined by the cpu_number values, from the general kernel</p>

<p>SMP balancing and scheduler algroithms. The only way to move a process onto or off an
  &ldquo;isolated&rdquo; CPU is via the CPU affinity syscalls. cpu_number begins at 0, so the
  maximum value is 1 less than the number of CPUs on the system.</p>

<blockquote>
<pre><code>        kernel:
          isolcpu: 1,2,3,4,5,6,7 # isolate first cpu 0
</code></pre>
</blockquote>

<p>SRIOV support</p>

<p>Enable SR-IOV support on server.</p>

<blockquote>
<pre><code>  Epic: PROD-8956
</code></pre>
</blockquote>

<p>Intel SR-IOV</p>

<p>PCI-SIG Single Root I/O Virtualization and Sharing (SR-IOV) specification defines a standardized mechanism to virtualize PCIe devices. The mechanism can virtualize a single PCIe Ethernet controller to appear as multiple PCIe devices.</p>

<blockquote>
<pre><code>          sriov: True
          unsafe_interrupts: False # Default is false. for older platforms and AMD we need to add interrupt remapping workaround
        rc:
          local: |
            #!/bin/sh -e
            # Enable 7 VF on eth1
</code></pre>

<p>echo 7 &gt; /sys/class/net/eth1/device/sriov_numvfs; sleep 2; ifup -a
  exit 0</p>
</blockquote>

<p>Hugepages support</p>

<p>Grub hugepages configuration and mount point action.</p>

<p>Huge Pages</p>

<p>Huge Pages give a performance boost to applications that intensively deal
  with memory allocation/deallocation by decreasing memory fragmentation.</p>

<blockquote>
<pre><code>          hugepages:
            small:
              size: 2M
              count: 107520
              mount_point: /mnt/hugepages_2MB
              mount: false/true # default false
            large:
              default: true # default automatically mounted
              size: 1G
              count: 210
              mount_point: /mnt/hugepages_1GB


Note: not recommended to use both pagesizes in concurrently.
</code></pre>
</blockquote>

<h3 id="jiri-broulik-6">Jiri Broulik<a href="#jiri-broulik-6" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>README fix for purging repos</p>

<p>Remove all repositories:</p>

<blockquote>
<pre><code>        purge_repos: true
</code></pre>
</blockquote>

<p>nfs storage mount</p>

<p>NFS mount</p>

<blockquote>
<pre><code>    storage:
      enabled: true
      mount:
        nfs_glance:
          path: /var/lib/glance/images
          device: 172.16.10.110:/var/nfs/glance
          file_system: nfs
          opts: rw,sync
</code></pre>
</blockquote>

<p>cpu governor</p>

<p>CPU
  ~~~</p>

<p>Disable ondemand cpu mode service:</p>

<blockquote>
<pre><code>        cpu:
          governor: performance
</code></pre>
</blockquote>

<h3 id="marek-celoud-1">Marek Celoud<a href="#marek-celoud-1" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>add service management support</p>

<p>Services
  ~~~~~~~~</p>

<p>Stop and disable linux service:</p>

<blockquote>
<pre><code>        service:
</code></pre>

<p>apt-daily.timer:</p>

<pre><code>            status: dead
</code></pre>
</blockquote>

<p>Possible status is dead (disable service by default), running (enable service by default), enabled, disabled.</p>

<h3 id="oleg-bondarev">Oleg Bondarev<a href="#oleg-bondarev" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>This is required in order to use vfio-pci driver.</p>

<p>More info: <a href="http://docs.openvswitch.org/en/latest/intro/install/dpdk/">http://docs.openvswitch.org/en/latest/intro/install/dpdk/</a></p>

<blockquote>
<pre><code>          driver: uio/vfio
            driver: igb_uio/vfio-pci
</code></pre>
</blockquote>

<p>This enables a more fine tuned dpdk for better performance.</p>

<p>More details on pmd-rxq-affinity config:</p>

<blockquote>
<pre><code>  http://docs.openvswitch.org/en/latest/howto/dpdk/
            pmd_rxq_affinity: &quot;0:1,1:2&quot;
</code></pre>
</blockquote>

<h3 id="oleksandr-vlasov">Oleksandr Vlasov<a href="#oleksandr-vlasov" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Add dhclient basic configuration</p>

<blockquote>
<pre><code>          # package manager fallback defaults
</code></pre>
</blockquote>

<p>DHCP client configuration</p>

<p>None of the keys is mandatory, include only those you really need. For full list
  of available options under send, supersede, prepend, append refer to dhcp-options(5)</p>

<blockquote>
<pre><code>     linux:
       network:
         dhclient:
           enabled: true
           backoff_cutoff: 15
           initial_interval: 10
           reboot: 10
           retry: 60
           select_timeout: 0
           timeout: 120
           send:
             - option: host-name
               declaration: &quot;= gethostname()&quot;
           supersede:
               declaration: &quot;spaceship&quot;
             - option: domain-name
               declaration: &quot;domain.home&quot;
             #- option: arp-cache-timeout
             #  declaration: 20
           prepend:
             - option: domain-name-servers
               declaration:
                 - 8.8.8.8
                 - 8.8.4.4
             - option: domain-search
                 - example.com
                 - eng.example.com
           #append:
             #- option: domain-name-servers
             #  declaration: 127.0.0.1
           # ip or subnet to reject dhcp offer from
           reject:
             - 192.33.137.209
             - 10.0.2.0/24
           request:
             - subnet-mask
             - broadcast-address
             - time-offset
             - routers
             - domain-name
             - domain-name-servers
             - domain-search
             - host-name
             - dhcp6.name-servers
             - dhcp6.domain-search
             - dhcp6.fqdn
             - dhcp6.sntp-servers
             - netbios-name-servers
             - netbios-scope
             - interface-mtu
             - rfc3442-classless-static-routes
             - ntp-servers
           require:
           # if per interface configuration required add below
           interface:
             ens2:
               initial_interval: 11
               reject:
                 - 192.33.137.210
             ens3:
               initial_interval: 12
                 - 192.33.137.211
</code></pre>
</blockquote>

<h3 id="petr-jediný-3">Petr Jediný<a href="#petr-jediný-3" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Introduce ipflush_onchange</p>

<p>Debian manual proto interfaces</p>

<p>When you are changing interface proto from static in up state to manual, you
  may need to flush ip addresses. For example, if you want to use the interface
  and the ip on the bridge. This can be done by setting the <code>ipflush_onchange</code>
  to true.</p>

<blockquote>
<pre><code>          eth1:
            type: eth
            mtu: 9100
            ipflush_onchange: true
</code></pre>
</blockquote>

<p>Add concat and remove for interfaces.d</p>

<p>Concatinating and removing interface files</p>

<p>Debian based distributions have <code>/etc/network/interfaces.d/</code> directory, where
  you can store configuration of network interfaces in separate files. You can
  concatinate the files to the defined destination when needed, this operation
  removes the file from the <code>/etc/network/interfaces.d/</code>. If you just need to
  remove iface files, you can use the <code>remove_iface_files</code> key.</p>

<blockquote>
<pre><code>        concat_iface_files:
        - src: '/etc/network/interfaces.d/50-cloud-init.cfg'
          dst: '/etc/network/interfaces'
        remove_iface_files:
        - '/etc/network/interfaces.d/90-custom.cfg'
</code></pre>
</blockquote>

<h3 id="petr-michalec-10">Petr Michalec<a href="#petr-michalec-10" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Add linux.network.systemd support</p>

<p>Linux network systemd settings:</p>

<blockquote>
<pre><code>          link:
            10-iface-dmz:
              Match:
                MACAddress: c8:5b:67:fa:1a:af
                OriginalName: eth0
              Link:
                Name: dmz0
          netdev:
            20-bridge-dmz:
              match:
                name: dmz0
              network:
                mescription: bridge
                bridge: br-dmz0
          network:
          # works with lowercase, keys are by default capitalized
            40-dhcp:
                name: '*'
                DHCP: yes
</code></pre>
</blockquote>

<p>Add system.env, system.profile, system.proxy and configure proxy under system.repo</p>

<p>Package manager proxy setup globally:</p>

<blockquote>
<pre><code>        repo:
          apt-mk:
            source: &quot;deb http://apt-mk.mirantis.com/ stable main salt&quot;


        proxy:
          pkg:
            ftp:   ftp://ftp-proxy-for-apt.host.local:2121


          # NOTE: Global defaults for any other componet that configure proxy on the system.
          #       If your environment has just one simple proxy, set it on linux:system:proxy.
          #
          # fall back system defaults if linux:system:proxy:pkg has no protocol specific entries
          # as for https and http
          ftp:   ftp://proxy.host.local:2121
          http:  http://proxy.host.local:3142
          https: https://proxy.host.local:3143
</code></pre>
</blockquote>

<p>Package manager proxy setup per repository:</p>

<blockquote>
<pre><code>          debian:


            # per repository proxy
            proxy:
              enabled: true
              http:  http://maas-01:8080
              https: http://maas-01:8080


          # package manager fallback defaults 
          # used if linux:system:repo:apt-mk:proxy has no protocol specific entries
            ftp:   ftp://proxy.host.local:2121
            #http:  http://proxy.host.local:3142
            #https: https://proxy.host.local:3143
</code></pre>

<p>&hellip;</p>

<pre><code>          # global system fallback system defaults
</code></pre>
</blockquote>

<p>RC
  ~~</p>

<p>Configure global environment variables
  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</p>

<p>Linux /etc/environment:
  <code>/etc/environment</code> is for static system wide variable assignment after boot. Variable expansion is frequently not supported.</p>

<blockquote>
<pre><code>        env:
          BOB_VARIABLE: Alice


          BOB_PATH:
            - /srv/alice/bin
            - /srv/bob/bin


          ftp_proxy:   none
          http_proxy:  http://global-http-proxy.host.local:8080
          https_proxy: ${linux:system:proxy:https}
          no_proxy:
            - 192.168.0.80
            - 192.168.1.80
            - .domain.com
            - .local
        # NOTE: global defaults proxy configuration.
          noproxy:
</code></pre>
</blockquote>

<p>Configure profile.d scripts
  ~~~~~~~~~~~~~~~~~~~~~~~~~~~</p>

<p>Linux /etc/profile.d:</p>

<p>The profile.d scripts are being sourced during .sh execution and support variable expansion in opposite to /etc/environment
  global settings in <code>/etc/environment</code>.</p>

<blockquote>
<pre><code>        profile:
          locales: |
</code></pre>

<p>export LANG=C
  export LC_ALL=C
  vi_flavors.sh: |
  export PAGER=view
  export EDITOR=vim
  alias vi=vim
  shell_locales.sh: |
  export LANG=en_US
  export LC_ALL=en_US.UTF-8
  shell_proxies.sh: |
  export FTP_PROXY=<a href="ftp://127.0.3.3:2121">ftp://127.0.3.3:2121</a>
  export NO_PROXY=&lsquo;.local&rsquo;</p>

<pre><code>  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


  Add sudo state, salt-managed aliases,users,groups


  Linux with system users, some with password set


  Configure sudo for users and groups under ``/etc/sudoers.d/``.


  This ways ``linux.system.sudo`` pillar map to actual sudo attributes:


&gt;	     # simplified template:


  {{ user }}   {{ hosts }}=({{ runas }}) NOPASSWD: {{ commands }}
  %{{ group }} {{ hosts }}=({{ runas }}) NOPASSWD: {{ commands }}


&gt;	     # when rendered:


&gt;	      system:
&gt;	        sudo:
&gt;	          enabled: true
&gt;	          alias:
&gt;	            host:
&gt;	              LOCAL:
&gt;	              - localhost
&gt;	              PRODUCTION:
&gt;	              - db1
&gt;	              - db2
&gt;	            runas:
&gt;	              DBA:
&gt;	              - postgres
&gt;	              - mysql
&gt;	              SALT:
&gt;	              - root
&gt;	            command:
&gt;	              # Note: This is not 100% safe when ALL keyword is used, user still may modify configs and hide his actions.
&gt;	              #       Best practice is to specify full list of commands user is allowed to run.
&gt;	              SUPPORT_RESTRICTED:
&gt;	              - /bin/vi /etc/sudoers*
&gt;	              - /bin/vim /etc/sudoers*
&gt;	              - /bin/nano /etc/sudoers*
&gt;	              - /bin/emacs /etc/sudoers*
&gt;	              - /bin/su - root
&gt;	              - /bin/su -
&gt;	              - /bin/su
&gt;	              - /usr/sbin/visudo
&gt;	              SUPPORT_SHELLS:
&gt;	              - /bin/sh
&gt;	              - /bin/ksh
&gt;	              - /bin/bash
&gt;	              - /bin/rbash
&gt;	              - /bin/dash
&gt;	              - /bin/zsh
&gt;	              - /bin/csh
&gt;	              - /bin/fish
&gt;	              - /bin/tcsh
&gt;	              - /usr/bin/login
&gt;	              - /usr/bin/su
&gt;	              - /usr/su
&gt;	              ALL_SALT_SAFE:
&gt;	              - /usr/bin/salt state*
&gt;	              - /usr/bin/salt service*
&gt;	              - /usr/bin/salt pillar*
&gt;	              - /usr/bin/salt grains*
&gt;	              - /usr/bin/salt saltutil*
&gt;	              - /usr/bin/salt-call state*
&gt;	              - /usr/bin/salt-call service*
&gt;	              - /usr/bin/salt-call pillar*
&gt;	              - /usr/bin/salt-call grains*
&gt;	              - /usr/bin/salt-call saltutil*
&gt;	              SALT_TRUSTED:
&gt;	              - /usr/bin/salt*
&gt;	          users:
&gt;	            # saltuser1 with default values: saltuser1 ALL=(ALL) NOPASSWD: ALL
&gt;	            saltuser1: {}
&gt;	            saltuser2:
&gt;	              hosts:
&gt;	              - LOCAL
&gt;	            # User Alias DBA
&gt;	            DBA:
&gt;	              - ALL
&gt;	              commands:
&gt;	              - ALL_SALT_SAFE
&gt;	          groups:
&gt;	            db-ops:
&gt;	              - '!PRODUCTION'
&gt;	              runas:
&gt;	              - DBA
&gt;	              - /bin/cat *
&gt;	              - /bin/less *
&gt;	              - /bin/ls *
&gt;	            salt-ops:
&gt;	              - 'ALL'
&gt;	              - SALT
&gt;	              - SUPPORT_SHELLS
&gt;	            salt-ops-2nd:
&gt;	              name: salt-ops
&gt;	              nopasswd: false
&gt;	              - '!SUPPORT_SHELLS'
&gt;	              - '!SUPPORT_RESTRICTED'


###  Piotr Kruk


  add startsector for partitions
  disk without any existing partitions. (set startsector=1, if you want to start partitions from 2048)


&gt;	                startsector: 1


###  Serhiy Ovsianikov


  Add atop


  Linux with atop service:


&gt;	          atop:
&gt;	            interval: 20
&gt;	            logpath: &quot;/var/log/atop&quot;
&gt;	            outfile: &quot;/var/log/atop/daily.log&quot;


###  Simon Pasquier


  Add linux.storage.loopback state


  This state allows to configure loopback device(s). This isn't meant to
  be used in production but offers a cheap way to test Cinder with LVM
  volumes.


  Linux with local loopback device


&gt;	        storage:
&gt;	          loopback:
&gt;	            disk1:
&gt;	              file: /srv/disk1
&gt;	              size: 50G


###  teoyaomiqui


  RIL-267 Adding cpu governor and kernel module opts (#101)


  Configure or blacklist kernel modules with additional options to `/etc/modprobe.d` following example 
  will add `/etc/modprobe.d/nf_conntrack.conf` file with line `options nf_conntrack hashsize=262144`:


&gt;	            module:
&gt;	              nf_conntrack:
&gt;	                option:
&gt;	                  hashsize: 262144


  Enable cpufreq governor for every cpu:


###  Tomas Kamm


  fixed typo in sudo part


&gt;	          aliases:


###  Tomáš Kukrál


  add support for kernel modules


  Load kernel modules and add them to `/etc/modules`:


&gt;	            modules:
&gt;	              - nf_conntrack
&gt;	              - tp_smapi
&gt;	              - 8021q


###  Vladimir Eremin


  netconsole remote kernel logger


  To configure:


  It works with both static and DHCP interfaces, and applies online.


  You could use bash-scripting in netconsole.conf.


  You could override the MAC.


  See tests/pillar/system.sls for further information.


  Netconsole Remote Kernel Logging


  Netconsole logger could be configured for configfs-enabled kernels
  (`CONFIG_NETCONSOLE_DYNAMIC` should be enabled). Configuration applies both in
  runtime (if network is already configured), and on-boot after interface
  initialization. Notes:


  * receiver could be located only in same L3 domain
  (or you need to configure gateway MAC manually)
  * receiver's MAC is detected only on configuration time
  * using broadcast MAC is not recommended


&gt;	      parameters:
&gt;	          system:
&gt;	            netconsole:
&gt;	              port: 514 (optional)
&gt;	              loglevel: debug (optional)
&gt;	              target:
  192.168.0.1:


&gt;	                  interface: bond0
&gt;	                  mac: &quot;ff:ff:ff:ff:ff:ff&quot; (optional)


# Formula lldp


###  Damian Szeluga


  Fix readme and tests
  ====


###  Marcin Iwinski


  fixing README


  Link Layer Discovery Protocol service.


&gt;	          #list of interfaces handling LLDPBPDUs


  initial commit
  ===


  LLDP 


  Link Layer Discovery Protocol services.


  Sample pillars


  LLDP client


&gt;	      lldp:
&gt;	        client:
&gt;	          enabled: true
&gt;	          intefaces:
&gt;	          - eth0
&gt;	          - eth1


# Formula logrotate


###  Filip Pytloun


  Allow deploying logrotate jobs using support meta


  Cross-formula relationship


  It's possible to use support meta to define logrotate rules from within other
  formula.


  Example ``meta/logrotate.yml`` for horizon formula:


&gt;	      job:
&gt;	        horizon:
&gt;	          - files:
&gt;	              - /var/log/horizon/*.log
&gt;	            options:
&gt;	              - compress
&gt;	              - delaycompress
&gt;	              - missingok
&gt;	              - notifempty
&gt;	              - rotate: 10
&gt;	              - daily
&gt;	              - minsize: 20M
&gt;	              - maxsize: 500M
&gt;	              - postrotate: &quot;if /etc/init.d/apache2 status &gt; /dev/null; then /etc/init.d/apache2 reload &gt; /dev/null; fi&quot;


  Unify Makefile, .gitignore and update readme


# Formula maas


###  azvyagintsev


  Add network commissioning script


  EME-70


&gt;	          00-maas-05-simplify-network-interfaces: /etc/maas/files/commisioning_scripts/00-maas-05-simplify-network-interfaces


&gt;	    Modules: add wait_for_machine_status function


&gt;	      maas:
&gt;	        cluster:
&gt;	          enabled: true
&gt;	          role: master/slave


  Module function's example:


  * Wait for status of selected machine's:


&gt;	     cat maas/machines/wait_for_machines_ready.sls


  ...


&gt;	      wait_for_machines_ready:
  module.run:


&gt;	        - name: maas.wait_for_machine_status
&gt;	        - kwargs:
&gt;	              machines:
&gt;	                - kvm01
&gt;	                - kvm02
&gt;	              timeout: 1200 # in seconds
&gt;	              req_status: &quot;Ready&quot;
&gt;	        - require:
&gt;	          - cmd: maas_login_admin


  If module run w/\o any extra paremeters - `wait_for_machines_ready` will wait for defined in salt machines. In those case, will be usefull to skip some machines:


&gt;	     cat maas/machines/wait_for_machines_deployed.sls


&gt;	              req_status: &quot;deployed&quot;
&gt;	              ignore_machines:
&gt;	                 - kvm01 # in case it's broken or whatever


  List of avaibled `req_status` defined in global variable:


###  Damian Szeluga


  Updated readme


&gt;	      salt_master_ip: 192.168.0.10
&gt;	        commissioning_scripts:
  00-maas-06-create-raid.sh: /srv/salt/reclass/scripts/commisioning_script.sh


&gt;	        maas_config:
&gt;	          domain: mydomain.local
&gt;	          http_proxy: http://192.168.0.10:3142
&gt;	          commissioning_distro_series: xenial
&gt;	          default_distro_series: xenial
&gt;	          default_osystem: 'ubuntu'
&gt;	          default_storage_layout: lvm
&gt;	          disk_erase_with_secure_erase: true
&gt;	          dnssec_validation: 'no'
&gt;	          enable_third_party_drivers: true
&gt;	          maas_name: cfg01
&gt;	          network_discovery: 'enabled'
&gt;	          active_discovery_interval: '600'
&gt;	          ntp_external_only: true
&gt;	          ntp_servers: 10.10.11.23 10.10.11.24
&gt;	          upstream_dns: 192.168.12.13
&gt;	          enable_http_proxy: true
&gt;	          default_min_hwe_kernel: ''
&gt;	         sshprefs:
&gt;	          - 'ssh-rsa ASDFOSADFISdfasdfasjdklfjasdJFASDJfASdf923@AAAAB3NzaC1yc2EAAAADAQABAAACAQCv8ISOESGgYUOycYw1SAs/SfHTqtSCTephD/7o2+mEZO53xN98sChiFscFaPA2ZSMoZbJ6MQLKcWKMK2OaTdNSAvn4UE4T6VP0ccdumHDNRwO3f6LptvXr9NR5Wocz2KAgptk+uaA8ytM0Aj9NT0UlfjAXkKnoKyNq6yG+lx4HpwolVaFSlqRXf/iuHpCrspv/u1NW7ReMElJoXv+0zZ7Ow0ZylISdYkaqbV8QatCb17v1+xX03xLsZigfugce/8CDsibSYvJv+Hli5CCBsKgfFqLy4R5vGxiLSVzG/asdjalskjdlkasjdasd/asdajsdkjalaksdjfasd/fa/sdf/asd/fas/dfsadf blah@blah'


  Expand readme


&gt;	    maas:
&gt;	      region:
&gt;	        theme: mirantis
&gt;	        bind:
&gt;	          host: 192.168.0.10:5240
&gt;	          port: 5240
&gt;	        admin:
&gt;	          username: exampleuser
&gt;	          password: examplepassword
&gt;	          email:  email@example.com
&gt;	        database:
&gt;	          engine: null
&gt;	          host: localhost
&gt;	          name: maasdb
&gt;	          password: qwqwqw
&gt;	          username: maas
&gt;	        enabled: true
&gt;	        user: mirantis
&gt;	        token: &quot;89EgtWkX45ddjMYpuL:SqVjxFG87Dr6kVf4Wp:5WLfbUgmm9XQtJxm3V2LUUy7bpCmqmnk&quot;
&gt;	        fabrics:
&gt;	          test-fabric:
&gt;	            description: Test fabric
&gt;	        subnets:
&gt;	          subnet1:
&gt;	            fabric: test-fabric
&gt;	            cidr: 2.2.3.0/24
&gt;	            gateway_ip: 2.2.3.2
&gt;	            iprange:
&gt;	              start: 2.2.3.20
&gt;	              end: 2.2.3.250
&gt;	        dhcp_snippets:
&gt;	          test-snippet:
&gt;	            value: option bootfile-name &quot;tftp://192.168.0.10/snippet&quot;;
&gt;	            description: Test snippet
&gt;	            enabled: true
&gt;	            subnet: subnet1
&gt;	        boot_resources:
&gt;	          bootscript1:
&gt;	            title: bootscript
&gt;	            architecture: amd64/generic
&gt;	            filetype: tgz
&gt;	            content: /srv/salt/reclass/nodes/path_to_file
&gt;	        package_repositories:
&gt;	          Saltstack:
&gt;	            url: http://repo.saltstack.com/apt/ubuntu/14.04/amd64/2016.3/
&gt;	            distributions:
&gt;	                 - trusty
&gt;	            components:
&gt;	                - main
&gt;	                - extra
&gt;	            arches: amd64
&gt;	                 Version: GnuPG v2


  mQENBFOpvpgBCADkP656H41i8fpplEEB8IeLhugyC2rTEwwSclb8tQNYtUiGdna9
  m38kb0OS2DDrEdtdQb2hWCnswxaAkUunb2qq18vd3dBvlnI+C4/xu5ksZZkRj+fW
  tArNR18V+2jkwcG26m8AxIrT+m4M6/bgnSfHTBtT5adNfVcTHqiT1JtCbQcXmwVw


  WbqS6v/LhcsBE//SHne4uBCK/GHxZHhQ5jz5h+3vWeV4gvxS3Xu6v1IlIpLDwUts
  kT1DumfynYnnZmWTGc6SYyIFXTPJLtnoWDb9OBdWgZxXfHEcBsKGha+bXO+m2tHA
  gNneN9i5f8oNxo5njrL8jkCckOpNpng18BKXABEBAAG0MlNhbHRTdGFjayBQYWNr


  YWdpbmcgVGVhbSA8cGFja2FnaW5nQHNhbHRzdGFjay5jb20+iQE4BBMBAgAiBQJT
  qb6YAhsDBgsJCAcDAgYVCAIJCgsEFgIDAQIeAQIXgAAKCRAOCKFJ3le/vhkqB/0Q


  WzELZf4d87WApzolLG+zpsJKtt/ueXL1W1KA7JILhXB1uyvVORt8uA9FjmE083o1
  yE66wCya7V8hjNn2lkLXboOUd1UTErlRg1GYbIt++VPscTxHxwpjDGxDB1/fiX2o
  nK5SEpuj4IeIPJVE/uLNAwZyfX8DArLVJ5h8lknwiHlQLGlnOu9ulEAejwAKt9CU
  4oYTszYM4xrbtjB/fR+mPnYh2fBoQO4d/NQiejIEyd9IEEMd/03AJQBuMux62tjA
  /NwvQ9eqNgLw9NisFNHRWtP4jhAOsshv1WW+zPzu3ozoO+lLHixUIz7fqRk38q8Q
  9oNR31KvrkSNrFbA3D89uQENBFOpvpgBCADJ79iH10AfAfpTBEQwa6vzUI3Eltqb
  9aZ0xbZV8V/8pnuU7rqM7Z+nJgldibFk4gFG2bHCG1C5aEH/FmcOMvTKDhJSFQUx
  uhgxttMArXm2c22OSy1hpsnVG68G32Nag/QFEJ++3hNnbyGZpHnPiYgej3FrerQJ
  zv456wIsxRDMvJ1NZQB3twoCqwapC6FJE2hukSdWB5yCYpWlZJXBKzlYz/gwD/Fr


  GL578WrLhKw3UvnJmlpqQaDKwmV2s7MsoZogC6wkHE92kGPG2GmoRD3ALjmCvN1E


  PsIsQGnwpcXsRpYVCoW7e2nW4wUf7IkFZ94yOCmUq6WreWI4NggRcFC5ABEBAAGJ


  AR8EGAECAAkFAlOpvpgCGwwACgkQDgihSd5Xv74/NggA08kEdBkiWWwJZUZEy7cK


  WWcgjnRuOHd4rPeT+vQbOWGu6x4bxuVf9aTiYkf7ZjVF2lPn97EXOEGFWPZeZbH4
  vdRFH9jMtP+rrLt6+3c9j0M8SIJYwBL1+CNpEC/BuHj/Ra/cmnG5ZNhYebm76h5f


  T9iPW9fFww36FzFka4VPlvA4oB7ebBtquFg3sdQNU/MmTVV4jPFWXxh4oRDDR+8N
  1bcPnbB11b5ary99F/mqr7RgQ+YFF0uKRE3SKa7a+6cIuHEZ7Za+zhPaQlzAOZlx
  fuBmScum8uQTrEF5+Um5zkwC7EXTdH1co/+/V/fpOtxIg4XO4kcugZefVm5ERfVS


  MA==
  =dtMN


&gt;	        machines:
&gt;	          machine1:
&gt;	            interfaces:
&gt;	              - one1: &quot;11:22:33:44:55:66&quot;
&gt;	            power_parameters:
&gt;	              power_type: ipmi
&gt;	              power_address: '192.168.10.10'
&gt;	              power_user: bmc_user
&gt;	              power_password: bmc_password
&gt;	        devices:
&gt;	          machine1-ipmi:
&gt;	            interface:
&gt;	              ip_address: 192.168.10.10
&gt;	              subnet: cidr:192.168.10.0/24
&gt;	            mac: '66:55:44:33:22:11'


###  Filip Pytloun


  Unify Makefile, .gitignore and update readme


###  Jiri Broulik


  maas-proxy upstream proxy support and jenkins slave proxy port


&gt;	        upstream_proxy:
&gt;	          address: 10.0.0.1
&gt;	          port: 8080


###  Maciej Relewicz


  fixes


&gt;	              mac: &quot;11:22:33:44:55:66&quot;
&gt;	            distro_series: xenial
&gt;	            hwe_kernel: hwe-16.04


###  Martin Polreich


  Add restore script for Maas DB


  MAAS region service with backup data


&gt;	        region:
&gt;	          database:
&gt;	            initial_data:
&gt;	              source: cfg01.local
&gt;	              host: 192.168.0.11


###  Michael Polenchuk


  Mend machines status


  Remove lower() function to prevent an exception of


&gt;	              req_status: &quot;Deployed&quot;


###  Ondrej Smola


  added user and password for maas upstream proxy


&gt;	          user: username      #OPTIONAL
&gt;	          password: password  #OPTIONAL


  added support for power driver setting


&gt;	              #Optional (for legacy HW)
&gt;	              power_driver: LAN


###  Petr Ruzicka


  Fix documentation and add virsh power_type support


&gt;	              # Used in case of power_type: virsh
&gt;	              power_id: my_libvirt_vm_name


###  Richard Felkl


  added support local mirrors


  Usage of local repos


&gt;	      cluster:
&gt;	          port: 80
&gt;	        saltstack_repo_key: |
&gt;	          Version: GnuPG v2


&gt;	        saltstack_repo_xenial: &quot;http://${_param:local_repo_url}/ubuntu-xenial stable salt&quot;
&gt;	        saltstack_repo_trusty: &quot;http://${_param:local_repo_url}/ubuntu-trusty stable salt&quot;


###  Roald Nefs


  Added link to official Ubuntu MAAS website


  Added link to official Ubuntu MAAS website in the 'Read more' section of README.rst.
  * https://maas.io/


# Formula magnum


###  Filip Pytloun


  Unify Makefile, .gitignore and update readme


# Formula mayan


###  Ales Komarek


  Docfix


  Mayan Formula


  Automated OCR of documents, automatic categorization, flexible metadata,
  extensive access control, Mayan EDMS has all this to offers and many more
  features to help you tame your documents.


  Sample pillars


  More Information


###  Filip Pytloun


  Unify Makefile, .gitignore and update readme
  * https://openode.readthedocs.org/en/latest/install_mayan_server.html


# Formula memcached


###  Filip Pytloun


  Unify Makefile, .gitignore and update readme


# Formula midonet


###  Filip Pytloun


  Unify Makefile, .gitignore and update readme
  * http://www.midokura.com/midonet/


# Formula model-manager


###  Adam Tengler


  Fixed server.sls and parametrized configuration


&gt;	          secret_key: supersecretkey
&gt;	            address: https://github.com/salt-formulas/django-model-manager.git
&gt;	            protocol: https
&gt;	            model_template:
&gt;	              remote: http
&gt;	              url: https://git.my-gitlab.io/group/project/raw/master/context.yaml
&gt;	              job: generator-job
&gt;	          delivery:


###  Ales Komarek


  Keystone identity docs
  model-manager service with Keystone authentication


&gt;	            host: keystone.endpoint.com
&gt;	            api_version: 3


  Use-cases doc image fix


&gt;	     :width: 60%


  Use-cases doc


&gt;	     :scale: 50%
&gt;	     :align: center


  Use cases for the model-manager service are:


  * Streamline the model generation of complex infrastructures
  * Wath the deployment process installing services across infrastructure
  * Align monitoring checks and metrics to infrastructure


  Visualization


  Documentation fixes
  model-manager formula


  Model-manager is a service for manipulating the metadata models of the


  SaltStack/reclass based deployments. It covers model management at various
  stages of deployment life-cycles.


  Sample metadata


  model-manager service with keystone authentication


&gt;	      model_manager:
&gt;	        server:
&gt;	          enabled: true
&gt;	          source:
&gt;	            engine: git
&gt;	            address: git@github.com:salt-formulas/django-model-manager.git
&gt;	            revision: master
&gt;	          identity:
&gt;	            engine: keystone
&gt;	            address: git@repo.com:repo.git


  model-manager service with model generator and Jenkins integration


&gt;	          config_files:
&gt;	          - _4000_integration
&gt;	          - _4010_models_panel_group
&gt;	          - _4020_integration_overview_panel
&gt;	          - _4030_integration_modeldesigner_panel
&gt;	          integration:
&gt;	            engine: jenkins
&gt;	            protocol: http
&gt;	            host: 127.0.0.1
&gt;	            port: 8080
&gt;	            user: model-manager
&gt;	            password: password
&gt;	          model_template:


  model-manager service with Salt master integration


&gt;	          - _5000_delivery
&gt;	          - _5010_resource_management_panel_group
&gt;	          - _5020_delivery_resource_topology_panel
&gt;	          - _5030_delivery_salt_control_panel
&gt;	          orchestration:
&gt;	            engine: salt
&gt;	            port: 6969


  * http://salt-formulas.readthedocs.io/en/latest/develop/overview-reclass.html


  Documentation and bugs


&gt;	      https://github.com/salt-formulas/salt-formula-model-manager/issues


  Initial commit


  model_manager formula


  Service model_manager description


  Sample pillars


  Single model_manager service


&gt;	          version: icehouse


  More information


  * a link


  * links


# Formula monasca


# Formula mongodb


###  Filip Pytloun


  Unify Makefile, .gitignore and update readme


# Formula moodle


###  Aleš Komárek


  docfix


  Moodle Formula


  Moodle is a Course Management System (CMS), also known as a Learning


  Management System (LMS) or a Virtual Learning Environment (VLE). It is
  a Free web application that educators can use to create effective online
  learning sites.


  Sample Pillars


&gt;	      moodle:
&gt;	        enabled: true
&gt;	        apps:
&gt;	        - enabled: true
&gt;	          name: 'uni'
&gt;	          prefix: 'uni_' # max 5 chars
&gt;	          version: '2.5'
&gt;	          database:
&gt;	            engine: 'postgresql'
&gt;	            host: '127.0.0.1'
&gt;	            name: 'moodle_uni'
&gt;	            password: 'pwd'
&gt;	            user: 'moodle_uni'
&gt;	          cache:
&gt;	            engine: 'memcached'
&gt;	          themes:
&gt;	          - name: uni
&gt;	            source:
&gt;	              type: git
&gt;	              address: git@repo.git.cz:domain/repo.git
&gt;	              branch: master


  More Information


  * https://moodle.org/plugins/view.php?plugin=cachestore_apc
  * http://midact.com/content/moodle-how-enable-memcached
  * http://docs.moodle.org/dev/The_Moodle_Universal_Cache_%28MUC%29
  * http://docs.moodle.org/24/en/Cron


# Formula mosquitto


###  Ales Komarek


  Doc fixes
  mosquitto formula


  External links


###  Filip Pytloun


  Unify Makefile, .gitignore and update readme


# Formula motion


# Formula murano


###  Aleš Komárek


  Update README.rst


  Murano formula


&gt;	          murano_agent_queue:
&gt;	            engine: rabbitmq
&gt;	            port: 5672
&gt;	            members:
&gt;	            - host: 192.168.1.13
&gt;	            - host: 192.168.1.14
&gt;	            - host: 192.168.1.15
&gt;	            user: openstack
&gt;	            password: supersecretcatalogpassword


  External links


###  Filip Pytloun


  Unify Makefile, .gitignore and update readme


###  Pavel Cizinsky


  add insecure param/fix murano_agent_queue


&gt;	          insecure: false
&gt;	            host: 192.168.1.10


# Formula mysql


###  Ales Komarek


  Readme fix


  MySQL Formula


  MySQL is the world's second most widely used open-source relational database
  management system (RDBMS).


  Sample Metadata


  Standalone setups


  Database with initial data


&gt;	      mysql:
&gt;	        server:
&gt;	          enabled: true
&gt;	          database:
&gt;	            datatabese_with_init_data:
&gt;	              encoding: 'utf8'
&gt;	              users:
&gt;	              - name: 'username'
&gt;	                password: 'password'
&gt;	                host: 'localhost'
&gt;	                rights: 'all privileges'
&gt;	              initial_data:
&gt;	                engine: backupninja
&gt;	                source: backup.host
&gt;	                host: original-host-name
&gt;	                database: original-database-name


  MySQL Galera cluster


  MySQL Galera cluster is configured for ring connection between 3 nodes. Each
  node should have just one member.


  MySQL client


  Sample Usage


  More Information


###  Filip Pytloun


  Unify Makefile, .gitignore and update readme


###  Jiri Broulik


  restore db


  Database with initial data (Restore DB)


&gt;	        client:
&gt;	          server:
&gt;	            database:
&gt;	              admin:
&gt;	                host: localhost
&gt;	                port: 3306
&gt;	                user: ${_param:mysql_admin_user}
&gt;	                password: ${_param:mysql_admin_password}
&gt;	                encoding: utf8
&gt;	              database:
&gt;	                neutron_upgrade:
&gt;	                  encoding: utf8
&gt;	                  users:
&gt;	                  - name: neutron
&gt;	                    password: ${_param:mysql_neutron_password}
&gt;	                    host: '%'
&gt;	                    rights: all
&gt;	                    host: ${_param:single_address}
&gt;	                  initial_data:
&gt;	                    engine: backupninja
&gt;	                    source: ${_param:backupninja_backup_host}
&gt;	                    host: ${linux:network:fqdn}
&gt;	                    database: neutron


  The provided setup restores db named neutron_upgrade with data from db called neutron.


# Formula nagios


###  Dmitrii Sutagin


  Add multi-user functionality


  Nagios UI configrations with HTTP basic authentication (use &quot;readonly&quot; flag to specify readonly users)


&gt;	                # this is the main admin, it cannot have a 'readonly' flag.
&gt;	                # 'users' section is optional, allows defining additional users.
&gt;	                users:
&gt;	                  - username: nagios_admin_2
&gt;	                    password: secret2
&gt;	                  - username: nagios_user
&gt;	                    password: secret3
&gt;	                    readonly: true


###  Filip Pytloun


  Unify Makefile, .gitignore and update readme


###  Jaymes Mosher


  Adds notification handlers for Slack, Salesforce, and Pagerduty.


  Also adding tests.


  Nagios Notification Handlers


  You can configure notification handlers.  Currently supported handlers are SMTP, Slack,


  Salesforce, and Pagerduty.


&gt;	      nagios:
&gt;	        server:
&gt;	          enabled: true
&gt;	          notification:
&gt;	            slack:
&gt;	              enabled: true
&gt;	              webhook_url: https://hooks.slack.com/services/abcdef/12345
&gt;	            pagerduty:
&gt;	              key: abcdef12345
&gt;	            sfdc:
&gt;	              client_id: abcdef12345
&gt;	              client_secret: abcdef12345
&gt;	              username: abcdef
&gt;	              password: abcdef
&gt;	              auth_url: https://abcedf.my.salesforce.com
&gt;	              environment: abcdef
&gt;	              organization_id: abcdef


&gt;	      # SMTP without auth
&gt;	            smtp:
&gt;	              auth: false
&gt;	              url: smtp://127.0.0.1:25
&gt;	              from: nagios@localhost
&gt;	              # Notification email subject can be defined, must be one line
&gt;	              # default subjects are:
&gt;	              host_subject: &gt;-
  ** $NOTIFICATIONTYPE$ Host Alert: $HOSTNAME$ is $HOSTSTATE$ **


&gt;	              service_subject: &gt;-
  ** $NOTIFICATIONTYPE$ Service Alert: $HOSTNAME$/$SERVICEDESC$ is $SERVICESTATE$ **


&gt;	      # An example using a Gmail account as a SMTP relay
&gt;	              auth: login
&gt;	              url: smtp://smtp.gmail.com:587
&gt;	              from: &lt;you&gt;@gmail.com
&gt;	              starttls: true
&gt;	              username: foo
&gt;	              password: secret


  Each handler adds two commands, `notify-host-by-&lt;HANDLER&gt;`, and `notify-service-by-&lt;HANDLER&gt;`, that you can
  reference in a contact.


&gt;	          objects:
&gt;	            contact:
&gt;	              sfdc:
&gt;	                alias: sfdc
&gt;	                contactgroups:
&gt;	                  - Operator
&gt;	                email: root@localhost
&gt;	                host_notification_commands: notify-host-by-sfdc
&gt;	                host_notification_options: d,r
&gt;	                host_notification_period: 24x7
&gt;	                host_notifications_enabled: 1
&gt;	                service_notification_commands: notify-service-by-sfdc
&gt;	                service_notification_options: c,r
&gt;	                service_notification_period: 24x7
&gt;	                service_notifications_enabled: 1


  By default in Stacklight, notifications are only enabled for `00-top-clusters` and individual host
  and SSH checks.  If you want to enable notifications for all checks you can enable this value:


&gt;	            alarm_enabled_override: true


  The notification interval defaults to zero, which will only send one notification when the alert
  triggers.  You can override the interval if you want notifications to repeat.  For example, to
  have them repeat every 30 minutes:


&gt;	            hosts:
&gt;	              generic_host_tpl:
&gt;	                notification_interval: 30
&gt;	            services:
&gt;	              generic_service_tpl:


  Platform support


###  Swann Croiset


  Use by default IPv4 for Host IP addresses


&gt;	            grain_interfaces: 'ip4_interfaces' # the default


  Fix typo in README


  There are 2 different ways to configure the Host IP adddresses, the preferred way


  Configure Host IP address belonging to a network


  Fixes issue #6


&gt;	                network: 10.0.0.0/8


  Note about dynamic hosts IP addresses configuration:


  There are 2 different ways to configure the Host IP adddress, the prefered way
  is to define the **network** of the nodes to pickup the first IP address found
  belonging to this network.


&gt;	          dynamic:
&gt;	            enabled: true
&gt;	              - target: '*'
&gt;	                contact_groups: Operator


  The alternative way is to define the **interface** list, to pickup the first IP
  address of the first interface found.


&gt;	                interface:
&gt;	                - eth0
&gt;	                - ens0


  If both properties are defined, the **network** option wins and the **interface** is
  ignored.


# Formula network


###  Filip Pytloun


  Unify Makefile, .gitignore and update readme


# Formula neutron


###  Aleš Komárek


  Update README.rst


  Neutron Formula


  Sample Pillars


&gt;	  Note: If you want contrail lbaas then backend is only required. Lbaas in
  pillar should be define only if it should be disabled.


  Neutron VXLAN tenant networks with Network nodes


  With DVR for East-West and Network node for North-South.
  routers will only be utilized for traffic that is router within the cloud
  infrastructure and that remains encapsulated. External traffic will be 
  routed to via the network nodes. 


  Neutron Server


  Network Node


  Neutron VXLAN tenant networks with Network Nodes with DVR


  With DVR for East-West and North-South, DVR everywhere, Network node for SNAT.


  Advanced Neutron Features (DPDK, SR-IOV)


  Neutron Client


###  Dennis Dmitriev


  Allow keystone endpoint_type interpolation for Neutron models


&gt;	            endpoint_type: internal


###  Dmitry Stremkouski


  One should be able to set up different mtus for different physnets
  external_mtu may differ from tenant_net_mtu


  This patchset fixes that issue


  Customer-Found


  Specify different mtu values for different physnets


&gt;	      neutron:
&gt;	        server:
&gt;	          version: mitaka
&gt;	          backend:
&gt;	            external_mtu: 1500
&gt;	            tenant_net_mtu: 9000
&gt;	            ironic_net_mtu: 9000


  One should be able to set path_mtu value for neutron


&gt;	          path_mtu: 1500


  Fixing inaccurate physnet mapping iterations


  In order to handle situations, when external vlan ranges are not
  applicable, we should iterate properly to generate config file.


  Removing physnet1 hardcode in such case.


&gt;	    Pillar:
  is not set by default anywhere, so it would be True and this
  patch does not break backward compatibility for physnet1 hardcode.


  Adding ironic physnet handling


  Disable physnet1 bridge


  By default we have external access turned on, so among any physnets in
  your reclass there would be additional one: physnet1, which is mapped to
  br-floating


  If you need internal nets only without this bridge, remove br-floating
  and configurations mappings. Disable mappings for this bridge on


&gt;	  neutron-servers:


&gt;	          external_access: false


&gt;	  gateways:


&gt;	        gateway:


  compute nodes:


&gt;	        compute:


###  Dmitry Ukov


  Policy.json should be defined by user


  User can override and add values to policy.json by creating flat
  key-value structure under neutron:server:policy.


  Configuration of policy.json file


  ....


&gt;	          policy:
&gt;	            create_subnet: 'rule:admin_or_network_owner'
  'get_network:queue_id': 'rule:admin_only'


&gt;	            # Add key without value to remove line from policy.json
  'create_network:shared':


###  Elena Ezhova


  Revert &quot;Refactor mechanism driver metadata&quot;


  This reverts commit 79ffa26858d682f404984175dc1ab93863a149ad.


  Fixes PROD-14994


&gt;	              ovs:
&gt;	                driver: openvswitch
&gt;	              sriov:
&gt;	                driver: sriovnicswitch


  Refactor mechanism driver metadata


  Using a quite heavy nested dict structure for defining a list of
  mechanism driver to load is an overkill. This change simplifies
  the structure by turning it into a list.


  The mechanism driver meta for gateway and compute nodes is now moved
  to the system level (like it is already done for control nodes).


  The l2population mechanism driver is still enabled by default,
  however its enablement is no longer unconditional.


&gt;	              - openvswitch
&gt;	              - l2population
&gt;	              - sriovnicswitch


  Remove references to the neutron:server:plugin parameter


  This parameter is present in service- and system-level metadata
  even though it became oblosete somewhere around adoption of Kilo
  release of OpenStack when it was replased by the


&gt;	    neutron:server:backend:engine param. Its presense currently only
  clutters metadata and the formula and confuses users.


  This change removes references to this param from the formula and
  fixes the 2 remaining cases of its usage:


  Add FWaaS support


  This adds all necessary FWaaS configuration for the Ocata version.


  Both V1 and V2 versions can be enabled, however only V1 is operable
  in this release.


  PROD-13639


  Neutron FWaaSv1 enablement


&gt;	    neutron:
&gt;	      fwaas:
&gt;	        enabled: true
&gt;	        version: ocata
&gt;	        api_version: v1


  Allow configuring Neutron LBaaSv2 with Octavia


  Updated lbaas settings in neutron-server.conf to allow enabling
  neutron_lbaas with Octavia driver.


  Neutron LBaaSv2 enablement


&gt;	            octavia:
&gt;	              engine: octavia
&gt;	              driver_path: 'neutron_lbaas.drivers.octavia.driver.OctaviaDriver'
&gt;	              base_url: 'http://127.0.0.1:9876'
&gt;	              driver_path: 'avi_lbaasv2.avi_driver.AviDriver'
&gt;	  Note: If the Contrail backend is set, Opencontrail loadbalancer would be enabled
  automatically. In this case lbaas should disabled in pillar:


&gt;	          enabled: false


###  Filip Pytloun


  Unify Makefile, .gitignore and update readme


###  Ilya Chukhnakov


  Make VLAN-aware-VMs configurable


  VLAN-aware-VMs are only supported with OVS firewall driver.


  This patch makes the feature configurable and leaves it disabled
  by default to ensure the feature is only available on supported
  configurations to avoid confusion.


  Neutron with VLAN-aware-VMs


&gt;	          vlan_aware_vms: true


###  Jakub Pavlik


  OVS-dpdk support


  Introduce dpdk support for neutron OVS configuration


&gt;	    Epic: PROD-8957


  Neutron OVS DPDK


  Enable datapath netdev for neutron openvswitch agent


  ...


&gt;	          dpdk: True


&gt;	          plugin: ml2
&gt;	            engine: ml2


&gt;	            mechanism:


  SRIOV support in Neutron OVS


  Enable SRIOV support for Neutron OVS


&gt;	    Epic: PROD-8956


  Neutron OVS SR-IOV


&gt;	            tenant_network_types: &quot;flat,vlan&quot; # Can be mixed flat,vlan,vxlan
&gt;	            sriov:
&gt;	              nic_one:
&gt;	                devname: eth1
&gt;	                physical_network: physnet3


###  Jiri Broulik


  neutron floating IPs implementation


  Floating IP addresses


&gt;	        client:
&gt;	          enabled: true
&gt;	          server:
&gt;	            identity:
&gt;	              floating_ip:
&gt;	                prx01-instance:
&gt;	                  server: prx01.mk22-lab-basic.local
&gt;	                  subnet: private-subnet1
&gt;	                  network: public-net1
&gt;	                  tenant: demo
&gt;	                gtw01-instance:


  Instance port in the stated subnet will be associated with the dynamically generated floating IP.


  neutron client per tenant


  Client role


  Neutron networks


&gt;	              network:
&gt;	                inet1:
&gt;	                  shared: False
&gt;	                  admin_state_up: True
&gt;	                  router_external: True
&gt;	                  provider_physical_network: inet
&gt;	                  provider_network_type: flat
&gt;	                  provider_segmentation_id: 2
&gt;	                  subnet:
&gt;	                    inet1-subnet1:
&gt;	                      cidr: 192.168.90.0/24
&gt;	                      enable_dhcp: False
&gt;	                inet2:
&gt;	                  tenant: admin
&gt;	                  provider_network_type: &quot;vlan&quot;
&gt;	                    inet2-subnet1:
&gt;	                      cidr: 192.168.92.0/24
&gt;	                    inet2-subnet2:
&gt;	                      cidr: 192.168.94.0/24
&gt;	                      enable_dhcp: True
&gt;	            identity1:


  Neutron routers


&gt;	              router:
&gt;	                inet1-router:
&gt;	                  gateway_network: inet
&gt;	                  interfaces:
&gt;	                    - inet1-subnet1
&gt;	                    - inet1-subnet2


&gt;	      TODO: implement adding new interfaces to a router while updating it


  Neutron security groups


&gt;	              security_group:
&gt;	                security_group1:
&gt;	                  description: security group 1
&gt;	                  rules:
&gt;	                    - direction: ingress
&gt;	                      ethertype: IPv4
&gt;	                      protocol: TCP
&gt;	                      port_range_min: 1
&gt;	                      port_range_max: 65535
&gt;	                      remote_ip_prefix: 0.0.0.0/0
&gt;	                      protocol: UDP
&gt;	                      protocol: ICMP


&gt;	      TODO: implement updating existing security rules (now it adds new rule if trying to update existing one)


  neutron client


&gt;	                   description: security group 1
&gt;	                   rules:
&gt;	                     - direction: ingress
&gt;	                       ethertype: IPv4
&gt;	                       protocol: TCP
&gt;	                       port_range_min: 1
&gt;	                       port_range_max: 65535
&gt;	                       remote_ip_prefix: 0.0.0.0/0
&gt;	                       protocol: UDP
&gt;	                       protocol: ICMP


###  Kirill Bespalov


  OpenStack HTTPS Endpoints support


  Communication between services usually done via internal
  endpoints that are located in internal network. In some
  cases it is required to encrypt traffic even on internal
  network. This patch unhardcode communication protocol between


  Glance and other services. Also adds possibility to specify
  ca_file to verify SSL certificates of remote peers.


  This change is fully backward compatible.


  Configuring TLS communications


  **Note:** by default system wide installed CA certs are used, so ``cacert_file`` param is optional, as well as ``cacert``.


&gt;	  - **RabbitMQ TLS**
&gt;	   neutron:
  server, gateway, compute:


&gt;	          port: 5671
  (optional) cacert: cert body if the cacert_file does not exists
  (optional) cacert_file: /etc/openstack/rabbitmq-ca.pem
  (optional) version: TLSv1_2


&gt;	  - **MySQL TLS**
&gt;	     server:
&gt;	        database:
  (optional) cacert_file: /etc/openstack/mysql-ca.pem


&gt;	  - **Openstack HTTPS API**
&gt;	        identity:
&gt;	           protocol: https
  (optional) cacert_file: /etc/openstack/proxy.pem


  RabbitMQ TLS support


  OSCORE-385


&gt;	    Releases: Mitaka, Newton, Ocata
&gt;	    Usage: see README.rst


  Client-side RabbitMQ TLS configuration:


  |


  To enable TLS for oslo.messaging you need to provide the CA certificate.


  By default system-wide CA certs are used. Nothing should be specified except `ssl.enabled`.


&gt;	        message_queue:
&gt;	          ssl:
&gt;	            enabled: True


  Use `cacert_file` option to specify the CA-cert file path explicitly:


&gt;	            cacert_file: /etc/ssl/rabbitmq-ca.pem


  To manage content of the `cacert_file` use the `cacert` option:


&gt;	            cacert: |


&gt;	            cacert_file: /etc/openstack/rabbitmq-ca.pem


&gt;	  Notice:
  * The `message_queue.port` is set to **5671** (AMQPS) by default if `ssl.enabled=True`.
  * Use `message_queue.ssl.version` if you need to specify protocol version. By default is TLSv1 for python &lt; 2.7.9 and TLSv1_2 for version above.


###  Mykyta Karpin


  Allow to set workers for neutron


  This change adds ability to set custom workers number for neutron
  defined in pillar, backward compatibility is kept and hardcoded values
  from template will be used.


&gt;	          api_workers: 2
&gt;	          rpc_workers: 2
&gt;	          rpc_state_report_workers: 2
&gt;	            workers: 2


###  Oleg Bondarev


  Use a separate dir for vhost_user sockets


  Currently when OVS-DPDK is enabled, instances fail to spawn
  due to permissions mismatch, see nova bug for details:


&gt;	    https://bugs.launchpad.net/nova/+bug/1670950


  This patch updates openvswitch agent config to use a separate
  directory for the sockets. Nova formula patch:


&gt;	    https://gerrit.mcp.mirantis.net/#/c/11213/ takes care
  of creating the dir with proper permissions.


  Corresponding reclass-system patch is:


&gt;	    https://gerrit.mcp.mirantis.net/13307/
&gt;	          vhost_socket_dir: /var/run/openvswitch


###  Oleg Iurchenko


  Add Designate integration


  This patch adds Designate support


  Neutron with Designate


&gt;	            extension:
&gt;	              dns:
&gt;	                enabled: True
&gt;	                host: 127.0.0.1
&gt;	                port: 9001
&gt;	                protocol: http


  Add OVN support


  Install docs:


&gt;	    https://docs.openstack.org/networking-ovn/latest/install/index.html


  Partial Prod: PROD-15003


&gt;	    Co-Authored-By: Elena Ezhova &lt;eezhova@mirantis.com&gt;


  Neutron with OVN


  Control node:


&gt;	            engine: ovn
&gt;	              ovn:
&gt;	                driver: ovn
&gt;	            tenant_network_types: &quot;geneve,flat&quot;


  Compute node:


&gt;	          local_ip: 10.2.0.105
&gt;	          controller_vip: 10.1.0.101


  Change extensions list type


  This patch changes type of extensions pillar to dictionary


  qos


  Add extensions list


  This patch adds extensions list instead of hardcoded.


  Enable Neutron extensions (QoS, DNS, etc.)


&gt;	              - dns
&gt;	              - qos


  Allow disabling security groups


  For some use cases it might be needed to disable security groups.


  For example best DPDK performance can be achieved only with disabled
  security groups.


  Neutron with security groups disabled


&gt;	          security_groups_enabled: False


###  Ondrej Smola


  enable support for cors params


  Enable CORS parameters


&gt;	          cors:
&gt;	            allowed_origin: https:localhost.local,http:localhost.local
&gt;	            expose_headers: X-Auth-Token,X-Openstack-Request-Id,X-Subject-Token
&gt;	            allow_methods: GET,PUT,POST,DELETE,PATCH
&gt;	            allow_headers: X-Auth-Token,X-Openstack-Request-Id,X-Subject-Token
&gt;	            allow_credentials: True
&gt;	            max_age: 86400


  added avinetworks lbaas funcionality


  Neutron lbaas provides on the controller node


&gt;	      server:
&gt;	        lbaas:
&gt;	          providers:
&gt;	            avi_adc:
&gt;	              enabled: true
&gt;	              engine: avinetworks
&gt;	              controller_address: 10.182.129.239
&gt;	              controller_user: admin
&gt;	              controller_password: Cloudlab2016
&gt;	              controller_cloud_name: Default-Cloud
&gt;	            avi_adc2:


&gt;	  Note: If you want contrail lbaas then backend is only required. Lbaas in pillar should be define only if it should be disabled.


&gt;	          enabled: disabled


###  Richard Felkl


  added support for custom endpoint type


&gt;	              endpoint_type: internalURL


###  Simon Pasquier


  Add suport for availability zones


  Neutron supports availability zones but only for DHCP and L3 agents.


&gt;	          availability_zone: az1


###  Swann Croiset


  Configure pagination


  The pagination is useful to retrieve a large bunch of resources,
  because a single request may fail (timeout).


&gt;	          allow_pagination: true
&gt;	          pagination_max_limit: 100
&gt;	  Note: The pagination is useful to retrieve a large bunch of resources,
  because a single request may fail (timeout). This is enabled with both
  parameters *allow_pagination* and *pagination_max_limit* as shown above.


###  Thom Gerdes


  Allow setting some VXLAN paramters


  Allow setting the multicast group and the VNI range that will be used
  when using the ML2 plugin.


  Additonal VXLAN tenant network settings


  The default multicast group of 224.0.0.1 only multicasts to a single subnet.


  Allow overriding it to allow larger underlay network topologies.


&gt;	          vxlan:
&gt;	            group: 239.0.0.0/8
&gt;	            vni_ranges: &quot;2:65535&quot;


###  Vasyl Saienko


  Cover bridge physnet mapping for Ironic


  Ironic in flat scenario require a separate neutron flat network.


  This patch adds physnet3:br-baremetal to bridge_mappings when
  neutron.gateway.ironic_enabled is defined
  is suggested only for those who really know what they're doing with Neutron).
  infrastructure and that remains encapsulated. External traffic will be
  routed to via the network nodes.


  The intention is that each tenant will require at least two (2) vrouters
  one to be utilised


&gt;	            password: pass
&gt;	          dvr: False


  This section describes a network solution that utilises VxLAN
  overlay networks with DVR with North-South and East-West. Network


&gt;	          external_access: True


# Formula nfs


###  Aleš Komárek


  Update and rename README.md to README.rst


  NFS Formula


  Sample Pillars


  NFS Server: Basic sharing


&gt;	      nfs:
&gt;	        server:
&gt;	          enabled: true
&gt;	          share:
&gt;	            home_majklk:
&gt;	              path: /home/majklk
&gt;	              host:
&gt;	                inter:
&gt;	                  host: 10.10.10.0/24
&gt;	                  params:
&gt;	                  - rw
&gt;	                  - no_root_squash
&gt;	                  - sync
&gt;	                pub:
&gt;	                  host: 10.0.0.0/24


  NFS Client with mounted directory


&gt;	        client:
&gt;	          mount:
&gt;	            samba1:
&gt;	              path: /media/myuser/public/
&gt;	              fstype: nfs
&gt;	              host: //192.168.0.1/storage


  NFS mount


&gt;	      linux:
&gt;	        storage:
&gt;	         mount:
&gt;	            nfs:
&gt;	              enabled: true
&gt;	              path: /var/lib/glance
&gt;	              file_system: nfs
&gt;	              device: 10.0.103.152:/storage/glance/vpc20


  More Information


  * http://wiki.ubuntu.cz/nfs


###  Tomáš Kukrál


  fix README


&gt;	              device: 192.168.0.1:/home/majklk


# Formula nginx


###  Aleš Komárek


  Update README.rst


  Nginx Formula


  Sample Pillars


  More Information


###  Dmitry Stremkovskiy


  Presenting upstream feature for nginx


  With this feature you will be able to set up nginx load balancer


  Simple load balancer


&gt;	      nginx:
&gt;	        server:
&gt;	          upstream:
&gt;	            horizon-upstream:
&gt;	              backend1:
&gt;	                address: 10.10.10.113
&gt;	                port: 8078
&gt;	                opts: weight=3
&gt;	              backend2:
&gt;	                address: 10.10.10.114
&gt;	          site:
&gt;	            nginx_proxy_openstack_web:
&gt;	              enabled: true
&gt;	              type: nginx_proxy
&gt;	              name: openstack_web
&gt;	              proxy:
&gt;	                upstream_proxy_pass: http://horizon-upstream
&gt;	              host:
&gt;	                name: 192.168.0.1
&gt;	                port: 31337


  Presenting stream feature for nginx. TCP/UDP port proxy/balancing


  With this patchset you will be able to set up tcp or udp proxy.


  Simple TCP/UDP proxy


&gt;	          stream:
&gt;	            rabbitmq:
&gt;	                port: 5672
&gt;	              backend:
&gt;	                server1:
&gt;	                  address: 10.10.10.113
&gt;	                  port: 5672
&gt;	                  least_conn: true
&gt;	                  hash: &quot;$remote_addr consistent&quot;
&gt;	            unbound:
&gt;	                bind: 127.0.0.1
&gt;	                port: 53
&gt;	                protocol: udp
&gt;	                  port: 5353


###  Filip Pytloun


  Unify Makefile, .gitignore and update readme


###  Niklaus Giger


  Fix Readme: type names must preceeded by nginx_


&gt;	              type: nginx_static


###  Ondrej Smola


  Added possibility to disable proxy_request_buffering in nginx


&gt;	                request_buffer: false


###  Ramon Melero


  Handle dynamic nginx ssl options
  ssl protocol options for nginx are hard coded in the formula
  added documentation for changing nginx ssl options


  Change nginx server ssl protocol options in openstack/proxy.yml


&gt;	            site01:
&gt;	              name: site01
&gt;	                name: site01.domain.com
&gt;	              ssl:
&gt;	                enabled: true
&gt;	                key_file:  /srv/salt/pki/${_param:cluster_name}/${salt:minion:cert:proxy:common_name}.key
&gt;	                cert_file: /srv/salt/pki/${_param:cluster_name}/${salt:minion:cert:proxy:common_name}.crt
&gt;	                chain_file: /srv/salt/pki/${_param:cluster_name}/${salt:minion:cert:proxy:common_name}-with-chain.crt
&gt;	                protocols: TLSv1 TLSv1.1 TLSv1.2
&gt;	                ciphers: &quot;ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:DHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-AES128-SHA256:ECDHE-RSA-AES128-SHA256:ECDHE-ECDSA-AES128-SHA:ECDHE-RSA-AES256-SHA384:ECDHE-RSA-AES128-SHA:ECDHE-ECDSA-AES256-SHA384:ECDHE-ECDSA-AES256-SHA:ECDHE-RSA-AES256-SHA:DHE-RSA-AES128-SHA256:DHE-RSA-AES128-SHA:DHE-RSA-AES256-SHA256:DHE-RSA-AES256-SHA:ECDHE-ECDSA-DES-CBC3-SHA:ECDHE-RSA-DES-CBC3-SHA:EDH-RSA-DES-CBC3-SHA:AES128-GCM-SHA256:AES256-GCM-SHA384:AES128-SHA256:AES256-SHA256:AES128-SHA:AES256-SHA:DES-CBC3-SHA:!DSS&quot;
&gt;	                prefer_server_ciphers: true
&gt;	                ecdh_curve: secp521r1


# Formula nodejs


###  Filip Pytloun


  Unify Makefile, .gitignore and update readme


# Formula nova


###  Aleš Komárek


  Update README.rst


  Nova Formula


  Sample Pillars


###  Damian Szeluga


  Expand readme, clean up code and move client.identity outside az


&gt;	          aggregates:
&gt;	          - hosts_with_fc
&gt;	          - hosts_with_ssd


  Create aggregates + add hosts to aggregates


  Aggregates


&gt;	      nova:
&gt;	        client:
&gt;	          enabled: true
&gt;	          server:
&gt;	            identity:
&gt;	              aggregates:
&gt;	              - aggregate1
&gt;	              - aggregate2


###  Dmitry Stremkovskiy


  Adding ability to set cinder/cross_az_attach setting


&gt;	          cross_az_attach: false


  Unhardcoding disk_cachemodes nova option


&gt;	          disk_cachemodes: network=writeback,block=none


  Adding ability to set DEFAULT/host


&gt;	          host: node-12.domain.tld


  Unhradcoding compute.user.groups for nova. Upgrade related


  Group membership for user nova (upgrade related)


&gt;	        compute:
  ...


&gt;	          user:
&gt;	            groups:
&gt;	            - libvirt


  Presenting upgrade_levels option


  Upgrade levels


&gt;	        controller:
&gt;	          upgrade_levels:
&gt;	            compute: juno


  Unhardcoding my_ip option for compute


&gt;	          my_ip: 10.1.0.16


  Adding support for instances_path


  Nova configured with NFS


&gt;	          instances_path: /mnt/nova/instances


&gt;	      linux:
&gt;	        storage:
&gt;	          mount:
&gt;	            nfs_nova:
&gt;	              enabled: true
&gt;	              path: ${nova:compute:instances_path}
&gt;	              device: 172.31.35.145:/data
&gt;	              file_system: nfs
&gt;	              opts: rw,vers=3


###  Dmitry Ukov


  Policy.json should be defined by user


  User can override and add values to policy.json by creating flat
  key-value structure under nova:controller:policy.


  Configuration of policy.json file


  ....


&gt;	          policy:
&gt;	            context_is_admin: 'role:admin or role:administrator'
  'compute:create': 'rule:admin_or_owner'


&gt;	            # Add key without value to remove line from policy.json
  'compute:create:attach_network':


###  Filip Pytloun


  Unify Makefile, .gitignore and update readme


###  Jakub Pavlik


  CPU pinning &amp; Hugepages libvirt mounts


  Hugepages mount point definition for libivirt. Nova vcpu pin set
  parameter definition.


&gt;	    Epic: PROD-8959


  CPU pinning &amp; Hugepages


  CPU pinning of virtual machine instances to dedicated physical CPU cores.


  Hugepages mount point for libvirt.


&gt;	    nova:
&gt;	      controller:
&gt;	        scheduler_default_filters: &quot;DifferentHostFilter,RetryFilter,AvailabilityZoneFilter,RamFilter,CoreFilter,DiskFilter,ComputeFilter,ComputeCapabilitiesFilter,ImagePropertiesFilter,ServerGroupAntiAffinityFilter,ServerGroupAffinityFilter,NUMATopologyFilter,AggregateInstanceExtraSpecsFilter&quot;


&gt;	      compute:
&gt;	        vcpu_pin_set: 2,3,4,5
&gt;	        hugepages:
&gt;	          mount_points:
&gt;	          - path: /mnt/hugepages_1GB
&gt;	          - path: /mnt/hugepages_2MB


  SRIOV support


  Enable SRIOV support in nova scheduler and compute.


&gt;	    Epic: PROD-8956


  SRIOV


  Add PciPassthroughFilter into scheduler filters and NICs on specific compute nodes.


&gt;	        sriov: true
&gt;	        scheduler_default_filters: &quot;DifferentHostFilter,RetryFilter,AvailabilityZoneFilter,RamFilter,CoreFilter,DiskFilter,ComputeFilter,ComputeCapabilitiesFilter,ImagePropertiesFilter,ServerGroupAntiAffinityFilter,ServerGroupAffinityFilter,PciPassthroughFilter&quot;


&gt;	        sriov:
&gt;	          nic_one:
&gt;	            devname: eth1
&gt;	            physical_network: physnet1


###  Jiri Broulik


  nova availability zones


&gt;	          availability_zone: availability_zone_01
&gt;	              flavor1:
&gt;	              flavor2:
&gt;	                flavor_id: auto
&gt;	                ram: 4096
&gt;	                disk: 20
&gt;	                vcpus: 2


  Availability zones


&gt;	              availability_zones:
&gt;	              - availability_zone_01
&gt;	              - availability_zone_02


  introduce nova client implementation


  Client role


  Nova flavors


&gt;	      client:
&gt;	        enabled: true
&gt;	        server:
&gt;	          identity:
&gt;	            flavor:
&gt;	              jirka-flavor1:
&gt;	                flavor_id: 10
&gt;	                disk: 10
&gt;	                vcpus: 1
&gt;	          identity1:


###  Kalynovskyi


  Adding feature to support lvm for ephemeral VMs


  Nova with ephemeral configured with LVM


&gt;	          lvm:
&gt;	            ephemeral: yes
&gt;	            images_volume_group: nova_vg
&gt;	            nova_vg:
&gt;	              name: nova_vg
&gt;	              devices:
&gt;	                - /dev/sdf
&gt;	                - /dev/sdd
&gt;	                - /dev/sdg
&gt;	                - /dev/sde
&gt;	                - /dev/sdc
&gt;	                - /dev/sdj
&gt;	                - /dev/sdh


###  Kirill Bespalov


  RabbitMQ TLS support


&gt;	    Releases: Mitaka, Newton, Ocata
&gt;	    Usage: see README.rst


  OSCORE-140


  Client-side RabbitMQ TLS configuration:


  To enable TLS for oslo.messaging you need to provide the CA certificate.


  By default system-wide CA certs is used. Nothing should be specified except `ssl.enabled`.


&gt;	        message_queue:
&gt;	          ssl:
&gt;	            enabled: True


  Use `cacert_file` option to specify the CA-cert file path explicitly:


&gt;	            cacert_file: /etc/ssl/rabbitmq-ca.pem


  To manage content of the `cacert_file` use the `cacert` option:


&gt;	            cacert: |


&gt;	            cacert_file: /etc/openstack/rabbitmq-ca.pem


&gt;	  Notice:
  * The `message_queue.port` is set to **5671** (AMQPS) by default if `ssl.enabled=True`.
  * Use `message_queue.ssl.version` if you need to specify protocol version. By default is TLSv1 for python &lt; 2.7.9 and TLSv1_2 for version above.


&gt;	       compute:


###  kkalynovskyi


  Change allows to set user and group for QEMU processes run by the system instance


  And specify whether libvirt should dynamically change file ownership to match the


  Configured user/group above


  Group and user to be used for QEMU processes run by the system instance


&gt;	          qemu:
&gt;	            user: nova
&gt;	            group: cinder
&gt;	            dynamic_ownership: 1


###  Michel Nederlof


  Add option max_concurrent_live_migrations


  Number of concurrent live migrates


  Default is to have no concurrent live migrations (so 1 live-migration at a time).


  Excerpt from config options page (https://docs.openstack.org/ocata/config-reference/compute/config-options.html):


  Maximum number of live migrations to run concurrently. This limit is
  enforced to avoid outbound live migrations overwhelming the host/network
  and causing failures. It is not recommended that you change this unless
  you are very sure that doing so is safe and stable in your environment.


  Possible values:


&gt;	    - 0 : treated as unlimited.
&gt;	    - Negative value defaults to 0.
&gt;	    - Any positive integer representing maximum number of live migrations to run concurrently.


  To configure this option:


&gt;	        max_concurrent_live_migrations: 1  # (1 is the default)


  Update README with exmple of configuring the config drive


  Config drive options


  See example below on how to configure the options for the config drive.


&gt;	        config_drive:
&gt;	          forced: True  # Default: True
&gt;	          cdrom: True  # Default: False
&gt;	          format: iso9660  # Default: vfat
&gt;	          inject_password: False  # Default: False


  Updating to singular groupname and hide empty section if no workaround defined


&gt;	        workaround:


  Add configurable workaround to enable live snapshots


  Nova compute workarounds


  Live snapshotting is disabled by default in nova. To enable this, it needs a manual switch.


  From manual:


&gt;	    # When using libvirt 1.2.2 live snapshots fail intermittently under load
&gt;	    # (likely related to concurrent libvirt/qemu operations). This config
&gt;	    # option provides a mechanism to disable live snapshot, in favor of cold
&gt;	    # snapshot, while this is resolved. Cold snapshot causes an instance
&gt;	    # outage while the guest is going through the snapshotting process.
&gt;	    #
&gt;	    # For more information, refer to the bug report:
&gt;	    #   https://bugs.launchpad.net/nova/+bug/1334398


  Configurable pillar data:


&gt;	        workarounds:
&gt;	          disable_libvirt_livesnapshot: False


  Adding Trim/Unmap support for libvirt / ceph


  Hardware Trip/Unmap Support


  To enable TRIM support for ephemeral images (thru nova managed images), libvirt has this option.


&gt;	        libvirt:
&gt;	          hw_disk_discard: unmap


  In order to actually utilize this feature, the following metadata must be set on the image as well, so the SCSI unmap is supported.


&gt;	    glance image-update --property hw_scsi_model=virtio-scsi &lt;image&gt;
&gt;	    glance image-update --property hw_disk_bus=scsi &lt;image&gt;


  Allow a custom filter to be used / injected.


  Custom Scheduler filters


  If you have a custom filter, that needs to be included in the scheduler, then you can include it like so:


&gt;	        scheduler_custom_filters:
&gt;	        - my_custom_driver.nova.scheduler.filters.my_custom_filter.MyCustomFilter


&gt;	        # Then add your custom filter on the end (make sure to include all other ones that you need as well)
&gt;	        scheduler_default_filters: &quot;DifferentHostFilter,RetryFilter,AvailabilityZoneFilter,RamFilter,CoreFilter,DiskFilter,ComputeFilter,ComputeCapabilitiesFilter,ImagePropertiesFilter,ServerGroupAntiAffinityFilter,ServerGroupAffinityFilter,PciPassthroughFilter,MyCustomFilter&quot;


###  Oleg Iurchenko


  Add Barbican integration to Nova


  This patch adds Barbican integration to Nova


&gt;	          barbican:
&gt;	            enabled: true


  Enable Barbican integration


###  Ondrej Smola


  added cors support into nova controller


  Enable CORS parameters


&gt;	          cors:
&gt;	            allowed_origin: https:localhost.local,http:localhost.local
&gt;	            expose_headers: X-Auth-Token,X-Openstack-Request-Id,X-Subject-Token
&gt;	            allow_methods: GET,PUT,POST,DELETE,PATCH
&gt;	            allow_headers: X-Auth-Token,X-Openstack-Request-Id,X-Subject-Token
&gt;	            allow_credentials: True
&gt;	            max_age: 86400


###  Petr Michalec


  Add opt. resume guests on host boot (#10)


&gt;	          resume_guests_state_on_host_boot: False


###  Petr Jediný


  SR-IOV VFs need to be accesible by qemu


  SR-IOV


###  Simon Pasquier


  Allow to configure the pagination


&gt;	          osapi_max_limit: 500


###  Thom Gerdes


  Allow overriding the  scheduler_host_manager


  Deployments with Ironic need a custom host manager. Allow overriding it.


  Don't add PciPassthroughFilter explicitly to the active filters.
  nova.scheduler.filters.all_filters should include this if it's in the
  default filters, and if it is removed from scheduler_default_filters in
  the pillar, the new scheduler_custom_filters will allow users to
  manually add it.


  Scheduler Host Manager


  Specify a custom host manager.


&gt;	        scheduler_host_manager: ironic_host_manager


  Allow setting cpu_mode
  libvirt CPU mode


  Allow setting the model of CPU that is exposed to a VM. This allows better
  support live migration between hypervisors with different hardware, among other
  things. Defaults to host-passthrough.


&gt;	        cpu_mode: host-model


# Formula ntp


###  Filip Pytloun


  Unify Makefile, .gitignore and update readme


# Formula octavia


###  Elena Ezhova


  Add creation of Octavia private key to the manager state


  Relates to PROD-12506


&gt;	          ssh:
&gt;	            private_key: |


  MIIEpAIBAAKCAQEAtjnPDJsQToHBtoqIo15mdSYpfi8z6DFMi8Gbo0KCN33OUn5u


  OctbdtjUfeuhvI6px1SCnvyWi09Ft8eWwq+KwLCGKbUxLvqKltuJ7K3LIrGXkt+m
  qZN4O9XKeVKfZH+mQWkkxRWgX2r8RKNV3GkdNtd74VjhP+R6XSKJQ1Z8b7eHM10v
  6IjTY/jPczjK+eyCeEj4qbSnV8eKlqLhhquuSQRmUO2DRSjLVdpdf2BB4/BdWFsD


  YOmX7mb8kpEr9vQ+c1JKMXDwD6ehzyU8kE+1kVm5zOeEy4HdYIMpvUfN49P1anRV
  2ISQ1ZE+r22IAMKl0tekrGH0e/1NP1DF5rINMwIDAQABAoIBAQCkP/cgpaRNHyg8


  ISKIHs67SWqdEm73G3ijgB+JSKmW2w7dzJgN//6xYUAnP/zIuM7PnJ0gMQyBBTMS


  NBTv5spqZLKJZYivj6Tb1Ya8jupKm0jEWlMfBo2ZYVrfgFmrfGOfEebSvmuPlh9M
  vuzlftmWVSSUOkjODmM9D6QpzgrbpktBuA/WpX+6esMTwJpOcQ5xZWEnHXnVzuTc


  SncodVweE4gz6F1qorbqIJz8UAUQ5T0OZTdHzIS1IbamACHWaxQfixAO2s4+BoUK


  ANGGZWkfneCxx7lthvY8DiKn7M5cSRnqFyDToGqaLezdkMNlGC7v3U11FF5blSEW
  fL1o/HwBAoGBAOavhTr8eqezTchqZvarorFIq7HFWk/l0vguIotu6/wlh1V/KdF+
  aLLHgPgJ5j+RrCMvTBoKqMeeHfVGrS2udEy8L1mK6b3meG+tMxU05OA55abmhYn7
  7vF0q8XJmYIHIXmuCgF90R8Piscb0eaMlmHW9unKTKo8EOs5j+D8+AMJAoGBAMo4
  8WW+D3XiD7fsymsfXalf7VpAt/H834QTbNZJweUWhg11eLutyahyyfjjHV200nNZ
  cnU09DWKpBbLg7d1pyT69CNLXpNnxuWCt8oiUjhWCUpNqVm2nDJbUdlRFTzYb2fS


  ZC4r0oQaPD5kMLSipjcwzMWe0PniySxNvKXKInFbAoGBAKxW2qD7uKKKuQSOQUft
  aAksMmEIAHWKTDdvOA2VG6XvX5DHBLXmy08s7rPfqW06ZjCPCDq4Velzvgvc9koX
  d/lP6cvqlL9za+x6p5wjPQ4rEt/CfmdcmOE4eY+1EgLrUt314LHGjjG3ScWAiirE


  QyDrGOIGaYoQf89L3KqIMr0JAoGARYAklw8nSSCUvmXHe+Gf0yKA9M/haG28dCwo
  780RsqZ3FBEXmYk1EYvCFqQX56jJ25MWX2n/tJcdpifz8Q2ikHcfiTHSI187YI34
  lKQPFgWb08m1NnwoWrY//yx63BqWz1vjymqNQ5GwutC8XJi5/6Xp+tGGiRuEgJGH


  EIPUKpkCgYAjBIVMkpNiLCREZ6b+qjrPV96ed3iTUt7TqP7yGlFI/OkORFS38xqC
  hBP6Fk8iNWuOWQD+ohM/vMMnvIhk5jwlcwn+kF0ra04gi5KBFWSh/ddWMJxUtPC1
  2htvlEc6zQAR6QfqXHmwhg1hP81JcpqpicQzCMhkzLoR1DC6stXdLg==


&gt;	            user: octavia
&gt;	            group: octavia


  Update SSL metadata


  Depends on: https://gerrit.mcp.mirantis.net/7678


  Related PROD: PROD-11933


&gt;	            client_cert_key: '/etc/octavia/certs/client.key'
&gt;	            client_cert_all: '/etc/octavia/certs/client_all.pem'


  Get Octavia network resources data from mine


  Get the following data from mine:


  Related prod: PROD-11917


  Get amphora image owner ID from mine


  Add api and manager states and metadata


  Added tests and updated the README.rst


  Octavia is an open source, operator-scale load balancing solution designed to
  work with OpenStack. It accomplishes its delivery of load balancing services
  by managing a fleet of virtual machines, known as amphorae, which it spins up
  on demand.


  Octavia is designed to “plug in” to Neutron LBaaS in the same way that any
  proprietary vendor solution would: through a Neutron LBaaS version 2 driver
  interface. Octavia plans to supplant Neutron LBaaS as the load balancing
  solution for OpenStack. At that time, third-party vendor drivers that presently
  “plug in” to Neutron LBaaS will plug in to Octavia instead. For end-users,
  this transition should be relatively seamless, because Octavia supports
  the Neutron LBaaS v2 API and it has a similar CLI interface.


  Octavia API service pillar:


&gt;	        api:
&gt;	          version: ocata
&gt;	          bind:
&gt;	            address: 127.0.0.1
&gt;	            port: 9876
&gt;	          database:
&gt;	            engine: mysql
&gt;	            host: 127.0.0.1
&gt;	            port: 3306
&gt;	            name: octavia
&gt;	            password: password
&gt;	          identity:
&gt;	            engine: keystone
&gt;	            region: RegionOne
&gt;	            port: 35357
&gt;	            tenant: service
&gt;	          message_queue:
&gt;	            engine: rabbitmq
&gt;	            port: 5672
&gt;	            user: openstack
&gt;	            virtual_host: '/openstack'
&gt;	          haproxy_amphora:
&gt;	            client_cert: '/etc/octavia/certs/client.pem'
&gt;	            server_ca: '/etc/octavia/certs/ca_01.pem'


  Octavia manager service pillar:


&gt;	      octavia:
&gt;	        manager:
&gt;	          enabled: true
&gt;	          certificates:
&gt;	            ca_private_key_passphrase: foobar
&gt;	            ca_private_key: '/etc/octavia/certs/private/cakey.pem'
&gt;	            ca_certificate: '/etc/octavia/certs/ca_01.pem'
&gt;	          controller_worker:
&gt;	            amp_boot_network_list: '01d3edaa-422c-40b9-b265-425c981691e7'
&gt;	            amp_flavor_id: '967972bb-ab54-4679-9f53-bf81d5e28154'
&gt;	            amp_image_owner_id: '68520e9f926441ddb37b7c744c4005b7'
&gt;	            amp_image_tag: amphora
&gt;	            amp_secgroup_list: '9fcd532e-5715-423a-8e3f-51abddbe7705'
&gt;	            amp_ssh_key_name: octavia_ssh_key
&gt;	            loadbalancer_topology: 'SINGLE'
&gt;	          health_manager:
&gt;	            bind_ip: 192.168.0.12
&gt;	            heartbeat_key: 'insecure'
&gt;	          house_keeping:
&gt;	            spare_amphora_pool_size: 0


  Initial commit


  Setup formula's skeleton


  Octavia


  Install and configure Octavia.


  Sample pillars


&gt;	        server:


  More information


  Octavia developer documentation:


&gt;	      https://docs.openstack.org/developer/octavia


  Release notes:


&gt;	      https://docs.openstack.org/releasenotes/octavia


# Formula octoprint


###  Ales Komarek


  Docfix


  The web interface for your 3D printer.


  Single printer [deprecated]


  Multi printers setup


&gt;	      octoprint:
&gt;	        server:
&gt;	          enabled: true
&gt;	          source:
&gt;	            engine: git
  address 'https://github.com/foosel/OctoPrint.git'


&gt;	            rev: &quot;master&quot;
&gt;	          printer:
&gt;	            printer01:
&gt;	              bind:
&gt;	                address: 0.0.0.0
&gt;	                port: 5001
&gt;	              device:
&gt;	                bus: serial
&gt;	                port: /dev/ACM01
&gt;	                model: prusa-mk2
&gt;	              camera:
&gt;	                protocol: mjpg
&gt;	                url: localhost
&gt;	                port: 1234
&gt;	            printer02:
&gt;	                port: /dev/ACM02
&gt;	                model: prusa-clone
&gt;	                port: 5002


  More Information


  Readme fix


  The snappy web interface for your 3D printer.


  External links


  Updated docs to rst


  Octoprint formula


  The responsive web interface for 3D printer


  Sample pillars


  OctoPrint web UI with printers.


&gt;	            engine: serial
&gt;	            webcam: true
&gt;	          webcam:
&gt;	            host: localhost
&gt;	            port: 1234


  Read more


  * http://octoprint.org/
  * https://github.com/foosel/OctoPrint


###  Filip Pytloun


  Unify Makefile, .gitignore and update readme


# Formula odoo


# Formula openbook


# Formula opencontrail


###  Ales Komarek


  Enforcing virtual routers, config/analytics/database nodes


  Contrail client


  Basic parameters with identity and host configs


&gt;	    opencontrail:
&gt;	      client:
&gt;	        identity:
&gt;	          user: admin
&gt;	          project: admin
&gt;	          password: adminpass
&gt;	          host: keystone_host
&gt;	        config:
&gt;	          host: contrail_api_host
&gt;	          port: contrail_api_ort


  Enforcing virtual routers


  ...


&gt;	        virtual_router:
&gt;	          cmp01:
&gt;	            ip_address: 172.16.0.11
&gt;	            dpdk_enabled: True
&gt;	          cmp02:
&gt;	            ip_address: 172.16.0.12


  Enforcing control nodes


&gt;	        bgp_router:
&gt;	          ntw01:
&gt;	            type: control-node
&gt;	          nwt02:
&gt;	          nwt03:
&gt;	            ip_address: 172.16.0.13


  Enforcing edge BGP routers


&gt;	          mx01:
&gt;	            type: router
&gt;	            ip_address: 172.16.0.21
&gt;	            asn: 64512
&gt;	          mx02:
&gt;	            ip_address: 172.16.0.22


  Enforcing config nodes


&gt;	        config_node:
&gt;	          ctl01:
&gt;	          ctl02:


  Enforcing database nodes


&gt;	        database_node:
&gt;	          ntw02:


  Enforcing analytics nodes


&gt;	        analytics_node:
&gt;	          nal01:
&gt;	            ip_address: 172.16.0.31
&gt;	          nal02:
&gt;	            ip_address: 172.16.0.32


  Install compute node


###  Aleš Komárek


  Update README.rst


  OpenContrail Formula


  Sample Pillars


  Kubernetes support


  Debugging


###  Andrey


  Define metadata_secret for vrouter agent


  Set up metadata secret for the Vrouter


  In order to get cloud-init within the instance to properly fetch 
  instance metadata, metadata_proxy_secret in the Vrouter agent config
  should match the value in nova.conf. The administrator should define
  it in the pillar:


&gt;	      opencontrail:
&gt;	        compute:
&gt;	          metadata:
&gt;	            secret: opencontrail


###  Dmitry Stremkovskiy


  Enabling log4j.rootLogger configuration parameter


&gt;	    Synopsis: By default, logger is using TRACE parameter which leads
&gt;	    Fix:      One can configure log4j.rootLogger to use INFO parameter
&gt;	          rootlogger: &quot;INFO, CONSOLE&quot;


  Unhardcoding concurrent_compactors variable


&gt;	    Tags: Customer-Related
&gt;	          concurrent_compactors: 1


  Adding ability to set DEFAULT/hostname


&gt;	          hostname: node-12.domain.tld


  Unhardcoding compaction_throughput_mb_per_sec variable


&gt;	          compaction_throughput_mb_per_sec: 16


###  Filip Pytloun


  Unify Makefile, .gitignore and update readme


###  Jakub Pavlik


  Parametrize socket_mem for dpdk vrouter


&gt;	            taskset: &quot;0x0000003C00003C&quot;
&gt;	            socket_mem: &quot;1024,1024&quot;


  OpenContrail DPDK support


  Enable DPDK vrouter agent installation and configuration


&gt;	    Epic: PROD-9023


  DPDK vRouter


&gt;	          dpdk:
&gt;	            enabled: true
&gt;	            taskset: 0x0000003C00003C
&gt;	          interface:
&gt;	            mac_address: 90:e2:ba:7c:22:e1
&gt;	            pci: 0000:81:00.1


###  Marek Celoud


  Enable multiple workers for alarm-gen


  Increase number of alarm-gen workers


  Port prefix will increment used ports by workers starting with 5901.


&gt;	      collector:
&gt;	        alarm_gen:
&gt;	          workers: 1
&gt;	          port_prefix: 59


###  Michel Nederlof


  Add configuration options for Default and vDNS default forwarders


  The default forwarders are read from resolv.conf file
  so basically it is just rendering a custom resolv.conf file
  and configuring the location.


  Contrail DNS custom forwarders


  By default Contrail uses the /etc/resolv.conf file to determine the upstream DNS servers.


  This can have some side-affects, like resolving internal DNS entries on you public instances.


  In order to overrule this default set, you can configure nameservers using pillar data.


  The formula is then responsible for configuring and generating a alternate resolv.conf file.


&gt;	  Note: this has been patched recently in the Contrail distribution of Mirantis:
&gt;	  https://github.com/Mirantis/contrail-controller/commit/ed9a25ccbcfebd7d079a93aecc5a1a7bf1265ea4
&gt;	  https://github.com/Mirantis/contrail-controller/commit/94c844cf2e9bcfcd48587aec03d10b869e737ade


  To change forwarders for the default-dns option (which is handled by compute nodes):


&gt;	      compute:
  ....


&gt;	        dns:
&gt;	          forwarders:
&gt;	          - 8.8.8.8
&gt;	          - 8.8.4.4


  To change forwarders for vDNS zones (handled by control nodes):


&gt;	      control:


  Ability to configure default quotas for new projects (#27)


  As documented on the Juniper / Contrail wiki:


&gt;	    https://github.com/Juniper/contrail-controller/wiki/Defining-default-Quotas


  Configuring OpenStack default quotasx


&gt;	      config:
&gt;	        quota:
&gt;	          network: 5
&gt;	          subnet: 10
&gt;	          router: 10
&gt;	          floating_ip: 100
&gt;	          secgroup: 1000
&gt;	          secgroup_rule: 1000
&gt;	          port: 1000
&gt;	          pool: -1
&gt;	          member: -1
&gt;	          health_monitor: -1
&gt;	          vip: -1


  Enforcing physical routers
  h


###  Pavel Svimbersky


  Config only


  Added Global VRouter config to OC client


  Enforcing global vrouter config


&gt;	        global_vrouter_config:
&gt;	          name: global-vrouter-config
&gt;	          parent_type: global-system-config
&gt;	          encap_priority: &quot;MPLSoUDP,MPLSoGRE&quot;
&gt;	          vxlan_vn_id_mode: automatic
&gt;	          fq_names:
&gt;	            - 'default-global-system-config'
&gt;	            - 'default-global-vrouter-config'


###  Petr Jediný


  Parameterization of cassandra gc logging rotation


  Cassandra GC logging


  From Contrail version 3 you can set a way you want to handle Cassandra GC logs.


  The behavior is controlled by `cassandra_gc_logging`. Valid values are
  'rotation' (default), 'legacy' and false.


&gt;	  - 'rotation' is supported by JDK 6u34 7u2 or later and handles rotation of log
  files automatically.


&gt;	  - 'legacy' is a way to support older JDKs and you will need to handle logs by
  other means. This can be handled for example by using
  `- service.opencontrail.database.cassandra_log_cleanup` in your reclass model.


&gt;	  - false will disable the cassandra gc logging


&gt;	        database:
&gt;	          cassandra_gc_logging: false


  Add LBaaSv2 Barbican support


  Barbican support is needed for HTTPS termination


  In order to get cloud-init within the instance to properly fetch


  Add auth info for Barbican on compute nodes


&gt;	          lbaas:
&gt;	            secret_manager:
&gt;	              engine: barbican
&gt;	              identity:
&gt;	                user: admin
&gt;	                password: &quot;supersecretpassword123&quot;
&gt;	                tenant: admin


  Add gateway_mode configuration


  Compute nodes with gateway_mode


  Gateway mode: can be server/ vcpe (default is none)


&gt;	          gateway_mode: server


  Introduce RPF default override


  Override RPF default in Contrail API


  From MCP1.1 with OpenContrail &gt;= 3.1.1 you can override RPF default for newly
  created virtual networks. This can be useful for usecases like running


  Calico and K8S in overlay. The `override_rpf_default_by` has valid values
  `disable`, `enable`. If not defined, the configuration fallbacks to Contrail
  default - currently `enable`.


&gt;	          override_rpf_default_by: 'disable'


  Periodic keystone project resync switch


  Add ability to switch from on demand mode.


  Contrail version must &gt;= 3.0. It is useful especially for Keystone v3.


  Switch from on demand to periodic keystone sync


  This can be useful when you want to sync projects from OpenStack to Contrail
  automatically. The period of sync is 60s.


&gt;	          identity:
&gt;	            sync_on_demand: false


  For OpenContrail version &gt;= 3.1.1 and Cassandra &gt;= 2.1 we should override WebUI's cassandra port from 9160 to 9042.


  Add link local services provisioning support


  Enforcing Link Local Services


&gt;	        linklocal_service:
&gt;	           # example with dns name address (only one permited)
&gt;	           meta1:
&gt;	             lls_ip: 10.0.0.23
&gt;	             lls_port: 80
&gt;	             ipf_addresses: &quot;meta.example.com&quot;
&gt;	             ipf_port: 80
&gt;	           # example with multiple ip addresses
&gt;	           meta2:
&gt;	             ipf_addresses:
&gt;	             - 10.10.10.10
&gt;	             - 10.20.20.20
&gt;	             - 10.30.30.30
&gt;	           # example with one ip address
&gt;	           meta3:
&gt;	           # example with name override
&gt;	           lls_meta4:
&gt;	             name: meta4


###  Petr Michalec


  Add's support for Juniper Contrail packaging + test-kitchen (#2)


  Package source


  Formula support OpenContrail as well as Juniper Contrail package repository in the backend.


  Differences withing the configuration and state run are controlled by
  ``opencontrail.common.vendor: [opencontrail|juniper]`` pillar attribute.


  Default value is set to ``opencontrail``.


  Juniper releases tested with this formula:


&gt;	   - 3.0.2.x


  To use Juniper Contrail repository as a source of packages override pillar as in this example:


&gt;	        common:
&gt;	          vendor: juniper


  Parameterize cassandra port in OpenContrail WebUI


  OpenContrail WebUI version &gt;= 3.1.1


  For OpenContrail version &gt;= 3.1.1 and Cassandra &gt;=2.1 we should override WebUI's cassandra port from 9160 to 9042.


  For appropriate node at class level:


&gt;	        web:
&gt;	          database:
&gt;	            port: 9042


###  Vasyl Saienko


  Extend contrail fromula


  This patch extends contrail states/modules and client to
  be able of:


  TSN nodes


  Configure TSN nodes


&gt;	        enabled: true
&gt;	        tor:
&gt;	          enabled: true
&gt;	          bind:
&gt;	            port: 8086
&gt;	          agent:
&gt;	            tor01:
&gt;	              id: 0
&gt;	              port: 6632
&gt;	              host: 127.0.0.1
&gt;	              address: 127.0.0.1


&gt;	        physical_router:
&gt;	          router1:
&gt;	            name: router1
&gt;	            dataplane_ip: 1.2.3.4
&gt;	            management_ip: 1.2.3.4
&gt;	            vendor_name: ovs
&gt;	            product_name: ovs
&gt;	            agents:
&gt;	             - tsn0-0
&gt;	             - tsn0


  Enforcing physical/logical interfaces for routers


  opencontrail


&gt;	      physical_router:
&gt;	        router1:


&gt;	            port1:
&gt;	              name: port1
&gt;	              logical_interface:
&gt;	                port1_l:
&gt;	                  name: 'port1.0'
&gt;	                  vlan_tag: 0
&gt;	                  interface_type: L2
&gt;	                  virtual_machine_interface:
&gt;	                    port1_port:
&gt;	                      name: port1_port
&gt;	                      ip_address: 192.168.90.107
&gt;	                      mac_address: '2e:92:a8:af:c2:21'
&gt;	                      security_group: 'default'
&gt;	                      virtual_network: 'virtual-network'


  Look for neighbours, if VM has 2, it's ok


# Formula openldap


###  Filip Pytloun


  Initial commit
  openldap


  Sample pillars


  Client


&gt;	      openldap:
&gt;	        client:
&gt;	          server:
&gt;	            basedn: dc=example,dc=local
&gt;	            host: ldap.example.local
&gt;	            tls: true
&gt;	            port: 389
&gt;	            auth:
&gt;	              user: cn=admin,dc=example,dc=local
&gt;	              password: dummypass
&gt;	          entry:
&gt;	            people:
&gt;	              type: ou
&gt;	              classes:
&gt;	                - top
&gt;	                - organizationalUnit
&gt;	              entry:
&gt;	                jdoe:
&gt;	                  type: cn
&gt;	                  # Change attributes that already exists with different content
&gt;	                  action: replace
&gt;	                  # Delete all other attributes
&gt;	                  purge: true
&gt;	                  attr:
&gt;	                    uid: jdoe
&gt;	                    uidNumber: 20001
&gt;	                    gidNumber: 20001
&gt;	                    gecos: John Doe
&gt;	                    givenName: John
&gt;	                    sn: Doe
&gt;	                    homeDirectory: /home/jdoe
&gt;	                    loginShell: /bin/bash
&gt;	                  classes:
&gt;	                    - posixAccount
&gt;	                    - inetOrgPerson
&gt;	                    - top
&gt;	                    - ldapPublicKey
&gt;	                    - shadowAccount
&gt;	                karel:
&gt;	                  # Simply remove cn=karel
&gt;	                  enabled: false


  Read more


&gt;	  - https://docs.saltstack.com/en/latest/ref/states/all/salt.states.ldap.html#manage-entries-in-an-ldap-database


# Formula openode


###  Filip Pytloun


  Unify Makefile, .gitignore and update readme
  * http://openode.readthedocs.org/


# Formula openssh


###  Dmitry Stremkouski


  Adding option for openssh to disable dns resolvings


&gt;	          use_dns: False


###  Filip Pytloun


  Unify Makefile, .gitignore and update readme


###  Jiri Broulik


  Readme pillar client fix


&gt;	              private_key:
&gt;	                type: rsa
&gt;	                key: ${_param:root_private_key}


###  Marek Celoud


  add option to add DSA keys


  Enable DSA legacy keys:


&gt;	      openssh:
&gt;	        server:
&gt;	          dss_enabled: true


###  Petr Michalec


  Configure Client/ServerAlive* options


  Configure keep alive settings:


&gt;	        client:
&gt;	          alive:
&gt;	            interval: 600
&gt;	            count: 3


&gt;	            keep: yes
&gt;	      #
&gt;	      # will give you an timeout of 30 minutes (600 sec x 3)


###  Tomas Kamm


  fix for backward compatibility for old reclass model definition with new salt 2017xx (#9)


&gt;	                fingerprint_hash_type: sha256|md5


# Formula openvpn


###  Filip Pytloun


  Unify Makefile, .gitignore and update readme


###  Michael Kutý


  Added support for google-authenticator.


  Changed auth.type to auth.engine.


  OpenVPN server with custom auth


&gt;	      openvpn:
&gt;	        server:
  ...


&gt;	          interface:
&gt;	            topology: subnet
&gt;	            network: 10.0.8.0
&gt;	            netmask: 255.255.255.0
&gt;	          auth:
&gt;	            engine: pam/google-authenticator
&gt;	          ssl:
&gt;	            authority: Domain_Service_CA
&gt;	            certificate: server.domain.com


  OpenVPN client auth


&gt;	        client:
&gt;	          enabled: true
&gt;	          tunnel:
&gt;	            tunnel01:
&gt;	              auth:
&gt;	                engine: pam/google-authenticator
&gt;	              ssl:
&gt;	                engine: salt
&gt;	                authority: Domain_Service_CA
&gt;	                certificate: client.domain.com


###  Roald Nefs


  Updated link to Ubuntu guide in README.rst


  Updated link to Ubuntu guide in README.rst to the latest LTS version.
  * https://help.ubuntu.com/lts/serverguide/openvpn.html


# Formula openvstorage


###  Filip Pytloun


  Unify Makefile, .gitignore and update readme


# Formula owncloud


###  Filip Pytloun


  Unify Makefile, .gitignore and update readme


# Formula packer


###  Filip Pytloun


  Unify Makefile, .gitignore and update readme


# Formula panko


# Formula pgbouncer


# Formula php


###  Aleš Komárek


  docfix


  PHP Formula


  PHP is a widely-used general-purpose scripting language that is especially
  suited for Web development and can be embedded into HTML.


  Sample Pillars


&gt;	      php:
&gt;	        environment:
&gt;	          enabled: true
&gt;	          cache:
&gt;	            engine: 'apc'
&gt;	            shm_size: 128
&gt;	            max_file_size: '10M'


  More Information


  * http://www.php.net/manual/en/


# Formula postfix


###  Filip Pytloun


  Unify Makefile, .gitignore and update readme


# Formula postgresql


###  Adam Tengler


  Support for PostgreSQL cluster deployment added


&gt;	                enabled: true
&gt;	                pkgs:
&gt;	                - postgresql-9.1-postgis-2.1


  Cluster


  Basic streaming replication.


  Master node


&gt;	    postgresql:
&gt;	      server:
&gt;	        enabled: true
&gt;	        version: 9.6
&gt;	        bind:
&gt;	          address: 0.0.0.0
&gt;	        database:
&gt;	          mydb: ...
&gt;	      cluster:
&gt;	        role: master
&gt;	        mode: hot_standby
&gt;	        members:
&gt;	        - host: &quot;172.16.10.101&quot;
&gt;	        - host: &quot;172.16.10.102&quot;
&gt;	        - host: &quot;172.16.10.103&quot;
&gt;	        replication_user:
&gt;	          name: repuser
&gt;	          password: password
&gt;	    keepalived:
&gt;	        enabled: True
&gt;	        instance:
&gt;	          VIP:
&gt;	            notify_action:
&gt;	              master:
&gt;	                - 'if [ -f /root/postgresql/flags/failover ]; then touch /var/lib/postgresql/${postgresql:server:version}/main/trigger; fi'
&gt;	              backup:
&gt;	                - 'if [ -f /root/postgresql/flags/failover ]; then service postgresql stop; fi'
&gt;	              fault:


  Slave node


&gt;	        role: slave
&gt;	        master:
&gt;	          host: &quot;172.16.10.100&quot;
&gt;	          port: 5432
&gt;	          user: repuser


  Multi-master cluster with 2ndQuadrant bi-directional replication plugin


&gt;	        version: 9.4
&gt;	          mydb:
&gt;	            extension:
&gt;	              bdr:
&gt;	              btree_gist:
  ...


&gt;	        mode: bdr
&gt;	        local: &quot;172.16.10.101&quot;


&gt;	        local: &quot;172.16.10.102&quot;
&gt;	        master: &quot;172.16.10.101&quot;


  Sample usage


  Read more


###  Ales Komarek


  Fixed readme, allow conditional database names


  PostgreSQL Formula


  PostgreSQL, often simply Postgres, is an object-relational database management
  system available for many platforms including Linux, FreeBSD, Solaris,


  Microsoft Windows and Mac OS X. It is released under the PostgreSQL License,
  which is an MIT-style license, and is thus free and open source software.


  PostgreSQL is developed by the PostgreSQL Global Development Group, consisting
  of a handful of volunteers employed and supervised by companies such as Red


  Hat and EnterpriseDB.


  Single deployment


  Single database server with initial data


&gt;	            users:
&gt;	            - name: 'username'


  Database extensions


  Master-slave cluster


&gt;	      postgresql:
&gt;	        server:
&gt;	          enabled: true
&gt;	          version: 9.6
&gt;	          bind:
&gt;	            address: 0.0.0.0
&gt;	          database:
&gt;	            mydb: ...
&gt;	        cluster:
&gt;	          role: master
&gt;	          mode: hot_standby
&gt;	          members:
&gt;	          - host: &quot;172.16.10.101&quot;
&gt;	          - host: &quot;172.16.10.102&quot;
&gt;	          - host: &quot;172.16.10.103&quot;
&gt;	          replication_user:
&gt;	            name: repuser
&gt;	            password: password
&gt;	      keepalived:
&gt;	          enabled: True
&gt;	          instance:
&gt;	            VIP:
&gt;	              notify_action:
&gt;	                master:
&gt;	                  - 'if [ -f /root/postgresql/flags/failover ]; then touch /var/lib/postgresql/${postgresql:server:version}/main/trigger; fi'
&gt;	                backup:
&gt;	                  - 'if [ -f /root/postgresql/flags/failover ]; then service postgresql stop; fi'
&gt;	                fault:


  Slave nodes


&gt;	          role: slave
&gt;	          master:
&gt;	            host: &quot;172.16.10.100&quot;
&gt;	            port: 5432
&gt;	            user: repuser


  Multi-master cluster


&gt;	          version: 9.4
&gt;	            mydb:
&gt;	              extension:
&gt;	                bdr:
&gt;	                  enabled: true
&gt;	                btree_gist:
&gt;	          mode: bdr
&gt;	          local: &quot;172.16.10.101&quot;
&gt;	          local: &quot;172.16.10.102&quot;
&gt;	          master: &quot;172.16.10.101&quot;


  More information


###  Filip Pytloun


  Fix database pillar in readme and make db users optional


&gt;	    Closes: #10
&gt;	            databasename:
&gt;	              encoding: 'UTF8'
&gt;	              locale: 'cs_CZ'
&gt;	              users:
&gt;	                - name: 'username'
&gt;	                  password: 'password'
&gt;	                  host: 'localhost'
&gt;	                  rights: 'all privileges'
&gt;	              initial_data:
&gt;	                engine: backupninja
&gt;	                source: backup.host
&gt;	                host: original-host-name
&gt;	                database: original-database-name
&gt;	              - name: 'username'
&gt;	                password: 'password'
&gt;	                host: 'localhost'
&gt;	                rights: 'all privileges'
&gt;	                  createdb: true
&gt;	                postgis_topology:
&gt;	                fuzzystrmatch:
&gt;	                postgis_tiger_geocoder:
&gt;	                postgis:
&gt;	                  pkgs:
&gt;	                  - postgresql-9.1-postgis-2.1


  Add client role


  Client


&gt;	        client:
&gt;	          server:
&gt;	            server01:
&gt;	              admin:
&gt;	                host: database.host
&gt;	                port: 5432
&gt;	                user: root
&gt;	                password: password
&gt;	              database:
&gt;	                mydb:
&gt;	                  encoding: 'UTF8'
&gt;	                  locale: 'en_US'
&gt;	                  users:
&gt;	                  - name: test
&gt;	                    password: test
&gt;	                    host: localhost
&gt;	                    createdb: true
&gt;	                    rights: all privileges


  Unify Makefile, .gitignore and update readme
  * https://gist.github.com/ibussieres/11262268 - upgrade instructions for ubuntu


###  Volodymyr Stoiko


  Add support for simple queries


&gt;	                  init:
&gt;	                    maintenance_db: mydb
&gt;	                    queries:
&gt;	                    - INSERT INTO login VALUES (11, 1) ;
&gt;	                    - INSERT INTO device VALUES (1, 11, 42);


# Formula powerdns


###  Ivan Suzdal


  Add api/api_key parameters


  Also add axfr_ips,version_string and webserver*
  as configurable parameters.


  Related PROD: 11411


&gt;	        axfr_ips:
&gt;	          - 10.11.0.0/16
&gt;	          - 127.0.0.1
&gt;	        api:
&gt;	          enabled: true
&gt;	          key: SecurePass
&gt;	        webserver:
&gt;	          password: SuperSecurePass
&gt;	          address: 0.0.0.0
&gt;	          port: 8081


  Huge changes in powerdns


  This split simplifies process of adding new types of backends.


  To add new backend type we just need describe required steps in
  particular state file, add backend config and specify the engine
  in powerdns:server:backend pillar.


&gt;	    powedns:
&gt;	      server:
&gt;	        enabled: true
&gt;	        backend:
&gt;	          engine: mysql
&gt;	          host: localhost
&gt;	          port: 3306
&gt;	          name: pdns
&gt;	          user: pdns
&gt;	          password: password
&gt;	        bind:
&gt;	          port: 53


  PowerDNS server with sqlite backend


&gt;	    powerdns:
&gt;	          engine: sqlite
&gt;	          dbname: pdns.sqlite
&gt;	          dbpath: /var/lib/powerdns
&gt;	          address: 127.0.0.1
&gt;	          port: 55
&gt;	        default-soa-name: ns1.domain.tld
&gt;	        soa-minimum-ttl: 3600


# Formula pritunl


###  Filip Pytloun


  Unify Makefile, .gitignore and update readme


# Formula prometheus


###  Bartosz Kupidura


  Add ssl options for etcd


&gt;	            cert_name: prometheus-server.crt
&gt;	            key_name: prometheus-server.key
&gt;	            scheme: https
&gt;	            ssl_dir: /opt/prometheus/config
&gt;	            member:
&gt;	              - host: ${_param:cluster_node01_address}
&gt;	                port: ${_param:cluster_node01_port}
&gt;	              - host: ${_param:cluster_node02_address}
&gt;	                port: ${_param:cluster_node02_port}
&gt;	              - host: ${_param:cluster_node03_address}
&gt;	                port: ${_param:cluster_node03_port}


  Get dns autodiscovery from reclass


&gt;	          dns:
&gt;	            - name: 'pushgateway'
&gt;	              domain:
&gt;	              - 'tasks.prometheus_pushgateway'
&gt;	              type: A
&gt;	              port: 9091
&gt;	            - name: 'prometheus'
&gt;	              - 'tasks.prometheus_server'
&gt;	              port: 9090
&gt;	            api_ip: ${_param:kubernetes_control_address}
&gt;	          etcd:
&gt;	            - host: ${_param:cluster_node01_address}
&gt;	              port: ${_param:cluster_node01_port}
&gt;	            - host: ${_param:cluster_node02_address}
&gt;	              port: ${_param:cluster_node02_port}
&gt;	            - host: ${_param:cluster_node03_address}
&gt;	              port: ${_param:cluster_node03_port}


  Better grain handling for telegraf


  Replace '-' to '_' in prometheus config dir


  Add possibility to add recording rules


&gt;	          config: /srv/volumes/prometheus
&gt;	        recording:
&gt;	          - name: 'instance:fd_utilization'
&gt;	            query: &gt;-
  process_open_fds / process_max_fds


  Add prometheus, alertmanager, pushgateway configs


  Sample pillars


  Configure prometheus server


&gt;	    prometheus:
&gt;	      server:
&gt;	        enabled: true
&gt;	        dir:
&gt;	          config: /srv/volumes/prometheus-config
&gt;	          config_in_container: /opt/prometheus/config
&gt;	        bind:
&gt;	          port: 9090
&gt;	          address: 0.0.0.0
&gt;	        external_port: 15010
&gt;	        target:
&gt;	          kubernetes:
&gt;	            api_ip: 127.0.0.1
&gt;	            cert_name: kubelet-client.crt
&gt;	            key_name: kubelet-client.key
&gt;	          etcd: ${etcd:server:members}
&gt;	        alert:
&gt;	          PrometheusTargetDown:
&gt;	            if: 'up != 1'
&gt;	            labels:
&gt;	              severity: down
&gt;	            annotations:
&gt;	              summary: 'Prometheus target down'
&gt;	        storage:
&gt;	          local:
&gt;	            engine: &quot;persisted&quot;
&gt;	            retention: &quot;360h&quot;
&gt;	            memory_chunks: 1048576
&gt;	            max_chunks_to_persist: 524288
&gt;	            num_fingerprint_mutexes: 4096
&gt;	        alertmanager:
&gt;	          notification_queue_capacity: 10000
&gt;	        config:
&gt;	          global:
&gt;	            scrape_interval: &quot;15s&quot;
&gt;	            scrape_timeout: &quot;15s&quot;
&gt;	            evaluation_interval: &quot;1m&quot;
&gt;	            external_labels:
&gt;	              region: 'region1'


  Configure alertmanager


&gt;	      alertmanager:
&gt;	          port: 9093
&gt;	        external_port: 15011
&gt;	            resolve_timeout: 5m
&gt;	          route:
&gt;	            group_by: ['alertname', 'region', 'service']
&gt;	            group_wait: 60s
&gt;	            group_interval: 5m
&gt;	            repeat_interval: 3h
&gt;	            receiver: HTTP-notification
&gt;	          inhibit_rules:
&gt;	            - source_match:
&gt;	                severity: 'down'
&gt;	              target_match:
&gt;	                severity: 'critical'
&gt;	              equal: ['region', 'service']
&gt;	                severity: 'warning'
&gt;	              equal: ['alertname', 'region', 'service']
&gt;	          receivers:
&gt;	            - name: 'HTTP-notification'
&gt;	              webhook_configs:
&gt;	                - url: http://127.0.0.1
&gt;	                  send_resolved: true


  Configure pushgateway


&gt;	      pushgateway:
&gt;	        external_port: 15012


###  Filip Pytloun


  Initial commit


  Salt Prometheus formula


  Power your metrics and alerting with a leading open-source monitoring
  solution.


###  Konstantin Hontar


  Add Slack and mail receiver to AlertManager config


&gt;	            - name: 'HTTP-slack'
&gt;	              slack_configs:
&gt;	                - api_url: http://127.0.0.1/slack
&gt;	            - name: 'smtp'
&gt;	              email_configs:
&gt;	                - to: test@example.com
&gt;	                  from: test@example.com
&gt;	                  smarthost: example.com
&gt;	                  auth_username: username
&gt;	                  auth_password: password


###  Martin Polreich


  Added Kitchen tests and Travis


&gt;	            enabled: true
&gt;	            endpoint:
&gt;	              - name: 'pushgateway'
&gt;	                domain:
&gt;	                - 'tasks.prometheus_pushgateway'
&gt;	                type: A
&gt;	                port: 9091
&gt;	              - name: 'prometheus'
&gt;	                - 'tasks.prometheus_server'
&gt;	                port: 9090
&gt;	              scheme: https
&gt;	              ssl_dir: /opt/prometheus/config
&gt;	              cert_name: prometheus-server.crt
&gt;	              key_name: prometheus-server.key
&gt;	              member:
&gt;	                - host: ${_param:cluster_node01_address}
&gt;	                  port: ${_param:cluster_node01_port}
&gt;	                - host: ${_param:cluster_node02_address}
&gt;	                  port: ${_param:cluster_node02_port}
&gt;	                - host: ${_param:cluster_node03_address}
&gt;	                  port: ${_param:cluster_node03_port}
&gt;	          instance:fd_utilization:


###  Ondrej Smola


  iadded posibility to define two endpoint in one receiver + test pillars + update Readme


  This option is usefull when all alarms should be send to two or more receivers simultaneously - without need to use continue parameter


&gt;	            group_by: ['region', 'service']
&gt;	            receiver: default
&gt;	          inhibit_rule:
&gt;	            InhibitCriticalWhenDown:
&gt;	              enabled: true
&gt;	              source_match:
&gt;	            InhibitWarningWhenDown:
&gt;	            InhibitWarningWhenCritical:
&gt;	          receiver:
&gt;	            HTTP-notification:
&gt;	                localhost:
&gt;	                  url: http://127.0.0.1
&gt;	            HTTP-slack:
&gt;	                slack:
&gt;	                  api_url: http://127.0.0.1/slack
&gt;	            smtp:
&gt;	                email:
&gt;	                  to: test@example.com
&gt;	            #Two endpoints in one receiver
&gt;	            Multi-receiver:
&gt;	                webhook:


# Formula pypiserver


# Formula python


###  Ales Komarek


  Fixed readme


  Python formula
  service.environment.environment:


  Basic Python environment
  service.environment.development:


  Python development environment
  python.environment.django:


  Python Django environment


###  Filip Pytloun


  Unify Makefile, .gitignore and update readme


###  Richard Felkl


  changed local mirror url handling


&gt;	    Related: PROD-15581
&gt;	                protocol: http
&gt;	                host: pypi.local


  fixed bug with empty user list


&gt;	            root:


  added support for local pypi mirrors


  Using offline mirrors


&gt;	      python:
&gt;	        environment:
&gt;	          enabled: true
&gt;	          user:
&gt;	          - root:
&gt;	              name: root
&gt;	              pypi_user: user
&gt;	              pypi_password: password
&gt;	              pypi_mirror:
&gt;	                host: http://pypi.local
&gt;	                port: 8084
&gt;	                upstream_fallback: true
&gt;	                user: user
&gt;	                password: password


# Formula rabbitmq


###  Filip Pytloun


  Unify Makefile, .gitignore and update readme


###  Kirill Bespalov


  Allow to use two ways of certs/keys files management:
  1) via specifing a path of a file:
  2) via specifing a path and content of a file:


  The files management via authority must be removed as an unused.


  To enable support of TLS for rabbitmq-server you need to provide a path to cacert, server cert and private key :


&gt;	            key_file: /etc/rabbitmq/ssl/key.pem
&gt;	            cert_file: /etc/rabbitmq/ssl/cert.pem
&gt;	            ca_file: /etc/rabbitmq/ssl/ca.pem


  To manage content of these files you can either use the following options:


&gt;	     rabbitmq:
&gt;	        server:
&gt;	          enabled: true
  ...


&gt;	          ssl:
&gt;	            enabled: True
&gt;	            cacert_chain: |


  Or you can use the `salt.minion.cert` salt state which
  creates all required files according to defined reclass model [1]. In this case you need just to enable ssl and nothing more:


  1. https://github.com/Mirantis/reclass-system-salt-model/tree/master/salt/minion/cert/rabbitmq


  TLS support for RabbitMQ


&gt;	              pattern: '^(?!amq\.).*'


  Enable TLS support


  The certs and private key passing:


&gt;	            key: |


&gt;	            cert: |


  Also you can pass them via specifing a name of ca authority at salt master:


&gt;	            authority: CA_Authority_Name


  In this case keys and certs will be pulled from:


  `salt://pki/{{ authority }}/certs/{ rabbitmq.{cert|key} | ca.cert }`


&gt;	  --


  Defaut port for TLS is **5671**:


&gt;	    rabbitmq:
&gt;	      server:
&gt;	        bind:
&gt;	           port: 5671


  [{nodes,[{disc,['rabbit@ctl-1','rabbit@ctl-2','rabbit@ctl-3']}]},


###  Nick Metz


  Fix for #42, samples for host/policy configuration wrong


&gt;	          host:
  '/monitor':


&gt;	              enabled: true
&gt;	              user: 'monitor'
&gt;	              password: 'password'


&gt;	              policies:
&gt;	              - name: HA
&gt;	                pattern: '^(?!amq\.).*'
&gt;	                definition: '{&quot;ha-mode&quot;: &quot;all&quot;}'


# Formula rally


###  Filip Pytloun


  Unify Makefile, .gitignore and update readme
  * https://trello.com/b/DoD8aeZy/rally


###  Jiri Broulik


  parametrize endpoints


&gt;	                endpoint_type: internal


# Formula reclass


###  Ales Komarek


  Interpolation in node cycling generation


  Reclass model with multiple node defined and interpolation enabled


&gt;	     :language: yaml


  Node classification/declassification - reactors and orchestration pipelines


  Classify node after creation and unclassify on node deletion


&gt;	      salt:
&gt;	        master:
&gt;	          reactor:
  reclass/minion/classify:


&gt;	            - salt://reclass/reactor/node_register.sls
  reclass/minion/declassify:


&gt;	            - salt://reclass/reactor/node_unregister.sls


  Event to trigger the node classification


  salt-call event.send 'reclass/minion/classify' &quot;{'node_master_ip': '$config_host', 'node_ip': '${node_ip}', 'node_domain': '$node_domain', 'node_cluster': '$node_cluster', 'node_hostname': '$node_hostname', 'node_os': '$node_os'}&quot;


  You can send any parameters in the event payload, all will be checked
  against dynamic node classification conditions.


  Both actions will use the minion ID as the node_name to be updated.


  Event to trigger the node declassification


  salt-call event.send 'reclass/minion/declassify'


  Documentation fixes


  Reclass storage with archive data source


  Reclass storage with archive data source with content hash check


  Sample class mapping for dynamic classification


  Reclass Formula


  Sample Metadata


  Reclass storage with simple class mappings


  Reclass models with dynamic node classification


  More Information


  doc fix
  reclass is an “external node classifier” (ENC) as can be used with automation
  tools, such as Puppet, Salt, and Ansible. It is also a stand-alone tool for
  merging data sources recursively.


###  Aleš Komárek


  Update README.rst
  reclass formula


  External links


###  Filip Pytloun


  Unify Makefile, .gitignore and update readme


# Formula redis


###  Adam Tengler


  Readme updated


  Redis cluster master


&gt;	      redis:
&gt;	        cluster:
&gt;	          enabled: True
&gt;	          master:
&gt;	            host: 192.168.1.100
&gt;	            port: 6379
&gt;	          mode: sentinel
&gt;	          quorum: 2
&gt;	          role: master
&gt;	      supervisor:
&gt;	        server:
&gt;	          service:
&gt;	            redis_sentinel:
&gt;	              name: sentinel
&gt;	              type: redis


  Redis cluster slave


&gt;	          role: slave


###  Ales Komarek


  Docfix


  Redis formula


  key value storage


  Sample pillars


  Redis localhost server


&gt;	      redis:
&gt;	        server:
&gt;	          enabled: true
&gt;	          bind:
&gt;	            address: 127.0.0.1 
&gt;	            port: 6379
&gt;	            protocol: tcp


  Redis world open


&gt;	            address: 0.0.0.0 


  Redis modes


&gt;	          appendfsync: no | everysec | always


  Redis cluster master


&gt;	        cluster:
&gt;	          enabled: True
&gt;	          master:
&gt;	            host: 192.168.1.100
&gt;	          mode: sentinel
&gt;	          quorum: 2
&gt;	          role: master
&gt;	      supervisor:
&gt;	          service:
&gt;	            redis_sentinel:
&gt;	              name: sentinel
&gt;	              type: redis


  Redis cluster slave


&gt;	          role: slave


  Command usage


  Removes data from your connection's CURRENT database.


&gt;	     redis-cli FLUSHDB


  Removes data from ALL databases.


&gt;	     redis-cli FLUSHALL


  More information


  * http://redis.io/topics/admin
  * http://redis.io/topics/quickstart
  * http://redis.io/topics/persistence


###  Filip Pytloun


  Unify Makefile, .gitignore and update readme


# Formula redmine


###  Aleš Komárek


  renamed file


  Redmine Formula


  Redmine is a flexible project management web application. Written using the Ruby on Rails framework, it is cross-platform and cross-database.


  Sample pillars


&gt;	      redmine:
&gt;	        server:
&gt;	          enabled: true
&gt;	          version: '2.3'
&gt;	          apps:
&gt;	          - name: majklk
&gt;	            database:
&gt;	              engine: postgresql
&gt;	              host: 127.0.0.1
&gt;	              name: db_name
&gt;	              password: pass
&gt;	              user: user_name
&gt;	            mail:
&gt;	              host: host-mail
&gt;	              user: email
&gt;	              domain: domain


  More Information


  * http://www.redmine.org/
  * http://www.redmine.org/projects/redmine/wiki/RedmineInstall


# Formula robophery


###  Ales Komarek


  Something to test


  RoboPhery service with MQTT backend


&gt;	        server:
&gt;	          module_default:
&gt;	            input_backends:
&gt;	            - messages
&gt;	            output_backends:
&gt;	          backend:


  RoboPhery service with StatsD backend


&gt;	      robophery:
&gt;	          enabled: true
&gt;	            - metrics


  RoboPhery service with filesystem backend


&gt;	            - files
&gt;	              engine: filesystem


  GPIO relay module


&gt;	          gpio_enabled: True


  I2C HTU21 module


&gt;	          module:
&gt;	              read_interval: 2000


  BLE Parrot Flower Power module


&gt;	          ble_enabled: True
&gt;	            livingroom01-flower:
&gt;	              engine: ble.flower_power
&gt;	              addr: 00:11:22:33:44:55:66


  Ininial commit


  RoboPhery formula


  Python library for interfacing low level hardware sensors and actuators with MQTT bindings.


  Sample pillars


  Single robophery service


&gt;	        service:
&gt;	          platform: rpi/bbb/micro
&gt;	          debug: False
&gt;	          output_interval: 60
&gt;	          output_backends:
&gt;	          - messages
&gt;	          - metrics
&gt;	          - files
&gt;	          output:
&gt;	            messages:
&gt;	              engine: mqtt
&gt;	              host: 127.0.0.1
&gt;	              port: 1100
&gt;	            metrics:
&gt;	              engine: statsd
&gt;	            files:
&gt;	              path: /log
&gt;	            light01:
&gt;	              engine: gpio.relay
&gt;	              port: 22
&gt;	            livingroom01-env:
&gt;	              engine: i2c.htu21
&gt;	              bus: 1
&gt;	              read_interval: 1


# Formula roundcube


###  Filip Pytloun


  Unify Makefile, .gitignore and update readme


# Formula rsync


###  Filip Pytloun


  Unify Makefile, .gitignore and update readme


###  Petr Michalec


  allow spec. timeout


&gt;	          timeout: 300


# Formula rsyslog


###  Filip Pytloun


  Unify Makefile, .gitignore and update readme


###  Swann Croiset


  Add syslog template per output file


  Custom templates


  It is possible to define a specific syslog template per output file instead of
  using the default one.


&gt;	      rsyslog:
&gt;	          output:
&gt;	            file:
  /var/log/your-app.log:


&gt;	                template: &quot;&quot;%syslogtag:1:32%%msg:::sp-if-no-1st-sp%%msg%\\n&quot;&quot;
&gt;	                filter: &quot;if $programname startswith 'your-app' then&quot;


  Add option to skip the log_collector configuration


  Support metadata


  If the *heka* support metadata is enabled, all output files are automatically
  parsed by the **log_collector** service.


  To skip the log_collector configuration, set the **skip_log_collector** to true.


&gt;	                skip_log_collector: true


  Add stop processing option


  This allows to use dedicated log files for some services. This is
  especially useful when journald is around and forward logs to syslog.


&gt;	             -/var/log/your-app.log:
&gt;	                owner: syslog
&gt;	                group: adm
&gt;	                createmode: 0640
&gt;	                umask: 0022
&gt;	                stop_processing: true


###  teoyaomiqui


  Adding rsyslog remote server functionality (#6)


  Remote rsyslog server


  It is possible to have rsyslog act as remote server, collecting, storing or forwarding logs.


  This functionality is provided via rsyslog input/output modules, rulesets and templates.


&gt;	        server:
&gt;	          enabled: true
&gt;	          module:
&gt;	            imudp: {}
&gt;	          template:
&gt;	            RemoteFilePath:
&gt;	              parameter:
&gt;	                type: string
&gt;	                string: /var/log/%HOSTNAME%/%programname%.log
&gt;	          ruleset:
&gt;	            remote10514:
&gt;	              description: action(type=&quot;omfile&quot; dynaFile=&quot;RemoteFilePath&quot;)
&gt;	          input:
&gt;	            imudp:
&gt;	              port: 10514
&gt;	              ruleset: remote10514


# Formula ruby


###  Filip Pytloun


  Unify Makefile, .gitignore and update readme


# Formula rundeck


###  Ilya Kharin


  Add ability to configure secret keys


  Configure Secret Keys
  ^^^^^^^^^^^^^^^^^^^^^


  It is possible to configure secret items in Key Storage in Rundeck:


&gt;	       rundeck:
&gt;	         client:
&gt;	           enabled: true
&gt;	           secret:
  openstack/username:


&gt;	               type: password
&gt;	               content: admin
  openstack/password:


&gt;	               content: password
  openstack/keypair/private:


&gt;	               type: private
&gt;	               content: &lt;private&gt;
  openstack/keypair/public:


&gt;	               type: public
&gt;	               content: &lt;public&gt;


  It is possible to disable keys to be sure that they are not present in Rundeck:


&gt;	      rundeck:


&gt;	               enabled: false


  Add tests for the formula


  Sample pillars


  Configure Server
</code></pre>
</blockquote>

<p>Rundeck is suppose to be configure for running in Docker Swarm and the server
  state prepares only configuration files, including binding parameters, system
  user, Rundeck users and API tokens:</p>

<blockquote>
<pre><code>      server:
        enabled: true
        user:
          uid: 550
          gid: 550
        api:
          host: 10.20.0.2
          port: 4440
          https: false
        ssh:
          user: runbook
          private_key: &lt;private&gt;
          public_key: &lt;public&gt;


        users:
          admin:
            name: admin
            password: password
            roles:
              - user
              - admin
              - architect
              - deploy
              - build
          john:
            name: John
            password: johnspassword
          kate:
            name: Kate
            password: katespassword


        tokens:
          admin: EcK8zhQw
</code></pre>
</blockquote>

<p>To configure Rundeck to use PostgreSQL instead of H2:</p>

<blockquote>
<pre><code>        datasource:
          engine: postgresql
          port: 5432
          username: ${_param:rundeck_postgresql_username}
          password: ${_param:rundeck_postgresql_password}
          database: ${_param:rundeck_postgresql_database}
</code></pre>
</blockquote>

<p>Configure Client</p>

<p>Configure Projects
  ^^^^^^^^^^^^^^^^^^</p>

<p>Projects can be configured with a set of nodes which are available to run jobs
  within them. Rundeck uses <code>rundeck:server:ssh</code> credentials to access nodes.</p>

<p>Jobs can be configured from a separate GIT repository using the SCM Import
  plugin.</p>

<blockquote>
<pre><code>      client:
        project:
          project0:
            description: project
            node:
              node01:
                nodename: node01
                hostname: node01.cluster.local
                username: runbook
                tags: [ubuntu, docker]
              node02:
                nodename: node02
                hostname: node02.cluster.local
                tags: [centos, docker]
            plugin:
              import:
                address: http://gerrit.cluster.local/jobs/rundeck-jobs.git
                branch: master
</code></pre>
</blockquote>

<h1 id="formula-sahara">Formula sahara</h1>

<h3 id="aleš-komárek-13">Aleš Komárek<a href="#aleš-komárek-13" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Update README.rst</p>

<p>Sahara formula</p>

<p>Sahara fomula</p>

<p>The Sahara project provides a simple means to provision a data-intensive application cluster (Hadoop or Spark) on top of OpenStack.</p>

<p>Sample pillars</p>

<p>External links</p>

<h3 id="filip-pytloun-61">Filip Pytloun<a href="#filip-pytloun-61" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Unify Makefile, .gitignore and update readme</p>

<h1 id="formula-salt">Formula salt</h1>

<h3 id="adam-tengler-3">Adam Tengler<a href="#adam-tengler-3" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Salt reactor features refactoring
  salt-call event.send &lsquo;salt/key/create&rsquo; <br />
&gt;        &ldquo;{&lsquo;node_id&rsquo;: &lsquo;id-of-minion&rsquo;, &lsquo;node_host&rsquo;: &lsquo;172.16.10.100&rsquo;, &lsquo;orch_post_create&rsquo;: &lsquo;kubernetes.orchestrate.compute_install&rsquo;, &lsquo;post_create_pillar&rsquo;: {&lsquo;node_name&rsquo;: &lsquo;id-of-minion&rsquo;}}&rdquo;</p>

<h3 id="ales-komarek-19">Ales Komarek<a href="#ales-komarek-19" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Add support for salt master engines</p>

<p>Salt engine definition for saltgraph metadata collector</p>

<blockquote>
<pre><code>    salt:
      master:
        engine:
          graph_metadata:
            engine: saltgraph
            host: 127.0.0.1
            port: 5432
            user: salt
            password: salt
            database: salt
</code></pre>
</blockquote>

<p>Salt engine definition for sending events from docker events</p>

<blockquote>
<pre><code>          docker_events:
            docker_url: unix://var/run/docker.sock
</code></pre>
</blockquote>

<p>Added support for sync modules and pillar after minion start</p>

<p>Synchronise modules and pillars on minion start.</p>

<blockquote>
<pre><code>        reactor:
</code></pre>

<p>&lsquo;salt/minion/*/start&rsquo;:</p>

<pre><code>          - salt://salt/reactor/minion_start.sls
</code></pre>
</blockquote>

<p>Minion key create/delete - reactors and orchestration pipelines</p>

<p>Run any defined orchestration pipeline</p>

<p>Add and/or remove the minion key
  salt/key/create:</p>

<blockquote>
<pre><code>          - salt://salt/reactor/key_create.sls
</code></pre>

<p>salt/key/remove:</p>

<pre><code>          - salt://salt/reactor/key_remove.sls
</code></pre>
</blockquote>

<p>Event to trigger the key creation
  salt-call event.send &lsquo;salt/key/create&rsquo; &ldquo;{&lsquo;node_name&rsquo;: &lsquo;id-of-minion&rsquo;, &lsquo;orch_post_create&rsquo;: &lsquo;kubernetes/orchestrate/compute_install.sls&rsquo;}&rdquo;</p>

<p>You can add pass additional <code>orch_pre_create</code>, <code>orch_post_create</code>,
  <code>orch_pre_remove</code> or <code>orch_post_remove</code> parameters to the event to call
  extra orchestrate files. This can be useful for example for
  registering/unregistering nodes from the monitoring alarms or dashboards.</p>

<p>The key creation event needs to be run from other machine than the one
  being registered.</p>

<p>Event to trigger the key removal</p>

<p>salt-call event.send &lsquo;salt/key/remove&rsquo;</p>

<p>Orchestrates and reactors</p>

<p>Salt synchronise node pillar and modules after start</p>

<blockquote>
<pre><code>          - salt://salt/reactor/node_start.sls
</code></pre>
</blockquote>

<p>Trigger basic node install</p>

<p>salt/minion/install:</p>

<blockquote>
<pre><code>          - salt://salt/reactor/node_install.sls
</code></pre>
</blockquote>

<p>Sample event to trigger the node installation</p>

<p>salt-call event.send &lsquo;salt/minion/install&rsquo;</p>

<p>Run any orchestration pipeline</p>

<p>Event to trigger the orchestration pipeline
  salt-call event.send &lsquo;salt/orchestrate/start&rsquo; &ldquo;{&lsquo;orchestrate&rsquo;: &lsquo;salt/orchestrate/infra_install.sls&rsquo;}&rdquo;</p>

<p>Classify node after start</p>

<p>Event to trigger the node classification</p>

<p>Salt orchestrate pipeline stars based on reactor triggers</p>

<p>Salt Reactor system sample</p>

<p>Run any orchestration pipeline from custom event</p>

<p>salt/orchestrate/start:</p>

<blockquote>
<pre><code>          - salt://salt/reactor/orchestrate_start.sls
</code></pre>
</blockquote>

<p>Sample event to trigger the basic orchestration pipeline</p>

<p>salt-call event.send &lsquo;salt/orchestrate/start&rsquo; &ldquo;{&lsquo;orchestrate&rsquo;: &lsquo;salt/orchestrate/infra_install.sls&rsquo;}</p>

<p>Classify node from custom event</p>

<p>reclass/minion/classify:</p>

<blockquote>
<pre><code>          - salt://reclass/reactor/node_register.sls
</code></pre>
</blockquote>

<p>Sample event to trigger the classification</p>

<p>salt-call event.send &lsquo;reclass/minion/classify&rsquo; &ldquo;{&lsquo;node_master_ip&rsquo;: &lsquo;$config_host&rsquo;, &lsquo;node_ip&rsquo;: &lsquo;${node_ip}&lsquo;, &lsquo;node_domain&rsquo;: &lsquo;$node_domain&rsquo;, &lsquo;node_cluster&rsquo;: &lsquo;$node_cluster&rsquo;, &lsquo;node_hostname&rsquo;: &lsquo;$node_hostname&rsquo;, &lsquo;node_os&rsquo;: &lsquo;$node_os&rsquo;}&rdquo;</p>

<p>Reactor system, documentation fixes</p>

<p>Salt Formula</p>

<p>Salt is a new approach to infrastructure management. Easy enough to get
  running in minutes, scalable enough to manage tens of thousands of servers,
  and fast enough to communicate with them in seconds.</p>

<p>Salt delivers a dynamic communication bus for infrastructures that can be used
  for orchestration, remote execution, configuration management and much more.</p>

<p>Sample Metadata</p>

<p>Salt master with base formulas and pillar metadata backend</p>

<p>Salt master with reclass ENC metadata backend</p>

<p>Salt master with pip based installation (optional)</p>

<p>Install formula through system package management</p>

<p>Salt master with logging handlers</p>

<p>Salt master peer setup for remote certificate signing</p>

<p>Salt Reactor system configuration</p>

<p>salt/minion/*/start:</p>

<blockquote>
<pre><code>          - salt://reactor/minion-started.sls
</code></pre>
</blockquote>

<p>Salt syndic</p>

<p>The master of masters</p>

<blockquote>
<pre><code>        enabled: true
        order_masters: True
</code></pre>
</blockquote>

<p>Lower syndicated master</p>

<blockquote>
<pre><code>      syndic:
        master:
          host: master-of-master-host
        timeout: 5
</code></pre>
</blockquote>

<p>Syndicated master with multiple master of masters</p>

<blockquote>
<pre><code>        masters:
        - host: master-of-master-host1
        - host: master-of-master-host2
</code></pre>
</blockquote>

<p>Salt-minion proxy</p>

<p>Salt minion behind HTTP proxy</p>

<p>Salt minion with PKI certificate authority (CA)</p>

<p>Salt minion using PKI certificate</p>

<p>More Information</p>

<p>Support of multi-master-of-masters</p>

<p>Salt syndic: Lower master with multi-master of masters</p>

<p>Cleaned up salt-syndic features</p>

<p>Salt syndic: Master of masters</p>

<p>Salt syndic: Lower master</p>

<h3 id="filip-pytloun-62">Filip Pytloun<a href="#filip-pytloun-62" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Unify Makefile, .gitignore and update readme</p>

<h3 id="jiri-broulik-7">Jiri Broulik<a href="#jiri-broulik-7" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>proxy_minion fix</p>

<blockquote>
<pre><code>        proxy_minion:
</code></pre>
</blockquote>

<p>salt-proxy</p>

<p>Salt proxy</p>

<p>Salt proxy pillar</p>

<blockquote>
<pre><code>      minion:
        proxy:
          master: localhost
          device:
</code></pre>

<p>vsrx01.mydomain.local:</p>

<pre><code>              enabled: true
              engine: napalm
</code></pre>

<p>csr1000v.mydomain.local:</p>
</blockquote>

<p>Proxy pillar for IOS device</p>

<blockquote>
<pre><code>    proxy:
      proxytype: napalm
      driver: ios
      host: csr1000v.mydomain.local
      username: root
      passwd: r00tme
</code></pre>
</blockquote>

<p>Proxy pillar for JunOS device</p>

<blockquote>
<pre><code>      driver: junos
      host: vsrx01.mydomain.local
      optional_args:
        config_format: set
</code></pre>
</blockquote>

<h3 id="ondrej-smola-7">Ondrej Smola<a href="#ondrej-smola-7" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>added readme for salt minion http proxy</p>

<p>Salt minion behind http proxy</p>

<blockquote>
<pre><code>          host: 127.0.0.1
          port: 3128
</code></pre>
</blockquote>

<h3 id="petr-michalec-11">Petr Michalec<a href="#petr-michalec-11" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>allow-multiple-ext-pillars-and-reclass-options</p>

<p>Salt master with multiple ext_pillars</p>

<blockquote>
<pre><code>   :language: yaml
</code></pre>
</blockquote>

<p>Encrypted pillars</p>

<blockquote>
<pre><code>Note: NACL + below configuration will be available in Salt &gt; 2017.7.
</code></pre>
</blockquote>

<p>External resources:</p>

<blockquote>
<pre><code>- Tutorial to configure salt + reclass ext_pillar and nacl: http://apealive.net/post/2017-09-salt-nacl-ext-pillar/
- Saltstack documentation: https://docs.saltstack.com/en/latest/ref/modules/all/salt.modules.nacl.html
</code></pre>
</blockquote>

<p>Configure salt NACL module:</p>

<blockquote>
<pre><code>  pip install --upgrade libnacl===1.5.2
  salt-call --local nacl.keygen /etc/salt/pki/master/nacl


    local:
</code></pre>

<p>saved sk_file:/etc/salt/pki/master/nacl  pk_file: /etc/salt/pki/master/nacl.pub</p>

<pre><code>        pillar:
          reclass: *reclass
          nacl:
            index: 99
        nacl:
          box_type: sealedbox
          sk_file: /etc/salt/pki/master/nacl
          pk_file: /etc/salt/pki/master/nacl.pub
          #sk: None
          #pk: None
</code></pre>
</blockquote>

<p>NACL encrypt secrets:</p>

<blockquote>
<pre><code>  salt-call --local nacl.enc 'my_secret_value' pk_file=/etc/salt/pki/master/nacl.pub
</code></pre>

<p>hXTkJpC1hcKMS7yZVGESutWrkvzusXfETXkacSklIxYjfWDlMJmR37MlmthdIgjXpg4f2AlBKb8tc9Woma7q</p>

<pre><code>  # or
</code></pre>

<p>salt-run nacl.enc &lsquo;myotherpass&rsquo;</p>
</blockquote>

<p>ADDFD0Rav6p6+63sojl7Htfrncp5rrDVyeE4BSPO7ipq8fZuLDIVAzQLf4PCbDqi+Fau5KD3/J/E+Pw=</p>

<p>NACL encrypted values on pillar:</p>

<p>Use Boxed syntax <code>NACL[CryptedValue=]</code> to encode value on pillar:</p>

<blockquote>
<pre><code>  my_pillar:
    my_nacl:
        key0: unencrypted_value
        key1: NACL[hXTkJpC1hcKMS7yZVGESutWrkvzusXfETXkacSklIxYjfWDlMJmR37MlmthdIgjXpg4f2AlBKb8tc9Woma7q]
</code></pre>
</blockquote>

<p>NACL large files:</p>

<p>salt-call nacl.enc_file /tmp/cert.crt out=/srv/salt/env/dev/cert.nacl</p>

<blockquote>
<pre><code>  # or more advanced
</code></pre>

<p>cert=$(cat /tmp/cert.crt)</p>

<pre><code>  salt-call --out=newline_values_only nacl.enc_pub data=&quot;$cert&quot; &gt; /srv/salt/env/dev/cert.nacl
</code></pre>
</blockquote>

<p>NACL within template/native pillars:</p>

<blockquote>
<pre><code>  pillarexample:
      user: root
      password1: {{salt.nacl.dec('DRB7Q6/X5gGSRCTpZyxS6hlbWj0llUA+uaVyvou3vJ4=')|json}}
      cert_key: {{salt.nacl.dec_file('/srv/salt/env/dev/certs/example.com/cert.nacl')|json}}
      cert_key2: {{salt.nacl.dec_file('salt:///certs/example.com/cert2.nacl')|json}}
</code></pre>
</blockquote>

<p>Update readme trusted_ca_minions</p>

<p>Salt minion trust CA certificates issued by salt CA on a specific host (ie: salt-master node)</p>

<blockquote>
<pre><code>  salt:
    minion:
      trusted_ca_minions:
        - cfg01
</code></pre>
</blockquote>

<p>version to be specified for salt formula</p>

<blockquote>
<pre><code>            keystone:
            nova:
              source: pkg
              name: salt-formula-keystone
              version: 0.1+0~20160818133412.24~1.gbp6e1ebb
            postresql:
              name: salt-formula-postgresql
              version: purged
</code></pre>
</blockquote>

<p>Formula keystone is installed latest version and the formulas without version are installed in one call to aptpkg module.</p>

<p>If the version attribute is present sls iterates over formulas and take action to install specific version or remove it.</p>

<p>The version attribute may have these values <code>[latest|purged|removed|&lt;VERSION&gt;]</code>.</p>

<h3 id="sam-stoelinga">Sam Stoelinga<a href="#sam-stoelinga" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Salt minion allow to specify HTTP backend</p>

<p>This is useful when using proxies. The default Tornado backend does not
  utilize proxy environment variables and isn&rsquo;t able to set no_proxy variable.</p>

<p>Salt minion to specify non-default HTTP backend. The default tornado backend
  does not respect HTTP proxy settings set as environment variables. This is
  useful for cases where you need to set no_proxy lists.</p>

<blockquote>
<pre><code>        backend: urllib2
</code></pre>
</blockquote>

<h3 id="tomáš-kukrál-6">Tomáš Kukrál<a href="#tomáš-kukrál-6" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>set state_output to &ldquo;changes&rdquo;</p>

<p>Terse output is not enough for debugging.</p>

<blockquote>
<pre><code>        state_output: changes
</code></pre>
</blockquote>

<p>configure state_output</p>

<p>This option can configure default output of state calls. Terse (default
  option) will make each call to be on single line and make salt output
  better.</p>

<p>Configure verbosity of state output (used for <code>salt</code> command)</p>

<blockquote>
<pre><code>        state_output: terse
</code></pre>
</blockquote>

<h1 id="formula-selenium">Formula selenium</h1>

<h3 id="ales-komarek-20">Ales Komarek<a href="#ales-komarek-20" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Revived the formula</p>

<p>Selenium Formula</p>

<p>Selenium automates browsers. That&rsquo;s it! What you do with that power is
  entirely up to you. Primarily, it is for automating web applications for
  testing purposes, but is certainly not limited to just that. Boring web-based
  administration tasks can (and should!) also be automated as well.</p>

<p>Sample Metadata</p>

<p>Selenium server grid hub</p>

<blockquote>
<pre><code>    selenium:
      hub:
        enabled: true
</code></pre>
</blockquote>

<p>Selenium server grid node with firefox and chrome</p>

<blockquote>
<pre><code>      node:
        hub:
          host: hub01.domain.com
        video:
          host: 127.0.0.1
          display: 99
        drivers:
        - firefox
        - chrome
</code></pre>
</blockquote>

<p>More Information</p>

<ul>
<li><a href="http://alex.nederlof.com/blog/2012/11/19/installing-selenium-with-jenkins-on-ubuntu/">http://alex.nederlof.com/blog/2012/11/19/installing-selenium-with-jenkins-on-ubuntu/</a></li>
<li><a href="http://www.seleniumhq.org/download/">http://www.seleniumhq.org/download/</a></li>
<li><a href="https://sites.google.com/a/chromium.org/chromedriver/">https://sites.google.com/a/chromium.org/chromedriver/</a></li>
</ul>

<h1 id="formula-sensu">Formula sensu</h1>

<h3 id="filip-pytloun-63">Filip Pytloun<a href="#filip-pytloun-63" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Unify Makefile, .gitignore and update readme</p>

<h3 id="guillaume-thouvenin-2">Guillaume Thouvenin<a href="#guillaume-thouvenin-2" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Allow a client to unsubscribe to channel</p>

<p>This patch allows a client to unsubscribe to a channel. It is used by
  example by collectd where we can have different checks. In StackLight we
  have a remote collectd that is running and also a collectd daemon. So
  there is two collectd processes. By default we subscribe to all channel
  according to our roles. But we don&rsquo;t want to subscribe to
  collectd.client, only to collectd.remote_client.</p>

<p>Sensu Client with subscriptions explicitly disabled</p>

<blockquote>
<pre><code>    sensu:
      client:
        enabled: true
        message_queue:
          engine: rabbitmq
          host: rabbitmq
          port: 5672
          user: monitor
          password: pwd
          virtual_host: '/monitor'
        unsubscribe:
          - collectd.client
          - git.client
</code></pre>
</blockquote>

<h3 id="pavel-cizinsky-1">Pavel Cizinsky<a href="#pavel-cizinsky-1" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>updated README for handlers</p>

<blockquote>
<pre><code>            sfdc_http_proxy: 'http://10.10.10.10:8888'
            token_cache_file: &quot;/path/to/cache/token&quot;
</code></pre>
</blockquote>

<p>Sensu Slack handler</p>

<blockquote>
<pre><code>      server:
        handler:
          default:
            enabled: true
            set:
            - slack
          stdout:
          slack:
            enabled: True
            channel: '#channel_name'
            webhook_url: 'https://hooks.slack.com/services/kastan12T/B57X3SDQA/fasfsaf0632hjkl3dsccLn9v'
            proxy_address: '10.10.10.10'
            proxy_port: '8888'
</code></pre>
</blockquote>

<h1 id="formula-sentry">Formula sentry</h1>

<h3 id="ales-komarek-21">Ales Komarek<a href="#ales-komarek-21" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>docfixes</p>

<p>Sentry is a realtime event logging and aggregation platform. At its core it
  specializes in monitoring errors and extracting all the information needed to
  do a proper post-mortem without any of the hassle of the standard user
  feedback loop.</p>

<p>It’s important to note that Sentry should not be thought of as a log stream,
  but as an event aggregator. It fits somewhere in-between a simple metrics
  solution (such as Graphite) and a full-on log stream aggregator (like</p>

<p>Logstash).</p>

<p>Server behind proxy</p>

<p>Sentry 8+ setup</p>

<p>Sentry formula</p>

<p>Sentry is a realtime event logging and aggregation platform. At its core it specializes in monitoring errors and extracting all the information needed to do a proper post-mortem without any of the hassle of the standard user feedback loop.</p>

<p>It’s important to note that Sentry should not be thought of as a log stream, but as an event aggregator. It fits somewhere in-between a simple metrics solution (such as Graphite) and a full-on log stream aggregator (like Logstash).</p>

<p>Sample pillars</p>

<p>Standalone server</p>

<blockquote>
<pre><code>   :language: yaml
</code></pre>
</blockquote>

<p>Simple server with proxy</p>

<p>More information</p>

<ul>
<li><a href="https://github.com/getsentry/sentry">https://github.com/getsentry/sentry</a></li>
<li><a href="https://docs.sentry.io/server/installation/">https://docs.sentry.io/server/installation/</a></li>
</ul>

<h1 id="formula-shibboleth">Formula shibboleth</h1>

<h3 id="alexander-noskov-3">Alexander Noskov<a href="#alexander-noskov-3" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Code refactoring</p>

<p>Shibboleth is among the world&rsquo;s most widely deployed federated identity solutions, connecting users to applications both within and between organizations.</p>

<p>Sample pillars</p>

<blockquote>
<pre><code>  shibboleth:
    server:
      enabled: true
      keystone_protocol: http
      keystone_public_address: ${_param:proxy_vip_address_public}
      keystone_port: 5000
      idp_url: &quot;https://saml.example.com/oam/fed&quot;
      idp_metadata_url: &quot;https://saml.example.com/oamfed/idp/metadata&quot;
      attributes:
      - name: test
        id: test
        name_format: urn:oasis:names:tc:SAML:2.0:attrname-format:basic
      key: |
</code></pre>
</blockquote>

<p>MIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQDmM1NIxgQ3Y70Q</p>

<p>GXoFQQnJ7nliaRtJR2xHAW47InyALQ+M3/VCtdFnNI0d2CHoytQ6mVg6BzOtdvT2
  ocEl0+LNkskSZsc6Nh59XooTQncL5PA7hXmo/nxCEgURH4oika5CC14K4hagwZca</p>

<p>CQZvW1m9KwfVaNc0Va0KepH2lGI+VdxyZgRMifTMl9qDLYr++ftyFTNn5uit0Yh8
  9QFU4HLVvT0rHSQUTcFbvYE=</p>

<blockquote>
<pre><code>      certificate: |
</code></pre>
</blockquote>

<p>MIIDDzCCAfegAwIBAgIJAOvxYAMLVkHZMA0GCSqGSIb3DQEBBQUAMCMxITAfBgNV</p>

<p>BAMTGGN0bC0wMS5qcGUyLmppb2Nsb3VkLmNvbTAeFw0xNzAxMTIxMDIwMTRaFw0y
  k3u0PIEqysz9sOpmuSmlY4FKRobYQ3viviTIMTTuqjoCAFKIApI3tZWOqj+zShje</p>

<p>Xr4ue39/lvQLj2jXV+Q2TOovQA==</p>

<blockquote>
<pre><code>      idp_certificate: |
</code></pre>
</blockquote>

<p>CcnueWJpG0lHbEcBbjsifIAtD4zf9UK10Wc0jR3YIejK1DqZWDoHM6129PZ8kx5k
  aN5DvAdir7oYCpHwD5/WvHahUgsrtcz9s+pzRfiStvICVwqCsGquThZHe8YAgGpZ
  04UU/56ncPbsHf5asS3DvfVGw==</p>

<h1 id="formula-sphinx">Formula sphinx</h1>

<h3 id="filip-pytloun-64">Filip Pytloun<a href="#filip-pytloun-64" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Unify Makefile, .gitignore and update readme</p>

<h1 id="formula-spinnaker">Formula spinnaker</h1>

<h1 id="formula-squid">Formula squid</h1>

<h3 id="ales-komarek-22">Ales Komarek<a href="#ales-komarek-22" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Docfixes</p>

<p>Squid Formula</p>

<p>Sample Pillars</p>

<p>Squid as proxy</p>

<blockquote>
<pre><code>    squid:
      proxy:
        enabled: true
        admin:
          user: manager
          password: passwd
        deny:
        - 192.168.2.30
        allow:
        - localnet
</code></pre>
</blockquote>

<p>More Information</p>

<ul>
<li><a href="https://raw.githubusercontent.com/saltstack-formulas/squid-formula">https://raw.githubusercontent.com/saltstack-formulas/squid-formula</a></li>
<li><a href="http://itkia.com/using-squid-to-cache-apt-updates-for-debian-and-ubuntu/">http://itkia.com/using-squid-to-cache-apt-updates-for-debian-and-ubuntu/</a></li>
<li><a href="http://serverascode.com/2014/03/29/squid-cache-yum.html">http://serverascode.com/2014/03/29/squid-cache-yum.html</a></li>
</ul>

<h1 id="formula-stackstorm">Formula stackstorm</h1>

<h1 id="formula-statsd">Formula statsd</h1>

<h3 id="ales-komarek-23">Ales Komarek<a href="#ales-komarek-23" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>dofix and metadata fix</p>

<p>Statsd formula</p>

<p>More information</p>

<h3 id="filip-pytloun-65">Filip Pytloun<a href="#filip-pytloun-65" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Unify Makefile, .gitignore and update readme</p>

<h1 id="formula-storm">Formula storm</h1>

<h1 id="formula-suitecrm">Formula suitecrm</h1>

<h3 id="filip-pytloun-66">Filip Pytloun<a href="#filip-pytloun-66" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Unify Makefile, .gitignore and update readme</p>

<h1 id="formula-superset">Formula superset</h1>

<h3 id="michael-kutý-1">Michael Kutý<a href="#michael-kutý-1" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Add cache to readme.</p>

<blockquote>
<pre><code>      cache:
        engine: redis
        host: ${_param:superset_cache_host}
        port: 6379
        number: 11
</code></pre>
</blockquote>

<p>Initial commit.</p>

<p>superset</p>

<p>Superset is a data exploration platform designed to be visual, intuitive and interactive.</p>

<p>Sample pillars</p>

<p>Single superset service</p>

<blockquote>
<pre><code>    superset:
      server:
        enabled: true
        backup: true
        debug: true
        auth:
          enabled: true
          user:
            test:
              username: test
              email: email@test.cz
              password: test
        bind:
          address: localhost
          protocol: tcp
          port: 8000
        worker: true
        secret_key: secret
        source:
          engine: pip
          version: 1.0.0       
        database:
          engine: postgres
          host: 127.0.0.1
          name: superset_prd
          password: password
          user: superset_prd
        broker:
          engine: redis
          port: 6379
          number: 10
        logging:
          engine: sentry
          dsn: dsn


    supervisor:
        service:
          superset:
            name: web
            type: superset
          superset_worker:
            name: worker
</code></pre>
</blockquote>

<p>Read more</p>

<ul>
<li><a href="https://github.com/airbnb/superset">https://github.com/airbnb/superset</a></li>
<li><a href="http://airbnb.io/superset/index.html">http://airbnb.io/superset/index.html</a></li>
</ul>

<h1 id="formula-supervisor">Formula supervisor</h1>

<h3 id="aleš-komárek-14">Aleš Komárek<a href="#aleš-komárek-14" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Update README.rst</p>

<p>Supervisor Formula</p>

<p>Sample Pillars</p>

<p>More Information</p>

<h3 id="filip-pytloun-67">Filip Pytloun<a href="#filip-pytloun-67" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Unify Makefile, .gitignore and update readme</p>

<h1 id="formula-swift">Formula swift</h1>

<h3 id="filip-pytloun-68">Filip Pytloun<a href="#filip-pytloun-68" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Unify Makefile, .gitignore and update readme</p>

<h1 id="formula-syncthing">Formula syncthing</h1>

<h1 id="formula-taiga">Formula taiga</h1>

<h3 id="filip-pytloun-69">Filip Pytloun<a href="#filip-pytloun-69" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Unify Makefile, .gitignore and update readme</p>

<h1 id="formula-telegraf">Formula telegraf</h1>

<h3 id="bartosz-kupidura-1">Bartosz Kupidura<a href="#bartosz-kupidura-1" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Move input definitions to support.yml</p>

<p>Initial commit</p>

<p>Salt Telegraf formula</p>

<p>Power your metrics and alerting with a leading open-source monitoring
  solution.</p>

<p>Sample pillars</p>

<blockquote>
<pre><code>  telegraf:
    agent:
      enabled: true
      interval: 15
      round_interval: false
      metric_batch_size: 1000
      metric_buffer_limit: 10000
      collection_jitter: 2
      output:
        prometheus_client:
          bind:
            address: 0.0.0.0
            port: 9126
          engine: prometheus
      input:
        diskio:
        processes:
        net:
</code></pre>
</blockquote>

<h3 id="petr-michalec-12">Petr Michalec<a href="#petr-michalec-12" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Add influxdb output</p>

<p>Influx output</p>

<blockquote>
<pre><code>        influxdb:
          urls:
            - http://127.0.0.1:8086
          database: test-telegraf
          write_consistency: any
          timeout: 10s
</code></pre>
</blockquote>

<h1 id="formula-tempest">Formula tempest</h1>

<h3 id="aleš-komárek-15">Aleš Komárek<a href="#aleš-komárek-15" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>docifx</p>

<p>Tempest Formula</p>

<p>This is a set of integration tests to be run against a live OpenStack cluster. Tempest has batteries of tests for OpenStack API validation, Scenarios, and other specific tests useful in validating an OpenStack deployment.</p>

<p>Sample Pillars</p>

<blockquote>
<pre><code>    tempest:
      test:
        enabled: true
        source:
          engine: git
          address: git://github.com/openstack/tempest.git
          revision: master
        suite:
          identity:
            disable_ssl_certificate_validation: true
            auth_version: v3
            uri_v3:
            region: RegionOne 
          identity-feature-enabled:
            trust: true
            api_v2: false
            api_v3: true
</code></pre>
</blockquote>

<p>More Information</p>

<ul>
<li><a href="http://docs.openstack.org/developer/tempest/overview.html">http://docs.openstack.org/developer/tempest/overview.html</a></li>
<li><a href="http://www.slideshare.net/masayukiigawa/tempest-scenariotests-20140512?related=1">http://www.slideshare.net/masayukiigawa/tempest-scenariotests-20140512?related=1</a></li>
<li><a href="https://github.com/stackforge/puppet-tempest">https://github.com/stackforge/puppet-tempest</a></li>
</ul>

<h1 id="formula-tftpd-hpa">Formula tftpd-hpa</h1>

<h3 id="ales-komarek-24">Ales Komarek<a href="#ales-komarek-24" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Documentation fixes</p>

<p>TFTPD HPA formula</p>

<p>A TFTP server is mainly required for booting operating systems or
  configurations over the network.</p>

<p>Sample pillars</p>

<p>More information</p>

<h3 id="filip-pytloun-70">Filip Pytloun<a href="#filip-pytloun-70" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Unify Makefile, .gitignore and update readme</p>

<h1 id="formula-tinyproxy">Formula tinyproxy</h1>

<h3 id="dmitry-stremkouski-4">Dmitry Stremkouski<a href="#dmitry-stremkouski-4" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Adding example variables</p>

<blockquote>
<pre><code>      listen: 172.16.21.101
      allow_nets:
      - 127.0.0.1
      - 172.16.21.0/24
      connect_ports:
      - 443
      upstream: 10.11.236.1:8080
</code></pre>
</blockquote>

<p>Populating formula from local repository</p>

<p>TinyProxy Formula</p>

<p>Sample Pillars</p>

<blockquote>
<pre><code>TinyProxy: Basic configuration


    tinyproxy:
      enabled: true
</code></pre>
</blockquote>

<p>More Information</p>

<ul>
<li><a href="https://en.wikipedia.org/wiki/Tinyproxy">https://en.wikipedia.org/wiki/Tinyproxy</a></li>
</ul>

<h3 id="filip-pytloun-71">Filip Pytloun<a href="#filip-pytloun-71" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Productize formula</p>

<blockquote>
<pre><code>      server:
        enabled: true
        bind:
          address: 172.16.21.101
          port: 8888
        allow:
          - 127.0.0.1
          - 172.16.21.0/24
        deny:
          - 8.8.8.8
        connect_ports:
          - 443
        upstream: 10.11.236.1:8080
</code></pre>
</blockquote>

<h1 id="formula-tvheadend">Formula tvheadend</h1>

<h1 id="formula-vagrant">Formula vagrant</h1>

<h3 id="ales-komarek-25">Ales Komarek<a href="#ales-komarek-25" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Allow version to control definition</p>

<blockquote>
<pre><code>              version: '2016.3'
</code></pre>
</blockquote>

<p>More information</p>

<p>updated version</p>

<p>Vagrant formula</p>

<p>Vagrant provides easy to configure, reproducible, and portable work
  environments built on top of industry-standard technology and controlled by a
  single consistent workflow to help maximize the productivity and flexibility
  of you and your team.</p>

<p>To achieve its magic, Vagrant stands on the shoulders of giants. Machines are
  provisioned on top of VirtualBox, VMware, AWS, or any other provider. Then,
  industry-standard provisioning tools such as shell scripts, Chef, or Puppet,
  can be used to automatically install and configure software on the machine.</p>

<blockquote>
<pre><code>                status: suspended
</code></pre>
</blockquote>

<p>Sample usage</p>

<p>Start and connect machine
  cd /srv/vagrant/<cluster_name>
  vagrant up <node_name>
  vagrant ssh <node_name></p>

<p>External links</p>

<h3 id="filip-pytloun-72">Filip Pytloun<a href="#filip-pytloun-72" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Unify Makefile, .gitignore and update readme</p>

<h1 id="formula-varnish">Formula varnish</h1>

<h3 id="filip-pytloun-73">Filip Pytloun<a href="#filip-pytloun-73" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Unify Makefile, .gitignore and update readme</p>

<h3 id="martin-polreich">Martin Polreich<a href="#martin-polreich" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Added tests script and sample pillars, Kitchen tests and Travis</p>

<p>And Supervisor like this:: yaml</p>

<p>Using nginx type:: yaml</p>

<h1 id="formula-virtualbox">Formula virtualbox</h1>

<h3 id="filip-pytloun-74">Filip Pytloun<a href="#filip-pytloun-74" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Unify Makefile, .gitignore and update readme</p>

<h1 id="formula-wordpress">Formula wordpress</h1>

<h3 id="ales-komarek-26">Ales Komarek<a href="#ales-komarek-26" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Overhaul</p>

<p>Wordpress formula</p>

<p>Sample metadata</p>

<p>Simple site</p>

<blockquote>
<pre><code>    wordpress:
      server:
        app:
          app_name:
            enabled: true
            version: '4.0'
            url: example.com
            title: TCPisekWeb
            admin_user: admin
            admin_password: password
            admin_email: nikicresl@gmail.com
            core_update: false
            theme_update: false
            plugin:
              bbpress:
                engine: http
                version: latest
              git_plugin:
                engine: git
                address: git@git.domain.com:git-repo
                revision: master
            database:
              engine: mysql
              host: 127.0.0.1
              name: w_site
              password: password
              user: w_tcpisek
              prefix: tcpisek
</code></pre>
</blockquote>

<p>Read more</p>

<h3 id="aleš-komárek-16">Aleš Komárek<a href="#aleš-komárek-16" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>docfix</p>

<p>Wordpress Formula</p>

<p>WordPress is web software you can use to create a beautiful website or blog.</p>

<p>More Information</p>

<ul>
<li><a href="http://codex.wordpress.org/Installing_WordPress">http://codex.wordpress.org/Installing_WordPress</a></li>
<li><a href="http://www.severalnines.com/blog/scaling-wordpress-and-mysql-multiple-servers-performance">http://www.severalnines.com/blog/scaling-wordpress-and-mysql-multiple-servers-performance</a></li>
</ul>

<h1 id="formula-xtrabackup">Formula xtrabackup</h1>

<h3 id="jiri-broulik-8">Jiri Broulik<a href="#jiri-broulik-8" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>initial commit</p>

<p>xtrabackup formula</p>

<p>Xtrabackup allows you to backup and restore databases from full backups or full backups and its incrementals.</p>

<p>Sample pillars</p>

<p>Backup client with ssh/rsync remote host</p>

<blockquote>
<pre><code>    xtrabackup:
      client:
        enabled: true
        full_backups_to_keep: 3
        hours_before_full: 48
        hours_before_incr: 12
        database:
          user: username
          password: password
        target:
          host: cfg01
</code></pre>
</blockquote>

<p>More options to relocate local backups can be done using salt-formula-backupninja.</p>

<p>Backup client with local backup only</p>

<p>Backup client with ssh/rsync remote host with compression:</p>

<blockquote>
<pre><code>        compression: true
        compression_threads: 2
</code></pre>
</blockquote>

<p>Backup server rsync</p>

<blockquote>
<pre><code>      server:
        full_backups_to_keep: 5
        key:
          xtrabackup_pub_key:
            enabled: true
            key: key
</code></pre>
</blockquote>

<p>Client restore from local backups:</p>

<blockquote>
<pre><code>        restore_full_latest: 1
        restore_from: local
        compressThreads: 2
        qpress:
          source: tar
          name: url
</code></pre>
</blockquote>

<p>Client restore from remote backups:</p>

<blockquote>
<pre><code>        restore_from: remote
</code></pre>
</blockquote>

<p>More information</p>

<ul>
<li><a href="https://labs.riseup.net/code/projects/xtrabackup/wiki/Configuration">https://labs.riseup.net/code/projects/xtrabackup/wiki/Configuration</a></li>
<li><a href="http://www.debian-administration.org/articles/351">http://www.debian-administration.org/articles/351</a></li>
<li><a href="http://duncanlock.net/blog/2013/08/27/comprehensive-linux-backups-with-etckeeper-xtrabackup/">http://duncanlock.net/blog/2013/08/27/comprehensive-linux-backups-with-etckeeper-xtrabackup/</a></li>
<li><a href="https://github.com/riseuplabs/puppet-xtrabackup">https://github.com/riseuplabs/puppet-xtrabackup</a></li>
<li><a href="http://www.ushills.co.uk/2008/02/backup-with-xtrabackup.html">http://www.ushills.co.uk/2008/02/backup-with-xtrabackup.html</a></li>
</ul>

<h3 id="marcin-iwinski">Marcin Iwinski<a href="#marcin-iwinski" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Spliting backup_dir in client runner.</p>

<p>Required to support different backup_dir between client and server.</p>

<p>Backup client with ssh/rsync to remote host with compression and non-default backup directory on server:</p>

<blockquote>
<pre><code>        enabled: false
        backup_dir: /srv/backup
</code></pre>

<p>default location, same on both client and server side.</p>
</blockquote>

<p>Backup server rsync and non-default backup directory:</p>

<h1 id="formula-zookeeper">Formula zookeeper</h1>

<h3 id="filip-pytloun-75">Filip Pytloun<a href="#filip-pytloun-75" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>Unify Makefile, .gitignore and update readme</p>

<h3 id="jiri-broulik-9">Jiri Broulik<a href="#jiri-broulik-9" class="hanchor" ariaLabel="Anchor"> 🔗&#xFE0E;</a> </h3>

<p>zookeeper backup</p>

<p>Backup client with ssh/rsync remote host</p>

<blockquote>
<pre><code>    zookeeper:
      backup:
        client:
          enabled: true
          full_backups_to_keep: 3
          hours_before_full: 24
          target:
            host: cfg01
</code></pre>
</blockquote>

<p>More options to relocate local backups can be done using salt-formula-backupninja.</p>

<p>Backup client with local backup only</p>

<p>Backup server rsync</p>

<blockquote>
<pre><code>        server:
          full_backups_to_keep: 5
          key:
            zookeeper_pub_key:
              enabled: true
              key: ssh_rsa
</code></pre>
</blockquote>

<p>Client restore from local backup:</p>

<blockquote>
<pre><code>          restore_latest: 1
          restore_from: local
</code></pre>
</blockquote>

<p>Client restore from remote backup:</p>

<blockquote>
<pre><code>          restore_from: remote
</code></pre>
</blockquote>
                <ul class="share-buttons">
    <li>Share this article:</li>
    <li>
        <a class="icon-facebook-squared" href="https://www.facebook.com/sharer/sharer.php?u=http%3a%2f%2fapealive.net%2fpost%2f2017-12-salt-formulas-updates%2f" onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;" title="Share on Facebook"></a>
    </li>
    <li>
        <a class="icon-twitter" href="https://twitter.com/share?text=Salt-Formulas%20updates&amp;url=http%3a%2f%2fapealive.net%2fpost%2f2017-12-salt-formulas-updates%2f" onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;" title="Tweet this article"></a>
    </li>
    <li>
        <a class="icon-gplus" href="https://plus.google.com/share?url=http%3a%2f%2fapealive.net%2fpost%2f2017-12-salt-formulas-updates%2f" onclick="window.open(this.href, 'google-plus-share', 'width=490,height=530');return false;" title="Share on Google&#43;"></a>
    </li>
    <li>
        <a class="icon-linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=http%3a%2f%2fapealive.net%2fpost%2f2017-12-salt-formulas-updates%2f&title=Salt-Formulas%20updates" onclick="window.open(this.href, 'linkedin-share', 'width=600,height=494');return false;" title="Share on Linkedin"></a>
    </li>
</ul>

            

        </article>
        
            <div class="comments">
                <h3>Comments</h3>
                <div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "apealive" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
            </div>
        
    </main>
    <aside  class="author">

  <a class="" href="http://apealive.net//about_petr" title="About Petr">
   <img class="profile-image" src="http://apealive.net/img/profile-image.png" alt="Petr Michalec" />
  </a>
  <p class="name">by 
  <strong>Petr Michalec</strong></p>
  <p class="address">Brandys nad Labem - Stara Boleslav</p>
  
  <p class="link"></p>
  <ul class="social">
    
<li><a href="//twitter.com/epcim" class="icon-twitter" target="_blank" title="Twitter"></a></li>



<li><a href="//twitter.com/apealive_net" class="icon-twitter" target="_blank" title="Twitter @apealive_net"></a></li>



<li><a href="//facebook.com/petr.michalec" class="icon-facebook" target="_blank" title="Facebook"></a></li>











<li><a href="//github.com/epcim" class="icon-github" target="_blank" title="Github"></a></li>



<li><a href="//plus.google.com/+PetrMichalec-Prague" class="icon-gplus" target="_blank" title="Google+"></a></li>



<li><a href="//delicious.com/epcim" class="icon-delicious" target="_blank" title="Delicious"></a></li>


<li><a href="http://apealive.net/post/index.xml" class="icon-rss" target="_blank" title="RSS"></a></li>

  </ul>
  <br><br>
  <center>
      <a class="link" href="http://apealive.net//about" title="About">About</a>
      |
      <a class="link" href="http://apealive.net//photo" title="Photographs">Photographs</a>
      |
      <a class="link" href="http://apealive.net//tags" title="Tags">Tags</a>
  </center>
</aside>

</div>


<footer class="main-footer">
  <div class="container clearfix">
        <a class="icon-rss" href="http://apealive.net/post/index.xml" title="RSS"></a>
        <p>&copy; 10127 &middot; &copy; APe, Powered by <a href="http://gohugo.io">Hugo</a>.</p>
  </div>
</footer>
<script src="http://apealive.net/js/load-photoswipe.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.1/photoswipe.min.js" integrity="sha256-UplRCs9v4KXVJvVY+p+RSo5Q4ilAUXh7kpjyIP5odyc=" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.1/photoswipe-ui-default.min.js" integrity="sha256-PWHOlUzc96pMc8ThwRIXPn8yH4NOLu42RQ0b9SpnpFk=" crossorigin="anonymous"></script>

<script src="http://apealive.net/js/highlight.pack.js">
<script src="http:\/\/apealive.net\/js/plugins.js"></script>

<script>hljs.initHighlightingOnLoad();</script>


    <!-- Google Analytics -->
    <script>
      var _gaq=[['_setAccount','UA-71675144-1'],['_trackPageview']];
      (function(d,t){var g=d.createElement(t),s=d.getElementsByTagName(t)[0];
      g.src=('https:'==location.protocol?'//ssl':'//www')+'.google-analytics.com/ga.js';
      s.parentNode.insertBefore(g,s)}(document,'script'));
    </script>


</body>
</html>


